# Copyright (c) 2023, Teriks
#
# dgenerate is distributed under the following BSD 3-Clause License
#
# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in
#    the documentation and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
import contextlib
import gc
import typing

from dgenerate.pipelinewrapper.util import get_quantizer_uri_class as _get_quantizer_uri_class
import dgenerate.pipelinewrapper.util as _pipelinewrapper_util
import dgenerate.promptupscalers.promptupscaler as _promptupscaler
import dgenerate.promptupscalers.exceptions as _exceptions
import dgenerate.prompt as _prompt
import dgenerate.memory as _memory
import dgenerate.messages as _messages
import dgenerate.promptupscalers.llmupscalermixin as _llmupscalermixin

import torch
import transformers


@contextlib.contextmanager
def _with_seed(seed: int | None):
    if seed is None:
        yield
    else:
        orig_state = torch.random.get_rng_state()
        torch.manual_seed(seed)
        try:
            yield
        finally:
            torch.random.set_rng_state(orig_state)


class MagicPromptUpscaler(_llmupscalermixin.LLMPromptUpscalerMixin, _promptupscaler.PromptUpscaler):
    """
    Upscale prompts using magicprompt or other LLMs via transformers.

    The "part" argument indicates which parts of the prompt to act on,
    possible values are: "both", "positive", and "negative"

    The "model" specifies the model path for magicprompt, the
    default value is: "Gustavosta/MagicPrompt-Stable-Diffusion". This
    can be a folder on disk or a Hugging Face repository slug.

    The "seed" argument can be used to specify a seed for prompt generation.

    The "variations" argument specifies how many variations should be produced.

    The "max-length" argument is the max prompt length for a
    generated prompt, this value defaults to 100.

    The "temperature" argument sets the sampling temperature
    to use when generating prompts.

    The "system" argument sets the system instruction for the LLM.

    The "preamble" argument sets a text input preamble for the LLM, this
    preamble will be removed from the output generated by the LLM.

    The "remove-prompt" argument specifies whether to remove the
    original prompt from the generated text.

    The "prepend-prompt" argument specifies whether to forcefully
    prepend the original prompt to the generated prompt, this
    might be necessary if you want a continuation with some
    models, the original prompt will be prepended with a
    space at the end.

    The "batch" argument enables and disables batching
    prompt text into the LLM, setting this to False tells
    the plugin that you only want the LLM to ever process
    one prompt at a time, this might be useful if you are
    memory constrained, but processing is much slower.

    The "quantizer" argument allows you to specify a quantization
    backend for loading the LLM, this is the same syntax and supported
    backends as with the dgenerate --quantizer argument.

    The "block-regex" argument is a python syntax regex that will
    block prompts that match the regex, the prompt will be regenerated
    until the regex does not match, up to "max-attempts". This
    regex is case-insensitive.

    The "max-attempts" argument specifies how many times to reattempt
    to generate a prompt if it is blocked by "block-regex"

    The "smart-truncate" argument enables intelligent truncation
    of the prompt generated by the LLM, i.e. it will remove incomplete
    sentences from the end of the prompt utilizing spaCy NLP.
    """

    NAMES = ['magicprompt']

    def __init__(self,
                 part: str = 'both',
                 model: str = "Gustavosta/MagicPrompt-Stable-Diffusion",
                 seed: int | None = None,
                 variations: int = 1,
                 max_length: int = 100,
                 temperature: float = 0.7,
                 system: str | None = None,
                 preamble: str | None = None,
                 remove_prompt: bool = False,
                 prepend_prompt: bool = False,
                 batch: bool = True,
                 quantizer: str | None = None,
                 block_regex: str | None = None,
                 max_attempts: int = 10,
                 smart_truncate: bool = False,
                 **kwargs
                 ):
        """
        :param kwargs: child class forwarded arguments
        """
        super().__init__(**kwargs,
                         part=part,
                         block_regex=block_regex,
                         max_attempts=max_attempts,
                         cleanup_mode='magic' if 'magicprompt' in model.lower() else 'other',
                         smart_truncate=smart_truncate)

        part = part.lower()

        if quantizer:
            try:
                quantization_config = _get_quantizer_uri_class(quantizer).parse(quantizer).to_config()
            except Exception as e:
                raise self.argument_error(f'Error loading "quantizer" argument "{quantizer}": {e}')
        else:
            quantization_config = None

        if part not in {'both', 'positive', 'negative'}:
            raise self.argument_error(
                'Argument "part" must be one of: "both", "positive", or "negative"'
            )

        if max_length < 1:
            raise self.argument_error(
                'Cannot specify "max-length" less than 1.'
            )

        if variations < 1:
            raise self.argument_error(
                'Argument "variations" may not be less than 1.')

        model_files = list(
            _pipelinewrapper_util.fetch_model_files_with_size(
                model,
                local_files_only=self.local_files_only,
                extensions={'.safetensors', '.bin'})
        )

        if len(model_files) > 1:
            model_files = [m for m in model_files if m[0].endswith('.safetensors')]

        estimated_size = 0

        for model_entry in model_files:
            estimated_size += model_entry[1]

        _messages.debug_log(
            f'Estimated the size of LLM model: '
            f'{model}, as: {estimated_size} Bytes ({_memory.bytes_best_human_unit(estimated_size)})')

        def load_method():
            if quantization_config is not None:
                self.memory_guard_device(self.device, self.size_estimate)
            return self._load_pipeline(model, quantization_config=quantization_config)

        self.set_size_estimate(estimated_size)

        self._pipeline = self.load_object_cached(
            tag=model + (quantizer if quantizer else ''),
            estimated_size=estimated_size,
            method=load_method
        )

        self._system = system
        self._preamble = preamble
        self._remove_prompt = remove_prompt
        self._seed = seed
        self._max_length = max_length
        self._temperature = temperature

        self._variations = variations
        self._accepts_batch = batch
        self._part = part
        self._quantizer = quantizer
        self._device = 'cpu'
        self._max_attempts = max_attempts
        self._prepend_prompt = prepend_prompt

    def _load_pipeline(self, model_name: str, quantization_config: typing.Optional = None) -> transformers.Pipeline:

        model = transformers.AutoModelForCausalLM.from_pretrained(
            model_name, trust_remote_code=True,
            quantization_config=quantization_config)
        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
        tokenizer.pad_token_id = model.config.eos_token_id

        og_model_generate = model.generate

        def generator_patch(*args, **kwargs):
            args = list(args)

            if quantization_config is None:
                for name, val in kwargs.items():
                    if hasattr(val, 'to'):
                        kwargs[name] = val.to(self._device)

                for idx, val in enumerate(args):
                    if hasattr(val, 'to'):
                        args[idx] = val.to(self._device)

            with _with_seed(self._seed):
                result = og_model_generate(*args, **kwargs)

            if quantization_config is None:
                for val in args:
                    if hasattr(val, 'to'):
                        val.to('cpu')

                for val in kwargs.values():
                    if hasattr(val, 'to'):
                        val.to('cpu')

            return result

        model.generate = generator_patch

        return transformers.pipeline(
            task="text-generation",
            tokenizer=tokenizer,
            model=model,
            device='cpu' if quantization_config is None else None,
            pad_token_id=tokenizer.eos_token_id,
        )

    def _to(self, device: str | torch.device):
        self._device = device
        self._pipeline.model.to(device)

    def _generate_prompts(self, original_prompts: list[str]) -> list[str]:
        def build_query(text):
            if self._preamble:
                return self._preamble + (' ' if not self._preamble.endswith(' ') else '') + text
            return text

        if self._system:
            formatted_prompts = [
                f"<|system|> {self._system} <|user|> {build_query(query)} <|assistant|>"
                for query in original_prompts
            ]
        else:
            formatted_prompts = [build_query(query) for query in original_prompts]

        if not self._accepts_batch:
            generated_prompts = []
            for ptext in formatted_prompts:
                generated_prompts.extend(
                    self._pipeline(
                        [ptext],
                        max_length=self._max_length,
                        temperature=self._temperature,
                        do_sample=True,
                        batch_size=1,
                    ))
        else:
            generated_prompts = self._pipeline(
                formatted_prompts,
                max_length=self._max_length,
                temperature=self._temperature,
                do_sample=True,
                batch_size=len(formatted_prompts),
            )

        generated_prompts = [p[0]["generated_text"] for p in generated_prompts]

        generated_prompts = [
            self._clean_prompt(
                formatted_prompt,
                generated_prompt,
                remove_prefixes=[self._system, self._preamble],
                remove_prompt=self._remove_prompt,
                prepend=original_prompt if self._prepend_prompt else None,
            )
            for original_prompt, formatted_prompt, generated_prompt in zip(
                original_prompts, formatted_prompts, generated_prompts
            )
        ]

        return generated_prompts

    @contextlib.contextmanager
    def _with_device(self):
        if self._quantizer:
            yield
            gc.collect()
            _memory.torch_gc()
        else:
            try:
                self.memory_guard_device(self.device, self.size_estimate)
                self._to(self.device)
                yield
            finally:
                self._to('cpu')
                gc.collect()
                _memory.torch_gc()

    @property
    def accepts_batch(self):
        """
        This prompt upscaler can accept a batch of prompts for efficient execution.
        :return: ``True``, unless the constructor argument ``batch`` was passed ``False``
        """
        return self._accepts_batch

    def upscale(self, prompts: _prompt.PromptOrPrompts) -> _prompt.PromptOrPrompts:

        if isinstance(prompts, _prompt.Prompt):
            prompts = [prompts]

        if len(prompts) > 1 and not self.accepts_batch:
            raise _exceptions.PromptUpscalerProcessingError(
                f'magicprompt prompt upscaler cannot accept batch input when '
                f'the argument "batch" is set to False.'
            )

        prompts = list(prompts) * self._variations

        try:
            with self._with_device():
                return self._process_prompts(prompts)
        except torch.cuda.OutOfMemoryError:
            prompt_count = len(prompts)
            if prompt_count > 1:
                raise _exceptions.PromptUpscalerProcessingError(
                    f'magicprompt prompt upscaler could not '
                    f'process {len(prompts)} incoming prompt(s) due to CUDA '
                    f'out of memory error, try using the argument "batch=False" '
                    f'to process only one prompt at a time (this is slow).')
            raise _exceptions.PromptUpscalerProcessingError(
                f'magicprompt prompt upscaler could not '
                f'process prompt due to CUDA out of memory error: {prompts[0]}'
            )
        except transformers.pipelines.PipelineException as e:
            raise _exceptions.PromptUpscalerProcessingError(
                f'magicprompt prompt upscaler could not process prompt(s) due '
                f'to transformers pipeline exception: {e}'
            )
