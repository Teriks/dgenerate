# Copyright (c) 2023, Teriks
#
# dgenerate is distributed under the following BSD 3-Clause License
#
# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in
#    the documentation and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

import os
import re

try:
    import gpt4all
except ImportError:
    gpt4all = None

import dgenerate.promptupscalers.promptupscaler as _promptupscaler
import dgenerate.promptupscalers.exceptions as _exceptions
import dgenerate.prompt as _prompt
import dgenerate.memory as _memory
import dgenerate.messages as _messages
import dgenerate.webcache as _webcache
import dgenerate.promptupscalers.util as _util


class GPT4ALLPromptUpscaler(_promptupscaler.PromptUpscaler):
    """
    Upscale prompts using LLMs loadable by GPT4ALL.

    The "part" argument indicates which parts of the prompt to act on,
    possible values are: "both", "positive", and "negative"

    The "model" specifies the model path for gpt4all, the
    default value is: "https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf".
    This can be a path to a GGUF file or a URL pointing to one.

    The "variations" argument specifies how many variations should be produced.

    The "max-length" argument is the max prompt length for a
    generated prompt, this value defaults to 100.

    The "temperature" argument sets the sampling temperature
    to use when generating prompts.

    The "system" argument sets the system instruction for the LLM.

    The "preamble" argument sets a text input preamble for the LLM, this
    preamble will be removed from the output generated by the LLM.

    The "remove-prompt" argument specifies whether to remove the
    original prompt from the generated text.

    The "prepend-prompt" argument specifies whether to forcefully
    prepend the original prompt to the generated prompt, this
    might be necessary if you want a continuation with some
    models, the original prompt will be prepended with a
    space at the end.

    The "batch" argument enables and disables batching
    prompt text into the LLM, setting this to False tells
    the plugin that you only want the LLM to ever process
    one prompt at a time, this might be useful if you are
    memory constrained, but processing is much slower.

    The "compute" argument lets you specify the GPT4ALL
    device string, this is distinct from torch device names,
    hence it is called "compute" here.

    This may be one of:

    NOWRAP!
    * "cpu": Model will run on the central processing unit.
    * "gpu": Use Metal on ARM64 macOS, otherwise the same as "kompute".
    * "kompute": Use the best GPU provided by the Kompute backend.
    * "cuda": Use the best GPU provided by the CUDA backend.
    * "amd", "nvidia": Use the best GPU provided by the Kompute backend from this vendor.

    The "block-regex" argument is a python syntax regex that will
    block prompts that match the regex, the prompt will be regenerated
    until the regex does not match, up to "max-attempts". This
    regex is case-insensitive.

    The "max-attempts" argument specifies how many times to reattempt
    to generate a prompt if it is blocked by "block-regex"

    The "context-tokens" argument specifies the amount of context
    tokens the model was trained on, you may need to adjust this
    if GPT4ALL warns about the number of specified context tokens.
    """

    NAMES = ['gpt4all']

    HIDE_ARGS = ['device']

    def __init__(self,
                 part: str = 'both',
                 model: str = "https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf",
                 variations: int = 1,
                 max_length: int = 100,
                 temperature: float = 0.7,
                 system: str | None = None,
                 preamble: str | None = None,
                 remove_prompt: bool = False,
                 prepend_prompt: bool = False,
                 compute: str | None = 'cpu',
                 block_regex: str | None = None,
                 max_attempts: int = 10,
                 context_tokens: int = 2048,
                 **kwargs
                 ):
        """
        :param kwargs: child class forwarded arguments
        """
        super().__init__(**kwargs)

        if gpt4all is None:
            raise self.argument_error(
                'Cannot use gpt4all prompt upscaler without gpt4all being installed.')

        part = part.lower()

        if part not in {'both', 'positive', 'negative'}:
            raise self.argument_error(
                'Argument "part" must be one of: "both", "positive", or "negative"'
            )

        if max_length < 1:
            raise self.argument_error(
                'Cannot specify "max-length" less than 1.'
            )

        if variations < 1:
            raise self.argument_error(
                'Argument "variations" may not be less than 1.')

        if max_attempts < 1:
            raise self.argument_error(
                'Argument "max_attempts" may not be less than 1.')

        if block_regex:
            try:
                self._blocklist_regex = re.compile(block_regex, re.IGNORECASE)
            except Exception as e:
                raise self.argument_error(f'Could not compile "block-regex": {e}')
        else:
            self._blocklist_regex = None

        if _webcache.is_downloadable_url(model):
            # Any mimetype
            _, model = _webcache.create_web_cache_file(model)
        else:
            if not os.path.exists(model):
                raise self.argument_error(
                    'Argument "model" must be a path to a GGUF file on disk if not a URL.')
            model = os.path.abspath(model)

        estimated_size = os.stat(model).st_size

        _messages.debug_log(
            f'Estimated the size of LLM model: '
            f'{model}, as: {estimated_size} Bytes ({_memory.bytes_best_human_unit(estimated_size)})')

        def load_method():
            if compute in {'cuda', 'gpu', 'kompute', 'amd'}:
                self.memory_guard_device(self.device, estimated_size)
            return gpt4all.GPT4All(model, device=compute, n_ctx=context_tokens)

        self.set_size_estimate(estimated_size)

        self._gpt4all: gpt4all.GPT4All = self.load_object_cached(
            tag=model + str(context_tokens) + compute,
            estimated_size=estimated_size,
            method=load_method
        )

        self._preamble = preamble
        self._remove_prompt = remove_prompt
        self._max_prompt_length = max_length
        self._temperature = temperature

        self._system = system
        self._variations = variations
        self._part = part
        self._max_attempts = max_attempts
        self._prepend_prompt = prepend_prompt

    def _generate(self, prompts: list[str] | str) -> list[str]:
        if not isinstance(prompts, list):
            prompts = [prompts]

        generated_prompts = self._generate_prompts(prompts)
        return self._regenerate_blocked_prompts(prompts, generated_prompts, self._max_attempts)

    def _regenerate_blocked_prompts(
            self,
            original_prompts: list[str],
            generated_prompts: list[str],
            max_attempts: int,
    ) -> list[str]:
        indexed_prompts_to_regenerate = []
        if self._blocklist_regex:
            for _ in range(max_attempts):
                indexed_prompts_to_regenerate = [
                    (i, prompt)
                    for i, prompt in enumerate(generated_prompts)
                    if self._blocklist_regex.search(prompt)
                ]

                if not indexed_prompts_to_regenerate:
                    break
                indexes = [x[0] for x in indexed_prompts_to_regenerate]
                prompts_to_regenerate = [original_prompts[index] for index in indexes]
                regenerated_prompts = self._generate_prompts(
                    prompts_to_regenerate,
                )
                for i, prompt in zip(indexes, regenerated_prompts):
                    generated_prompts[i] = prompt

            if len(indexed_prompts_to_regenerate) > 0:
                _messages.log(
                    f"Could not generate prompts for "
                    f"{len(indexed_prompts_to_regenerate)} prompts after {max_attempts} attempts.",
                    level=_messages.WARNING
                )
        return generated_prompts

    def _generate_prompts(self, orig_prompts: list[str]) -> list[str]:
        def build_query(text):
            if self._preamble:
                return self._preamble + (' ' if not self._preamble.endswith(' ') else '') + text
            return text

        user_prompts = orig_prompts.copy()

        if self._system:
            orig_prompts = [
                f"<|system|> {self._system} <|user|> {build_query(query)} <|assistant|>"
                for query in orig_prompts
            ]
        else:
            orig_prompts = [build_query(query) for query in orig_prompts]

        prompts = []

        for ptext in orig_prompts:
            prompts.append(self._gpt4all.generate(
                ptext,
                max_tokens=self._max_prompt_length,
                temp=self._temperature
            ))

        prompts = [
            (user_prompt.rstrip() + ' ' if self._prepend_prompt else '') +
            _util.clean_up_llm_prompt(
                orig_prompt,
                prompt,
                remove_system=self._system,
                remove_preamble=self._preamble,
                remove_prompt=self._remove_prompt
            )
            for orig_prompt, prompt, user_prompt in zip(orig_prompts, prompts, user_prompts)
        ]

        return prompts

    @property
    def accepts_batch(self):
        """
        Returns ``True``, this prompt upscaler can always accept a batch of prompts.
        :return: ``True``
        """
        return True

    def upscale(self, prompts: _prompt.PromptOrPrompts) -> _prompt.PromptOrPrompts:

        if isinstance(prompts, _prompt.Prompt):
            prompts = [prompts]

        if len(prompts) > 1 and not self.accepts_batch:
            raise _exceptions.PromptUpscalerProcessingError(
                f'gpt4all prompt upscaler cannot accept batch input when '
                f'the argument "batch" is set to False.'
            )

        prompts = list(prompts) * self._variations

        try:

            generated_pos_prompts = [p.positive for p in prompts]
            generated_neg_prompts = [p.negative for p in prompts]

            if self._part in {'both', 'positive'}:
                non_empty_idx = [idx for idx, p in enumerate(prompts) if p.positive]

                pos_prompts = [p.positive for p in prompts if p.positive]

                if pos_prompts:
                    generated = self._generate(pos_prompts)

                    for idx, non_empty_idx in enumerate(non_empty_idx):
                        generated_pos_prompts[non_empty_idx] = generated[idx]

            if self._part in {'both', 'negative'}:
                non_empty_idx = [idx for idx, p in enumerate(prompts) if p.negative]

                neg_prompts = [p.negative for p in prompts if p.negative]

                if neg_prompts:
                    generated = self._generate(neg_prompts)

                    for idx, non_empty_idx in enumerate(non_empty_idx):
                        generated_neg_prompts[non_empty_idx] = generated[idx]

        except Exception as e:
            raise _exceptions.PromptUpscalerProcessingError(
                f'gpt4all prompt upscaler could not process prompt(s) due '
                f'to transformers pipeline exception: {e}'
            )

        output = []
        for idx, (generated_pos_prompt, generated_neg_prompt) in \
                enumerate(zip(generated_pos_prompts, generated_neg_prompts)):
            prompt_obj = _prompt.Prompt(
                positive=generated_pos_prompt,
                negative=generated_neg_prompt,
                delimiter=prompts[idx].delimiter
            )

            # We need to preserve the embedded diffusion
            # arguments from the original incoming prompt
            # that were parsed out by dgenerate
            prompt_obj.copy_embedded_args_from(prompts[idx])

            # append the generated prompt to the expanded
            # output list of prompts
            output.append(prompt_obj)

        return output
