# Copyright (c) 2023, Teriks
#
# dgenerate is distributed under the following BSD 3-Clause License
#
# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in
#    the documentation and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

import os

try:
    import gpt4all
except ImportError:
    gpt4all = None

import dgenerate.promptupscalers.promptupscaler as _promptupscaler
import dgenerate.promptupscalers.exceptions as _exceptions
import dgenerate.prompt as _prompt
import dgenerate.memory as _memory
import dgenerate.messages as _messages
import dgenerate.webcache as _webcache
import dgenerate.promptupscalers.llmupscalermixin as _llmupscalermixin
import dgenerate.hfhub as _hfhub


class GPT4ALLPromptUpscaler(_llmupscalermixin.LLMPromptUpscalerMixin, _promptupscaler.PromptUpscaler):
    """
    Upscale prompts using LLMs loadable by GPT4ALL.

    The "part" argument indicates which parts of the prompt to act on,
    possible values are: "both", "positive", and "negative"

    The "model" specifies the model path for gpt4all, the
    default value is: "https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf".
    This can be a path to a GGUF file or a URL pointing to one.

    The "variations" argument specifies how many variations should be produced.

    The "max-length" argument is the max prompt length for a
    generated prompt, this value defaults to 100.

    The "temperature" argument sets the sampling temperature
    to use when generating prompts. Larger values increase
    creativity but decrease factuality.

    The "top_k" argument sets the "top_k" generation value, i.e.
    randomly sample from the "top_k" most likely tokens at each generation step.
    Set this to 1 for greedy decoding.

    The "top_p" argument sets the "top_p" generation value, i.e.
    randomly sample at each generation step from the top most likely
    tokens whose probabilities add up to "top_p".

    The "min_p" argument sets the "min_p" generation value, i.e. randomly
    sample at each generation step from the top most likely tokens
    whose probabilities are at least "min_p".

    The "system" argument sets the system instruction for the LLM.

    The "preamble" argument sets a text input preamble for the LLM, this
    preamble will be removed from the output generated by the LLM.

    The "remove-prompt" argument specifies whether to remove the
    original prompt from the generated text.

    The "prepend-prompt" argument specifies whether to forcefully
    prepend the original prompt to the generated prompt, this
    might be necessary if you want a continuation with some
    models, the original prompt will be prepended with a
    space at the end.

    The "compute" argument lets you specify the GPT4ALL
    device string, this is distinct from torch device names,
    hence it is called "compute" here.

    This may be one of:

    NOWRAP!
    * "cpu": Model will run on the central processing unit.
    * "gpu": Use Metal on ARM64 macOS, otherwise the same as "kompute".
    * "kompute": Use the best GPU provided by the Kompute backend.
    * "cuda": Use the best GPU provided by the CUDA backend.
    * "amd", "nvidia": Use the best GPU provided by the Kompute backend from this vendor.

    The "block-regex" argument is a python syntax regex that will
    block prompts that match the regex, the prompt will be regenerated
    until the regex does not match, up to "max-attempts". This
    regex is case-insensitive.

    The "max-attempts" argument specifies how many times to reattempt
    to generate a prompt if it is blocked by "block-regex"

    The "context-tokens" argument specifies the amount of context
    tokens the model was trained on, you may need to adjust this
    if GPT4ALL warns about the number of specified context tokens.

    The "smart-truncate" argument enables intelligent truncation
    of the prompt generated by the LLM, i.e. it will remove incomplete
    sentences from the end of the prompt utilizing spaCy NLP.

    The "cleanup-config" argument allows you to specify a
    custom LLM output cleanup configuration file in
    .json, .toml, or .yaml format. This file can be used
    to run custom pattern substitutions or python functions
    over the LLMs raw output, and overrides the built-in cleanup
    excluding "smart-truncate" which occurs before your configuration.
    """

    NAMES = ['gpt4all']

    HIDE_ARGS = ['device']

    OPTION_ARGS = {
        'part': ['both', 'positive', 'negative'],
        'compute': ['cpu', 'gpu', 'kompute', 'cuda', 'amd']
    }

    FILE_ARGS = {
        'model': {'mode': 'in', 'filetypes': [('GGUF', ['*.gguf'])]},
        'cleanup-config': {'mode': 'in', 'filetypes': [('Cleanup Config', ('*.json', '*.toml', '*.yaml', '*.yml'))]}
    }

    def __init__(self,
                 part: str = 'both',
                 model: str = "https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf",
                 variations: int = 1,
                 max_length: int = 100,
                 temperature: float = 0.7,
                 top_k: int = 40,
                 top_p: float = 0.4,
                 min_p: float = 0.0,
                 system: str | None = None,
                 preamble: str | None = None,
                 remove_prompt: bool = False,
                 prepend_prompt: bool = False,
                 compute: str | None = 'cpu',
                 block_regex: str | None = None,
                 max_attempts: int = 10,
                 context_tokens: int = 2048,
                 smart_truncate: bool = False,
                 cleanup_config: str | None = None,
                 **kwargs
                 ):
        """
        :param kwargs: child class forwarded arguments
        """
        super().__init__(**kwargs,
                         part=part,
                         block_regex=block_regex,
                         max_attempts=max_attempts,
                         cleanup_mode='magic' if 'magicprompt' in model.lower() else 'other',
                         smart_truncate=smart_truncate,
                         cleanup_config=cleanup_config)

        if gpt4all is None:
            raise _exceptions.PromptUpscalerError(
                'Cannot use gpt4all prompt upscaler without gpt4all being installed.')

        part = part.lower()

        if max_length < 1:
            raise self.argument_error(
                'Cannot specify "max-length" less than 1.'
            )

        if variations < 1:
            raise self.argument_error(
                'Argument "variations" may not be less than 1.')

        if temperature < 0.0:
            raise self.argument_error(
                'Argument "temperature" may not be less than 1.')

        if top_k < 1:
            raise self.argument_error(
                'Argument "top-k" may not be less than 1.')

        if top_p < 0.0:
            raise self.argument_error(
                'Argument "top-p" may not be less than 0.')

        if top_p > 1.0:
            raise self.argument_error(
                'Argument "top-p" may not be greater than 1.')

        if min_p < 0.0:
            raise self.argument_error(
                'Argument "min-p" may not be less than 0.')

        if min_p > 1.0:
            raise self.argument_error(
                'Argument "min-p" may not be greater than 1.')

        if _webcache.is_downloadable_url(model):
            # Any mimetype
            try:
                model = _hfhub.webcache_or_hf_blob_download(model, local_files_only=self.local_files_only)
            except Exception as e:
                raise self.argument_error(
                    f'Could not download argument "model": "{model}", error: {e}') from e
        else:
            if not os.path.exists(model):
                raise self.argument_error(
                    'Argument "model" must be a path to a GGUF file on disk if not a URL.')
            model = os.path.abspath(model)

        estimated_size = os.stat(model).st_size

        _messages.debug_log(
            f'Estimated the size of LLM model: '
            f'{model}, as: {estimated_size} Bytes ({_memory.bytes_best_human_unit(estimated_size)})')

        def load_method():
            if compute in {'cuda', 'gpu', 'kompute', 'amd'}:
                self.memory_guard_device(self.device, estimated_size)
            try:
                return gpt4all.GPT4All(model, device=compute, n_ctx=context_tokens)
            except RuntimeError as e:
                raise self.argument_error(f'Argument "model", could not load: {e}') from e

        self.set_size_estimate(estimated_size)

        self._gpt4all: gpt4all.GPT4All = self.load_object_cached(
            tag=model + str(context_tokens) + compute,
            estimated_size=estimated_size,
            method=load_method
        )

        self._preamble = preamble
        self._remove_prompt = remove_prompt
        self._max_length = max_length
        self._temperature = temperature
        self._top_k = top_k
        self._top_p = top_p
        self._min_p = min_p

        self._system = system
        self._variations = variations
        self._part = part
        self._max_attempts = max_attempts
        self._prepend_prompt = prepend_prompt

    def _generate_prompts(self, original_prompts: list[str]) -> list[str]:
        def build_query(text):
            if self._preamble:
                return self._preamble + (' ' if not self._preamble.endswith(' ') else '') + text
            return text

        if self._system:
            formatted_prompts = [
                f"<|system|> {self._system} <|user|> {build_query(query)} <|assistant|>"
                for query in original_prompts
            ]
        else:
            formatted_prompts = [build_query(query) for query in original_prompts]

        generated_prompts = []

        for ptext in formatted_prompts:
            generated_prompts.append(
                self._gpt4all.generate(
                    ptext,
                    max_tokens=self._max_length,
                    temp=self._temperature,
                    top_k=self._top_k,
                    top_p=self._top_p,
                    min_p=self._min_p
                ))

        generated_prompts = [
            self._clean_prompt(
                formatted_prompt,
                generated_prompt,
                remove_prefixes=[self._system, self._preamble],
                remove_prompt=self._remove_prompt,
                prepend=original_prompt if self._prepend_prompt else None,
            )
            for original_prompt, formatted_prompt, generated_prompt in zip(
                original_prompts, formatted_prompts, generated_prompts
            )
        ]

        return generated_prompts

    @property
    def accepts_batch(self) -> bool:
        """
        Returns ``True``, this prompt upscaler can always accept a batch of prompts.
        :return: ``True``
        """
        return True

    def upscale(self, prompts: _prompt.PromptOrPrompts) -> _prompt.PromptOrPrompts:

        if isinstance(prompts, _prompt.Prompt):
            prompts = [prompts]

        if len(prompts) > 1 and not self.accepts_batch:
            raise _exceptions.PromptUpscalerProcessingError(
                f'gpt4all prompt upscaler cannot accept batch input when '
                f'the argument "batch" is set to False.'
            )

        prompts = list(prompts) * self._variations

        try:
            return self._process_prompts(prompts)
        except Exception as e:
            raise _exceptions.PromptUpscalerProcessingError(
                f'gpt4all prompt upscaler could not process prompt(s) due '
                f'to pipeline exception: {e}'
            ) from e
