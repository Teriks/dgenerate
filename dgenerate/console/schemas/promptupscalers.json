{"attention": {"part": {"optional": false, "types": ["str"], "default": "both", "options": ["both", "positive", "negative"]}, "min": {"optional": false, "types": ["int"], "default": 0.1}, "max": {"optional": false, "types": ["int"], "default": 0.9}, "seed": {"optional": true, "types": ["int"], "default": null}, "lang": {"optional": false, "types": ["str"], "default": "en", "options": ["en", "de", "fr", "es", "it", "nl", "pt", "ru", "zh"]}, "syntax": {"optional": false, "types": ["str"], "default": "sd-embed", "options": ["sd-embed", "compel"]}, "PROMPT_UPSCALER_HELP": "attention:\n    arguments:\n        part: str = \"both\"\n        min: int = 0.1\n        max: int = 0.9\n        seed: int | None = None\n        lang: str = \"en\"\n        syntax: str = \"sd-embed\"\n\n    Add random attention values to your prompt tokens.\n\n    This is ment for use with --prompt-weighter plugins such as \"sd-embed\" or \"compel\"\n\n    The \"part\" argument indicates which parts of the prompt to act on, possible values are: \"both\",\n    \"positive\", and \"negative\"\n\n    The \"min\" argument sets the minimum value for random attention added. The default value is 0.1\n\n    The \"max\" argument sets the maximum value for random attention added. The Default value is 0.9\n\n    The \"seed\" argument can be used to specify a seed for the random attenuation values that are\n    added to your prompt.\n\n    The \"lang\" argument can be used to specify the prompt language, the default value is 'en' for\n    english, this can be one of: 'en', 'de', 'fr', 'es', 'it', 'nl', 'pt', 'ru', 'zh'.\n\n    The \"syntax\" argument specifies the token attention value syntax, this can be one of \"sd-embed\"\n    (SD Web UI Syntax) or \"compel\" (InvokeAI Syntax)."}, "dynamicprompts": {"part": {"optional": false, "types": ["str"], "default": "both", "options": ["both", "positive", "negative"]}, "random": {"optional": false, "types": ["bool"], "default": false}, "seed": {"optional": true, "types": ["int"], "default": null}, "variations": {"optional": true, "types": ["int"], "default": null}, "wildcards": {"optional": true, "types": ["str"], "default": null}, "PROMPT_UPSCALER_HELP": "dynamicprompts:\n    arguments:\n        part: str = \"both\"\n        random: bool = False\n        seed: int | None = None\n        variations: int | None = None\n        wildcards: str | None = None\n\n    Upscale prompts with the dynamicprompts library.\n\n    This upscaler allows you to use a special syntax for combinatorial prompt variations.\n\n    See: https://github.com/adieyal/dynamicprompts\n\n    The \"part\" argument indicates which parts of the prompt to act on, possible values are: \"both\",\n    \"positive\", and \"negative\"\n\n    The \"random\" argument specifies that instead of strictly combinatorial output, dynamicprompts\n    should produce N random variations of your prompt given the possibilities you have provided.\n\n    The \"seed\" argument can be used to specify a seed for the \"random\" prompt generation.\n\n    The \"variations\" argument specifies how many variations should be produced when \"random\" is set\n    to true. This argument cannot be used without specifying \"random\". The default value is 1.\n\n    The \"wildcards\" argument can be used to specify a wildcards directory for dynamicprompts\n    wildcard syntax."}, "gpt4all": {"part": {"optional": false, "types": ["str"], "default": "both", "options": ["both", "positive", "negative"]}, "model": {"optional": false, "types": ["str"], "default": "https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf", "files": {"mode": "in", "filetypes": [["GGUF", ["*.gguf"]]]}}, "variations": {"optional": false, "types": ["int"], "default": 1}, "max-length": {"optional": false, "types": ["int"], "default": 100}, "temperature": {"optional": false, "types": ["float"], "default": 0.7}, "top-k": {"optional": false, "types": ["int"], "default": 40}, "top-p": {"optional": false, "types": ["float"], "default": 0.4}, "min-p": {"optional": false, "types": ["float"], "default": 0.0}, "system": {"optional": true, "types": ["str"], "default": null}, "preamble": {"optional": true, "types": ["str"], "default": null}, "remove-prompt": {"optional": false, "types": ["bool"], "default": false}, "prepend-prompt": {"optional": false, "types": ["bool"], "default": false}, "compute": {"optional": true, "types": ["str"], "default": "cpu", "options": ["cpu", "gpu", "kompute", "cuda", "amd"]}, "block-regex": {"optional": true, "types": ["str"], "default": null}, "max-attempts": {"optional": false, "types": ["int"], "default": 10}, "context-tokens": {"optional": false, "types": ["int"], "default": 2048}, "smart-truncate": {"optional": false, "types": ["bool"], "default": false}, "cleanup-config": {"optional": true, "types": ["str"], "default": null, "files": {"mode": "in", "filetypes": [["Cleanup Config", ["*.json", "*.toml", "*.yaml", "*.yml"]]]}}, "PROMPT_UPSCALER_HELP": "gpt4all:\n    arguments:\n        part: str = \"both\"\n        model: str = \"https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf\"\n        variations: int = 1\n        max-length: int = 100\n        temperature: float = 0.7\n        top-k: int = 40\n        top-p: float = 0.4\n        min-p: float = 0.0\n        system: str | None = None\n        preamble: str | None = None\n        remove-prompt: bool = False\n        prepend-prompt: bool = False\n        compute: str | None = \"cpu\"\n        block-regex: str | None = None\n        max-attempts: int = 10\n        context-tokens: int = 2048\n        smart-truncate: bool = False\n        cleanup-config: str | None = None\n\n    Upscale prompts using LLMs loadable by GPT4ALL.\n\n    The \"part\" argument indicates which parts of the prompt to act on, possible values are: \"both\",\n    \"positive\", and \"negative\"\n\n    The \"model\" specifies the model path for gpt4all, the default value is:\n    \"https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf\".\n    This can be a path to a GGUF file or a URL pointing to one.\n\n    The \"variations\" argument specifies how many variations should be produced.\n\n    The \"max-length\" argument is the max prompt length for a generated prompt, this value defaults\n    to 100.\n\n    The \"temperature\" argument sets the sampling temperature to use when generating prompts. Larger\n    values increase creativity but decrease factuality.\n\n    The \"top_k\" argument sets the \"top_k\" generation value, i.e. randomly sample from the \"top_k\"\n    most likely tokens at each generation step. Set this to 1 for greedy decoding.\n\n    The \"top_p\" argument sets the \"top_p\" generation value, i.e. randomly sample at each generation\n    step from the top most likely tokens whose probabilities add up to \"top_p\".\n\n    The \"min_p\" argument sets the \"min_p\" generation value, i.e. randomly sample at each generation\n    step from the top most likely tokens whose probabilities are at least \"min_p\".\n\n    The \"system\" argument sets the system instruction for the LLM.\n\n    The \"preamble\" argument sets a text input preamble for the LLM, this preamble will be removed\n    from the output generated by the LLM.\n\n    The \"remove-prompt\" argument specifies whether to remove the original prompt from the generated\n    text.\n\n    The \"prepend-prompt\" argument specifies whether to forcefully prepend the original prompt to the\n    generated prompt, this might be necessary if you want a continuation with some models, the\n    original prompt will be prepended with a space at the end.\n\n    The \"compute\" argument lets you specify the GPT4ALL device string, this is distinct from torch\n    device names, hence it is called \"compute\" here.\n\n    This may be one of:\n\n    * \"cpu\": Model will run on the central processing unit.\n    * \"gpu\": Use Metal on ARM64 macOS, otherwise the same as \"kompute\".\n    * \"kompute\": Use the best GPU provided by the Kompute backend.\n    * \"cuda\": Use the best GPU provided by the CUDA backend.\n    * \"amd\", \"nvidia\": Use the best GPU provided by the Kompute backend from this vendor.\n\n    The \"block-regex\" argument is a python syntax regex that will block prompts that match the\n    regex, the prompt will be regenerated until the regex does not match, up to \"max-attempts\". This\n    regex is case-insensitive.\n\n    The \"max-attempts\" argument specifies how many times to reattempt to generate a prompt if it is\n    blocked by \"block-regex\"\n\n    The \"context-tokens\" argument specifies the amount of context tokens the model was trained on,\n    you may need to adjust this if GPT4ALL warns about the number of specified context tokens.\n\n    The \"smart-truncate\" argument enables intelligent truncation of the prompt generated by the LLM,\n    i.e. it will remove incomplete sentences from the end of the prompt utilizing spaCy NLP.\n\n    The \"cleanup-config\" argument allows you to specify a custom LLM output cleanup configuration\n    file in .json, .toml, or .yaml format. This file can be used to run custom pattern substitutions\n    or python functions over the LLMs raw output, and overrides the built-in cleanup excluding\n    \"smart-truncate\" which occurs before your configuration."}, "magicprompt": {"part": {"optional": false, "types": ["str"], "default": "both", "options": ["both", "positive", "negative"]}, "model": {"optional": false, "types": ["str"], "default": "Gustavosta/MagicPrompt-Stable-Diffusion", "files": {"mode": "dir"}}, "dtype": {"optional": false, "types": ["str"], "default": "float32", "options": ["float32", "float16", "bfloat16"]}, "seed": {"optional": true, "types": ["int"], "default": null}, "variations": {"optional": false, "types": ["int"], "default": 1}, "max-length": {"optional": false, "types": ["int"], "default": 100}, "temperature": {"optional": false, "types": ["float"], "default": 0.7}, "top-k": {"optional": false, "types": ["int"], "default": 50}, "top-p": {"optional": false, "types": ["float"], "default": 1.0}, "system": {"optional": true, "types": ["str"], "default": null}, "preamble": {"optional": true, "types": ["str"], "default": null}, "remove-prompt": {"optional": false, "types": ["bool"], "default": false}, "prepend-prompt": {"optional": false, "types": ["bool"], "default": false}, "batch": {"optional": false, "types": ["bool"], "default": true}, "max-batch": {"optional": true, "types": ["int"], "default": 50}, "quantizer": {"optional": true, "types": ["str"], "default": null}, "block-regex": {"optional": true, "types": ["str"], "default": null}, "max-attempts": {"optional": false, "types": ["int"], "default": 10}, "smart-truncate": {"optional": false, "types": ["bool"], "default": false}, "cleanup-config": {"optional": true, "types": ["str"], "default": null, "files": {"mode": "in", "filetypes": [["Cleanup Config", ["*.json", "*.toml", "*.yaml", "*.yml"]]]}}, "device": {"optional": true, "types": ["str"], "default": null}, "PROMPT_UPSCALER_HELP": "magicprompt:\n    arguments:\n        part: str = \"both\"\n        model: str = \"Gustavosta/MagicPrompt-Stable-Diffusion\"\n        dtype: str = \"float32\"\n        seed: int | None = None\n        variations: int = 1\n        max-length: int = 100\n        temperature: float = 0.7\n        top-k: int = 50\n        top-p: float = 1.0\n        system: str | None = None\n        preamble: str | None = None\n        remove-prompt: bool = False\n        prepend-prompt: bool = False\n        batch: bool = True\n        max-batch: int | None = 50\n        quantizer: str | None = None\n        block-regex: str | None = None\n        max-attempts: int = 10\n        smart-truncate: bool = False\n        cleanup-config: str | None = None\n        device: str | None = None\n\n    Upscale prompts using magicprompt or other LLMs via transformers.\n\n    The \"part\" argument indicates which parts of the prompt to act on, possible values are: \"both\",\n    \"positive\", and \"negative\"\n\n    The \"model\" specifies the model path for magicprompt, the default value is:\n    \"Gustavosta/MagicPrompt-Stable-Diffusion\". This can be a folder on disk or a Hugging Face\n    repository slug.\n\n    The \"dtype\" argument specifies the torch dtype (compute dtype) to load the model with, this\n    defaults to: float32, and may be one of: float32, float16, or bfloat16.\n\n    The \"seed\" argument can be used to specify a seed for prompt generation.\n\n    The \"variations\" argument specifies how many variations should be produced.\n\n    The \"max-length\" argument is the max prompt length for a generated prompt, this value defaults\n    to 100.\n\n    The \"temperature\" argument sets the sampling temperature to use when generating prompts. Larger\n    values increase creativity but decrease factuality.\n\n    The \"top_k\" argument sets the \"top_k\" generation value, i.e. randomly sample from the \"top_k\"\n    most likely tokens at each generation step. Set this to 1 for greedy decoding.\n\n    The \"top_p\" argument sets the \"top_p\" generation value, i.e. randomly sample at each generation\n    step from the top most likely tokens whose probabilities add up to \"top_p\".\n\n    The \"system\" argument sets the system instruction for the LLM.\n\n    The \"preamble\" argument sets a text input preamble for the LLM, this preamble will be removed\n    from the output generated by the LLM.\n\n    The \"remove-prompt\" argument specifies whether to remove the original prompt from the generated\n    text.\n\n    The \"prepend-prompt\" argument specifies whether to forcefully prepend the original prompt to the\n    generated prompt, this might be necessary if you want a continuation with some models, the\n    original prompt will be prepended with a space at the end.\n\n    The \"batch\" argument enables and disables batching prompt text into the LLM, setting this to\n    False tells the plugin that you only want the LLM to ever process one prompt at a time, this\n    might be useful if you are memory constrained, but processing is much slower.\n\n    The \"max-batch\" argument allows you to adjust how many prompts can be processed by the LLM\n    simultaneously, processing too many prompts at once will run your system out of memory,\n    processing too little prompts at once will be slow. Specifying \"None\" indicates unlimited batch\n    size.\n\n    The \"quantizer\" argument allows you to specify a quantization backend for loading the LLM, this\n    is the same syntax and supported backends as with the dgenerate --quantizer argument.\n\n    The \"block-regex\" argument is a python syntax regex that will block prompts that match the\n    regex, the prompt will be regenerated until the regex does not match, up to \"max-attempts\". This\n    regex is case-insensitive.\n\n    The \"max-attempts\" argument specifies how many times to reattempt to generate a prompt if it is\n    blocked by \"block-regex\"\n\n    The \"smart-truncate\" argument enables intelligent truncation of the prompt generated by the LLM,\n    i.e. it will remove incomplete sentences from the end of the prompt utilizing spaCy NLP.\n\n    The \"cleanup-config\" argument allows you to specify a custom LLM output cleanup configuration\n    file in .json, .toml, or .yaml format. This file can be used to run custom pattern substitutions\n    or python functions over the LLMs raw output, and overrides the built-in cleanup excluding\n    \"smart-truncate\" which occurs before your configuration.\n\n    The \"device\" argument can be used to set the device the prompt upscaler will run any models on,\n    for example: cpu, cuda, cuda:1. this argument will default to the value of the dgenerate\n    argument --device."}, "translate": {"input": {"optional": false, "types": ["str"]}, "output": {"optional": false, "types": ["str"], "default": "en"}, "part": {"optional": false, "types": ["str"], "default": "both", "options": ["both", "positive", "negative"]}, "provider": {"optional": false, "types": ["str"], "default": "argos", "options": ["argos", "mariana"]}, "batch": {"optional": false, "types": ["bool"], "default": true}, "max-batch": {"optional": true, "types": ["int"], "default": 50}, "device": {"optional": true, "types": ["str"], "default": null}, "PROMPT_UPSCALER_HELP": "translate:\n    arguments:\n        input: str\n        output: str = \"en\"\n        part: str = \"both\"\n        provider: str = \"argos\"\n        batch: bool = True\n        max-batch: int | None = 50\n        device: str | None = None\n\n    Local language translation using argostranslate or Helsinki-NLP opus (mariana).\n\n    Please note that translation models require a one time download, so run at least once with\n    --offline-mode disabled to download the desired model.\n\n    argostranslate (argos) offers lightweight translation via CPU inference.\n\n    Helsinki-NLP (mariana) offers slightly more heavy duty (accurate) CPU or GPU inference.\n\n    The \"input\" argument indicates the input language code (IETF) e.g. \"en\", \"zh\", or literal name\n    of the language for example: \"english\", \"chinese\".\n\n    The \"output\" argument indicates the output language code (IETF), or literal name of the\n    language, this value defaults to \"en\" (English).\n\n    The \"provider\" argument indicates the translation provider, which may be one of \"argos\" or\n    \"mariana\". The default value is \"argos\", indicating argostranslate. argos will only ever use the\n    \"cpu\" regardless of the current --device or \"device\" argument value. Mariana will default to\n    using the value of --device which will usually be a GPU.\n\n    The \"batch\" argument enables and disables batching prompt text into the translator, setting this\n    to False tells the plugin that you only want to ever process one prompt at a time, this might be\n    useful if you are memory constrained and using the provider \"mariana\", but processing is much\n    slower.\n\n    The \"max-batch\" argument allows you to adjust how many prompts can be processed by the model\n    simultaneously, processing too many prompts at once will run your system out of memory\n    (specifically for the mariana translation provider), processing too little prompts at once will\n    be slow. Specifying \"None\" indicates unlimited batch size. This argument has no effect on\n    argostranslate performance.\n\n    The \"device\" argument can be used to set the device the prompt upscaler will run any models on,\n    for example: cpu, cuda, cuda:1. this argument will default to the value of the dgenerate\n    argument --device."}}