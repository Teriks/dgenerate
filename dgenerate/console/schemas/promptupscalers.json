{"dynamicprompts": {"part": {"optional": false, "types": ["str"], "default": "both"}, "attention": {"optional": false, "types": ["bool"], "default": false}, "attention-min": {"optional": false, "types": ["int"], "default": 0.1}, "attention-max": {"optional": false, "types": ["int"], "default": 0.9}, "random": {"optional": false, "types": ["bool"], "default": false}, "seed": {"optional": true, "types": ["int"], "default": null}, "variations": {"optional": true, "types": ["int"], "default": null}, "wildcards": {"optional": true, "types": ["str"], "default": null}, "PROMPT_UPSCALER_HELP": "dynamicprompts:\n    arguments:\n        part: str = \"both\"\n        attention: bool = False\n        attention-min: int = 0.1\n        attention-max: int = 0.9\n        random: bool = False\n        seed: int | None = None\n        variations: int | None = None\n        wildcards: str | None = None\n\n    Upscale prompts with the dynamicprompts library.\n\n    This upscaler allows you to use a special syntax for combinatorial prompt variations, and can\n    also add random attention values to your prompt.\n\n    See: https://github.com/adieyal/dynamicprompts\n\n    The \"part\" argument indicates which parts of the prompt to act on, possible values are: \"both\",\n    \"positive\", and \"negative\"\n\n    The \"attention\" argument enables random token attention values, this requires the use of the\n    \"sd-embed\" prompt weighter or \"compel\" in SD Web UI syntax mode, i.e. \"compel;syntax=sdwui\"\n\n    The \"attention-min\" argument sets the minimum value for random attention added by\n    \"attention=True\". The default value is 0.1\n\n    The \"attention-max\" argument sets the maximum value for random attention added by\n    \"attention=True\" The Default value is 0.9\n\n    The \"random\" argument specifies that instead of strictly combinatorial output, dynamicprompts\n    should produce N random variations of your prompt given the possibilities you have provided.\n\n    The \"seed\" argument can be used to specify a seed for the \"random\" prompt generation.\n\n    The \"variations\" argument specifies how many variations should be produced when \"random\" is set\n    to true. This argument cannot be used without specifying \"random\". The default value is 1.\n\n    The \"wildcards\" argument can be used to specify a wildcards directory for dynamicprompts\n    wildcard syntax."}, "gpt4all": {"part": {"optional": false, "types": ["str"], "default": "both"}, "model": {"optional": false, "types": ["str"], "default": "https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf"}, "variations": {"optional": false, "types": ["int"], "default": 1}, "max-length": {"optional": false, "types": ["int"], "default": 100}, "temperature": {"optional": false, "types": ["float"], "default": 0.7}, "system": {"optional": true, "types": ["str"], "default": null}, "preamble": {"optional": true, "types": ["str"], "default": null}, "remove-prompt": {"optional": false, "types": ["bool"], "default": false}, "prepend-prompt": {"optional": false, "types": ["bool"], "default": false}, "compute": {"optional": true, "types": ["str"], "default": "cpu"}, "block-regex": {"optional": true, "types": ["str"], "default": null}, "max-attempts": {"optional": false, "types": ["int"], "default": 10}, "context-tokens": {"optional": false, "types": ["int"], "default": 2048}, "smart-truncate": {"optional": false, "types": ["bool"], "default": false}, "PROMPT_UPSCALER_HELP": "gpt4all:\n    arguments:\n        part: str = \"both\"\n        model: str = \"https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf\"\n        variations: int = 1\n        max-length: int = 100\n        temperature: float = 0.7\n        system: str | None = None\n        preamble: str | None = None\n        remove-prompt: bool = False\n        prepend-prompt: bool = False\n        compute: str | None = \"cpu\"\n        block-regex: str | None = None\n        max-attempts: int = 10\n        context-tokens: int = 2048\n        smart-truncate: bool = False\n\n    Upscale prompts using LLMs loadable by GPT4ALL.\n\n    The \"part\" argument indicates which parts of the prompt to act on, possible values are: \"both\",\n    \"positive\", and \"negative\"\n\n    The \"model\" specifies the model path for gpt4all, the default value is:\n    \"https://huggingface.co/failspy/Phi-3-mini-128k-instruct-abliterated-v3-GGUF/resolve/main/Phi-3-mini-128k-instruct-abliterated-v3_q4.gguf\".\n    This can be a path to a GGUF file or a URL pointing to one.\n\n    The \"variations\" argument specifies how many variations should be produced.\n\n    The \"max-length\" argument is the max prompt length for a generated prompt, this value defaults\n    to 100.\n\n    The \"temperature\" argument sets the sampling temperature to use when generating prompts.\n\n    The \"system\" argument sets the system instruction for the LLM.\n\n    The \"preamble\" argument sets a text input preamble for the LLM, this preamble will be removed\n    from the output generated by the LLM.\n\n    The \"remove-prompt\" argument specifies whether to remove the original prompt from the generated\n    text.\n\n    The \"prepend-prompt\" argument specifies whether to forcefully prepend the original prompt to the\n    generated prompt, this might be necessary if you want a continuation with some models, the\n    original prompt will be prepended with a space at the end.\n\n    The \"compute\" argument lets you specify the GPT4ALL device string, this is distinct from torch\n    device names, hence it is called \"compute\" here.\n\n    This may be one of:\n\n    * \"cpu\": Model will run on the central processing unit.\n    * \"gpu\": Use Metal on ARM64 macOS, otherwise the same as \"kompute\".\n    * \"kompute\": Use the best GPU provided by the Kompute backend.\n    * \"cuda\": Use the best GPU provided by the CUDA backend.\n    * \"amd\", \"nvidia\": Use the best GPU provided by the Kompute backend from this vendor.\n\n    The \"block-regex\" argument is a python syntax regex that will block prompts that match the\n    regex, the prompt will be regenerated until the regex does not match, up to \"max-attempts\". This\n    regex is case-insensitive.\n\n    The \"max-attempts\" argument specifies how many times to reattempt to generate a prompt if it is\n    blocked by \"block-regex\"\n\n    The \"context-tokens\" argument specifies the amount of context tokens the model was trained on,\n    you may need to adjust this if GPT4ALL warns about the number of specified context tokens.\n\n    The \"smart-truncate\" argument enables intelligent truncation of the prompt generated by the LLM,\n    i.e. it will remove incomplete sentences from the end of the prompt utilizing spaCy NLP."}, "magicprompt": {"part": {"optional": false, "types": ["str"], "default": "both"}, "model": {"optional": false, "types": ["str"], "default": "Gustavosta/MagicPrompt-Stable-Diffusion"}, "seed": {"optional": true, "types": ["int"], "default": null}, "variations": {"optional": false, "types": ["int"], "default": 1}, "max-length": {"optional": false, "types": ["int"], "default": 100}, "temperature": {"optional": false, "types": ["float"], "default": 0.7}, "system": {"optional": true, "types": ["str"], "default": null}, "preamble": {"optional": true, "types": ["str"], "default": null}, "remove-prompt": {"optional": false, "types": ["bool"], "default": false}, "prepend-prompt": {"optional": false, "types": ["bool"], "default": false}, "batch": {"optional": false, "types": ["bool"], "default": true}, "quantizer": {"optional": true, "types": ["str"], "default": null}, "block-regex": {"optional": true, "types": ["str"], "default": null}, "max-attempts": {"optional": false, "types": ["int"], "default": 10}, "smart-truncate": {"optional": false, "types": ["bool"], "default": false}, "device": {"optional": true, "types": ["str"], "default": null}, "PROMPT_UPSCALER_HELP": "magicprompt:\n    arguments:\n        part: str = \"both\"\n        model: str = \"Gustavosta/MagicPrompt-Stable-Diffusion\"\n        seed: int | None = None\n        variations: int = 1\n        max-length: int = 100\n        temperature: float = 0.7\n        system: str | None = None\n        preamble: str | None = None\n        remove-prompt: bool = False\n        prepend-prompt: bool = False\n        batch: bool = True\n        quantizer: str | None = None\n        block-regex: str | None = None\n        max-attempts: int = 10\n        smart-truncate: bool = False\n        device: str | None = None\n\n    Upscale prompts using magicprompt or other LLMs via transformers.\n\n    The \"part\" argument indicates which parts of the prompt to act on, possible values are: \"both\",\n    \"positive\", and \"negative\"\n\n    The \"model\" specifies the model path for magicprompt, the default value is:\n    \"Gustavosta/MagicPrompt-Stable-Diffusion\". This can be a folder on disk or a Hugging Face\n    repository slug.\n\n    The \"seed\" argument can be used to specify a seed for prompt generation.\n\n    The \"variations\" argument specifies how many variations should be produced.\n\n    The \"max-length\" argument is the max prompt length for a generated prompt, this value defaults\n    to 100.\n\n    The \"temperature\" argument sets the sampling temperature to use when generating prompts.\n\n    The \"system\" argument sets the system instruction for the LLM.\n\n    The \"preamble\" argument sets a text input preamble for the LLM, this preamble will be removed\n    from the output generated by the LLM.\n\n    The \"remove-prompt\" argument specifies whether to remove the original prompt from the generated\n    text.\n\n    The \"prepend-prompt\" argument specifies whether to forcefully prepend the original prompt to the\n    generated prompt, this might be necessary if you want a continuation with some models, the\n    original prompt will be prepended with a space at the end.\n\n    The \"batch\" argument enables and disables batching prompt text into the LLM, setting this to\n    False tells the plugin that you only want the LLM to ever process one prompt at a time, this\n    might be useful if you are memory constrained, but processing is much slower.\n\n    The \"quantizer\" argument allows you to specify a quantization backend for loading the LLM, this\n    is the same syntax and supported backends as with the dgenerate --quantizer argument.\n\n    The \"block-regex\" argument is a python syntax regex that will block prompts that match the\n    regex, the prompt will be regenerated until the regex does not match, up to \"max-attempts\". This\n    regex is case-insensitive.\n\n    The \"max-attempts\" argument specifies how many times to reattempt to generate a prompt if it is\n    blocked by \"block-regex\"\n\n    The \"smart-truncate\" argument enables intelligent truncation of the prompt generated by the LLM,\n    i.e. it will remove incomplete sentences from the end of the prompt utilizing spaCy NLP.\n\n    The \"device\" argument can be used to set the device the prompt upscaler will run any models on,\n    for example: cpu, cuda, cuda:1. this argument will default to the value of the dgenerate\n    argument --device."}}