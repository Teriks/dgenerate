{"bnb": {"bits": {"optional": false, "types": ["int"], "default": 8, "options": [8, 4]}, "bits4-compute-dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32", "float64", "int8", "uint8"]}, "bits4-quant-type": {"optional": false, "types": ["str"], "default": "fp4", "options": ["fp4", "nf4"]}, "bits4-use-double-quant": {"optional": false, "types": ["bool"], "default": false}, "bits4-quant-storage": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32", "float64", "int8", "uint8"]}, "QUANTIZER_HELP": "bnb | bitsandbytes:\n    arguments:\n        bits: int = 8\n        bits4-compute-dtype: str | None = None\n        bits4-quant-type: str = \"fp4\"\n        bits4-use-double-quant: bool = False\n        bits4-quant-storage: str | None = None\n\n    Global quantization configuration via URI.\n\n    This URI specifies the quantization backend and its configuration.\n\n    Quantization will be applied to all text encoders, and unet / transformer models with the\n    provided settings when using this argument.\n\n    If you wish to specify different quantization types per encoder or unet / transformer, you\n    should use the \"quantizer\" URI argument of --text-encoders and or --unet / --transformer to\n    specify the quantization settings on a per model basis.\n\n    Available backends are: (bnb / bitsandbytes, sdnq)\n\n    bitsandbytes can be specified with \"bnb\" or \"bitsandbytes\"\n\n    Example:\n\n    --quantizer bnb;bits=4\n\n    or:\n\n    --quantizer bitsandbytes;bits=4\n\n    The bitsandbytes backend URI possesses these arguments and defaults:\n\n    * bits: int = 8 (must be 4 or 8)\n    * bits4-compute-dtype: str = None (auto set when not specified)\n    * bits4-quant-type: str = \"fp4\"\n    * bits4-use-double-quant = False\n    * bits4-quant-storage: str = None\n\n    SDNQ (SD.Next Quantization) backend can be specified with \"sdnq\"\n\n    Example:\n\n    --quantizer sdnq;type=int4\n\n    The SDNQ backend URI possesses these arguments and defaults:\n\n    * type: str = \"int8\"\n    * group-size: int = 0 (how many tensor elements will share a quantization group, must be >= 0)\n    * quant-conv: bool = False (quantize convolutional layers)\n    * quantized-matmul: bool = False (use quantized matrix multiplication)\n    * quantized-matmul-conv: bool = False (use quantized matrix multiplication for convolutional layers)\n\n    SDNQ supports the quantization types:\n\n    * bool\n    * int8, int7, int6, int5, int4, int3, int2\n    * uint8, uint7, uint6, uint5, uint4, uint3, uint2, uint1,\n    * float8_e4m3fn, float8_e4m3fnuz, float8_e5m2, float8_e5m2fnuz"}, "sdnq": {"type": {"optional": false, "types": ["str"], "default": "int8", "options": ["int8", "int7", "int6", "int5", "int4", "int3", "int2", "uint8", "uint7", "uint6", "uint5", "uint4", "uint3", "uint2", "uint1", "bool", "float8_e4m3fn", "float8_e4m3fnuz", "float8_e5m2", "float8_e5m2fnuz"]}, "group-size": {"optional": false, "types": ["int"], "default": 0}, "quant-conv": {"optional": false, "types": ["bool"], "default": false}, "quantized-matmul": {"optional": false, "types": ["bool"], "default": false}, "quantized-matmul-conv": {"optional": false, "types": ["bool"], "default": false}, "QUANTIZER_HELP": "sdnq:\n    arguments:\n        type: str = \"int8\"\n        group-size: int = 0\n        quant-conv: bool = False\n        quantized-matmul: bool = False\n        quantized-matmul-conv: bool = False\n\n    Global quantization configuration via URI.\n\n    This URI specifies the quantization backend and its configuration.\n\n    Quantization will be applied to all text encoders, and unet / transformer models with the\n    provided settings when using this argument.\n\n    If you wish to specify different quantization types per encoder or unet / transformer, you\n    should use the \"quantizer\" URI argument of --text-encoders and or --unet / --transformer to\n    specify the quantization settings on a per model basis.\n\n    Available backends are: (bnb / bitsandbytes, sdnq)\n\n    bitsandbytes can be specified with \"bnb\" or \"bitsandbytes\"\n\n    Example:\n\n    --quantizer bnb;bits=4\n\n    or:\n\n    --quantizer bitsandbytes;bits=4\n\n    The bitsandbytes backend URI possesses these arguments and defaults:\n\n    * bits: int = 8 (must be 4 or 8)\n    * bits4-compute-dtype: str = None (auto set when not specified)\n    * bits4-quant-type: str = \"fp4\"\n    * bits4-use-double-quant = False\n    * bits4-quant-storage: str = None\n\n    SDNQ (SD.Next Quantization) backend can be specified with \"sdnq\"\n\n    Example:\n\n    --quantizer sdnq;type=int4\n\n    The SDNQ backend URI possesses these arguments and defaults:\n\n    * type: str = \"int8\"\n    * group-size: int = 0 (how many tensor elements will share a quantization group, must be >= 0)\n    * quant-conv: bool = False (quantize convolutional layers)\n    * quantized-matmul: bool = False (use quantized matrix multiplication)\n    * quantized-matmul-conv: bool = False (use quantized matrix multiplication for convolutional layers)\n\n    SDNQ supports the quantization types:\n\n    * bool\n    * int8, int7, int6, int5, int4, int3, int2\n    * uint8, uint7, uint6, uint5, uint4, uint3, uint2, uint1,\n    * float8_e4m3fn, float8_e4m3fnuz, float8_e5m2, float8_e5m2fnuz"}}