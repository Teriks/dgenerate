{"bnb": {"bits": {"optional": false, "types": ["int"], "default": 8, "options": [8, 4]}, "bits4-compute-dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32", "float64", "int8", "uint8"]}, "bits4-quant-type": {"optional": false, "types": ["str"], "default": "fp4", "options": ["fp4", "nf4"]}, "bits4-use-double-quant": {"optional": false, "types": ["bool"], "default": false}, "bits4-quant-storage": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32", "float64", "int8", "uint8"]}, "QUANTIZER_HELP": "bnb | bitsandbytes:\n    arguments:\n        bits: int = 8\n        bits4-compute-dtype: str | None = None\n        bits4-quant-type: str = \"fp4\"\n        bits4-use-double-quant: bool = False\n        bits4-quant-storage: str | None = None\n\n    Bitsandbytes quantization backend configuration.\n\n    This backend can be specified as \"bnb\" or \"bitsandbytes\" in the URI.\n\n    URI Format: bnb;argument1=value1;argument2=value2\n\n    Example: bnb;bits=4;bits4-quant-type=nf4\n\n    The argument \"bits\" is Quantization bit width. Must be 4 or 8.\n\n      - bits=8: Uses LLM.int8() quantization method\n      - bits=4: Uses QLoRA 4-bit quantization method\n\n    The argument \"bits4-compute-dtype\" is the compute data type for 4-bit quantization. Only applies\n    when bits=4. When None, automatically determined. This should generally match the dtype that you\n    loaded the model with.\n\n    The argument \"bits4-quant-type\" is the quantization data type for 4-bit weights. Only applies\n    when bits=4.\n\n      - \"fp4\": 4-bit floating point (default)\n      - \"nf4\": Normal Float 4 data type, adapted for weights from normal distribution.\n\n    The argument \"bits4-use-double-quant\" Enables nested quantization for 4-bit mode. Only applies\n    when bits=4. When True, performs a second quantization of already quantized weights to save an\n    additional 0.4 bits/parameter with no performance cost.\n\n    The argument \"bits4-quant-storage\" is the storage data type for 4-bit quantized weights. Only\n    applies when bits=4. When None, uses default storage format. Controls memory layout of quantized\n    parameters."}, "sdnq": {"type": {"optional": false, "types": ["str"], "default": "int8", "options": ["int8", "int7", "int6", "int5", "int4", "int3", "int2", "uint8", "uint7", "uint6", "uint5", "uint4", "uint3", "uint2", "uint1", "bool", "float8_e4m3fn", "float8_e4m3fnuz", "float8_e5m2", "float8_e5m2fnuz"]}, "group-size": {"optional": false, "types": ["int"], "default": 0}, "quant-conv": {"optional": false, "types": ["bool"], "default": false}, "quantized-matmul": {"optional": false, "types": ["bool"], "default": false}, "quantized-matmul-conv": {"optional": false, "types": ["bool"], "default": false}, "QUANTIZER_HELP": "sdnq:\n    arguments:\n        type: str = \"int8\"\n        group-size: int = 0\n        quant-conv: bool = False\n        quantized-matmul: bool = False\n        quantized-matmul-conv: bool = False\n\n    SD.Next quantization backend configuration.\n\n    This backend can be specified as \"sdnq\" in the URI.\n\n    URI Format: sdnq;argument1=value1;argument2=value2\n\n    Example: sdnq;type=int4;group-size=8;quant-conv=true\n\n    The argument \"type\" is the target data type for weights after quantization.\n\n    Integer types:\n      - int8 (default),\n      - int7\n      - int6\n      - int5\n      - int4\n      - int3\n      - int2\n    \n    Unsigned integer types:\n      - uint8\n      - uint7\n      - uint6\n      - uint5\n      - uint4\n      - uint3\n      - uint2\n      - uint1\n      - bool\n    \n    Floating point types:\n      - float8_e4m3fn\n      - float8_e4m3fnuz\n      - float8_e5m2\n      - float8_e5m2fnuz\n\n    The argument \"group-size\" is used to decide how many elements of a tensor will share the same\n    quantization group. Must be >= 0. When 0 (default), uses per-tensor quantization. When > 0,\n    groups tensor elements for more granular quantization scaling.\n\n    The argument \"quant-conv\" is enables quantization of convolutional layers in UNet models. When\n    True, quantizes Conv2d layers in addition to Linear layers. Only affects UNet architectures.\n\n    The argument \"quantized-matmul\" is enables use of quantized INT8 or FP8 matrix multiplication\n    instead of BF16/FP16. When True, uses optimized quantized matmul operations for improved\n    performance and reduced memory usage.\n\n    The argument \"quantized-matmul-conv\" is enables quantized matrix multiplication for\n    convolutional layers. Same as quantized-matmul but specifically for convolutional layers in\n    UNets like SDXL."}}