{"--verbose": "Output information useful for debugging, such as pipeline call and model load parameters.", "--version": "Show dgenerate's version and exit", "--file": "Convenience argument for reading a configuration script from a file instead of using a pipe. This is\na meta argument which can not be used within a configuration script and is only valid from the\ncommand line or during a popen invocation of dgenerate. This argument understands glob syntax, even\non windows, and can accept multiple config file names, which will be executed in sequence.", "--shell": "When reading configuration from STDIN (a pipe), read forever, even when configuration errors occur.\nThis allows dgenerate to run in the background and be controlled by another process sending\ncommands. Launching dgenerate with this option and not piping it input will attach it to the\nterminal like a shell. Entering configuration into this shell requires two newlines to submit a\ncommand due to parsing lookahead. IE: two presses of the enter key. This is a meta argument which\ncan not be used within a configuration script and is only valid from the command line or during a\npopen invocation of dgenerate.", "--no-stdin": "Can be used to indicate to dgenerate that it will not receive any piped in input. This is useful for\nrunning dgenerate via popen from Python or another application using normal arguments, where it\nwould otherwise try to read from STDIN and block forever because it is not attached to a terminal.\nThis is a meta argument which can not be used within a configuration script and is only valid from\nthe command line or during a popen invocation of dgenerate.", "--console": "Launch a terminal-like Tkinter GUI that interacts with an instance of dgenerate running in the\nbackground. This allows you to interactively write dgenerate config scripts as if dgenerate were a\nshell / REPL. This is a meta argument which can not be used within a configuration script and is\nonly valid from the command line or during a popen invocation of dgenerate.", "--set": "Set template variables that will be applied before config execution. Mirrors the functionality of\nthe \\set config directive. Must use the syntax --set variable=value [variable2=value2 ...]. Can\naccept multiple variable=value pairs and can be used multiple times. All --set and --setp arguments\nare processed in the order they appear on the command line. This is a meta argument which can not be\nused within a configuration script and is only valid from the command line or during a popen\ninvocation of dgenerate.", "--setp": "Set template variables to the result of evaluating python expressions that will be applied before\nconfig execution. Mirrors the functionality of the \\setp config directive. Must use the syntax\n--setp variable=expression [variable2=expression2 ...]. Can accept multiple variable=expression\npairs and can be used multiple times. All --set and --setp arguments are processed in the order they\nappear on the command line. This is a meta argument which can not be used within a configuration\nscript and is only valid from the command line or during a popen invocation of dgenerate.", "--plugin-modules": "Specify one or more plugin module folder paths (folder containing __init__.py) or Python .py file\npaths, or Python module names to load as plugins. Plugin modules can currently implement image\nprocessors, config directives, config template functions, prompt weighters, and sub-commands.", "--sub-command": "Specify the name a sub-command to invoke. dgenerate exposes some extra image processing\nfunctionality through the use of sub-commands. Sub commands essentially replace the entire set of\naccepted arguments with those of a sub-command which implements additional functionality. See\n--sub-command-help for a list of sub-commands and help.", "--sub-command-help": "Use this option alone (or with --plugin-modules) and no model specification in order to list\navailable sub-command names. Calling a sub-command with \"--sub-command name --help\" will produce\nargument help output for that sub-command. When used with --plugin-modules, sub-commands implemented\nby the specified plugins will also be listed.", "--offline-mode": "Prevent dgenerate from downloading resources that do not already exist on disk. Referencing a model\non Hugging Face hub that has not been cached because it was not previously downloaded will result in\na failure when using this option, as well as attempting to download any new content into dgenerates\nweb cache.  This will prevent dgenerate from downloading anything, it will only look for cached\nresources when processing URLs or Hugging Face slugs. It will not be able to download any default\nmodels that have been baked into the code as well. This option is fed to sub-commands when using the\n--sub-command argument, meaning that all sub-commands can parse this argument by default, though\nthey may complain if it is not supported, such as with the \"civitai-links\" sub-command.", "--templates-help": "Print a list of template variables available in the interpreter environment used for dgenerate\nconfig scripts, particularly the variables set after a dgenerate invocation occurs. When used as a\ncommand line option, their values are not presented, just their names and types. Specifying names\nwill print type information for those variable names.", "--directives-help": "Use this option alone (or with --plugin-modules) and no model specification in order to list\navailable config directive names. Providing names will print documentation for the specified\ndirective names. When used with --plugin-modules, directives implemented by the specified plugins\nwill also be listed.", "--functions-help": "Use this option alone (or with --plugin-modules) and no model specification in order to list\navailable config template function names. Providing names will print documentation for the specified\nfunction names. When used with --plugin-modules, functions implemented by the specified plugins will\nalso be listed.", "--global-config": "Provide a json, yaml, or toml file to configure dgenerate's global settings. These settings include\nvarious default values for generation and garbage collection settings for the in memory caches.", "--model-type": "Use when loading different model types. Currently supported: sd, pix2pix, sdxl, sdxl-pix2pix,\nkolors, upscaler-x2, upscaler-x4, if, ifs, ifs-img2img, s-cascade, sd3, flux, or flux-fill.\n(default: sd)", "--revision": "The model revision to use when loading from a Hugging Face repository, (The Git branch / tag,\ndefault is \"main\")", "--variant": "If specified when loading from a Hugging Face repository or folder, load weights from \"variant\"\nfilename, e.g. \"pytorch_model.<variant>.safetensors\". Defaults to automatic selection.", "--subfolder": "Main model subfolder. If specified when loading from a Hugging Face repository or folder, load\nweights from the specified subfolder.", "--original-config": "This argument can be used to supply an original LDM config .yaml file that was provided with a\nsingle file checkpoint.", "--second-model-original-config": "This argument can be used to supply an original LDM config .yaml file that was provided with a\nsingle file checkpoint for the secondary model, i.e. the SDXL Refiner or Stable Cascade Decoder.", "--auth-token": "Huggingface auth token. Required to download restricted repositories that have access permissions\ngranted to your Hugging Face account.", "--batch-size": "The number of image variations to produce per set of individual diffusion parameters in one\nrendering step simultaneously on a single GPU.\n\nWhen generating animations with a --batch-size greater than one, a separate animation (with the\nfilename suffix \"animation_N\") will be written to for each image in the batch.\n\nIf --batch-grid-size is specified when producing an animation then the image grid is used for the\noutput frames.\n\nDuring animation rendering each image in the batch will still be written to the output directory\nalong side the produced animation as either suffixed files or image grids depending on the options\nyou choose. (Default: 1)", "--batch-grid-size": "Produce a single image containing a grid of images with the number of COLUMNSxROWS given to this\nargument when --batch-size is greater than 1. If not specified with a --batch-size greater than 1,\nimages will be written individually with an image number suffix (image_N) in the filename signifying\nwhich image in the batch they are.", "--adetailer-detectors": "Specify one or more adetailer YOLO detector model URIs. When specifying this option, you must\nprovide an image to --image-seeds, inpaint masks will be auto generated based on what is detected by\nthe provided detector models.\n\nThe models will be used in sequence to detect and then inpaint your image within the detection\nareas. This can be used for face detailing, face swapping, hand detailing, etc. on any arbitrary\nimage provided using an image generation model of your choice.\n\nThis option supports: --model-type sd, sdxl, kolors, sd3, flux, and flux-fill\n\nExample: --adetailer-detectors Bingsu/adetailer;weight-name=face_yolov8n.pt\n\nThe \"revision\" argument specifies the model revision to use for the adetailer model when loading\nfrom Hugging Face repository, (The Git branch / tag, default is \"main\").\n\nThe \"subfolder\" argument specifies the adetailer model subfolder, if specified when loading from a\nHugging Face repository or folder, weights from the specified subfolder.\n\nThe \"weight-name\" argument indicates the name of the weights file to be loaded when loading from a\nHugging Face repository or folder on disk.\n\nThe \"class-filter\" (overrides --adetailer-class-filter) argument is a list of class IDs or class\nnames that indicates what YOLO detection classes to keep. This filter is applied first, before\nindex-filter. Detections that don't match any of the specified classes will be ignored.\n\nExample \"class-filter\" values:\n\n    * Only keep detection class ID 0:\n    class-filter=0\n\n    * Only keep detection class \"hand\":\n    class-filter=hand\n\n    * Keep class IDs 2 and 3:\n    class-filter=2,3\n\n    * Keep class ID 0 and class name \"hand\":\n    class-filter=0,hand\n\n    * String digits are interpreted as integers:\n    class-filter=\"0\" (interpreted as class name \"0\", not likely useful)\n\n    * List syntax is also supported:\n    class-filter=[0, \"hand\"]\n\nThe \"index-filter\" (overrides --adetailer-index-filter) argument is a list values or a\nsingle value that indicates what YOLO detection indices to keep, the index values start\nat zero. Detections are sorted by their top left bounding box coordinate from left to right,\ntop to bottom, by (confidence descending). The order of detections in the image is identical to\nthe reading order of words on a page (english). Inpainting will only be performed on the\nspecified detection indices, if no indices are specified, then inpainting\nwill be performed on all detections. This filter is applied after class-filter.\n\nExample \"index-filter\" values:\n\n    * keep the first, leftmost, topmost detection:\n    index-filter=0\n\n    * keep detections 1 and 3:\n    index-filter=[1, 3]\n\n    * CSV syntax is supported (tuple):\n    index-filter=1,3\n\nThe \"detector-padding\" (overrides --adetailer-detector-paddings) argument specifies the amount of\npadding that will be added to the detection rectangle which is used to generate a masked area. The\ndefault is 0, you can make the mask area around the detected feature larger with positive padding\nand smaller with negative padding.\n\nPadding examples:\n\n    32 (32px Uniform, all sides)\n\n    10x20 (10px Horizontal, 20px Vertical)\n\n    10x20x30x40 (10px Left, 20px Top, 30px Right, 40px Bottom)\n\nThe \"mask-padding\" (overrides --adetailer-mask-paddings) argument indicates how much padding to\nplace around the masked area when cropping out the image to be inpainted. This value must be large\nenough to accommodate any feathering on the edge of the mask caused by \"mask-blur\" or\n\"mask-dilation\" for the best result, the default value is 32. The syntax for specifying this value\nis identical to \"detector-padding\".\n\nThe \"mask-shape\" (overrides --adetailer-mask-shapes) argument indicates what mask shape adetailer\nshould attempt to draw around a detected feature, the default value is \"rectangle\". You may also\nspecify \"circle\" to generate an ellipsoid shaped mask, which might be helpful for achieving better\nblending. Valid values are: (\"r\", \"rect\", \"rectangle\"), or (\"c\", \"circle\", \"ellipse\").\n\nThe \"mask-blur\" (overrides --adetailer-mask-blurs) argument indicates the level of gaussian blur to\napply to the generated inpaint mask, which can help with smooth blending in of the inpainted feature\n\nThe \"model-masks\" (overrides --adetailer-model-masks) argument indicates that masks generated by the\nmodel itself should be preferred over masks generated from the detection bounding box. If this is\nTrue, and the model itself returns mask data, \"mask-shape\", \"mask-padding\", and \"detector-padding\"\nwill all be ignored.\n\nThe \"mask-dilation\" (overrides --adetailer-mask-dilations) argument indicates the amount of dilation\napplied to the inpaint mask, see: cv2.dilate\n\nThe \"confidence\" argument indicates the confidence value to use with the YOLO detector model, this\nvalue defaults to 0.3 if not specified.\n\nThe \"prompt\" (overrides --prompt positive) argument overrides the positive inpainting prompt for\ndetections by this detector.\n\nThe \"negative-prompt\" (overrides --prompt negative) argument overrides the negative inpainting\nprompt for detections by this detector.\n\nThe \"device\" argument indicates a device override for the YOLO detector model, the detector model\ncan be set to run on a different device if desired, for example: cuda:0, cuda:1, cpu, etc. It runs\non the same device as --device by default.\n\nIf you wish to load a weights file directly from disk, use: --adetailer-detectors \"yolo_model.pt\"\n\nYou may also load a YOLO model directly from a URL or Hugging Face blob link.\n\nExample: --adetailer-detectors https://modelsite.com/yolo-model.pt", "--adetailer-model-masks": "Indicates that masks generated by the model itself should be preferred over masks generated from the\ndetection bounding box. If this is specified, and the model itself returns mask data,\n--adetailer-mask-shapes, --adetailer-mask-paddings, and --adetailer-detector-paddings will all be\nignored.", "--adetailer-class-filter": "A list of class IDs or class names that indicates what YOLO detection classes to keep. This filter\nis applied before index-filter. Detections that don't match any of the specified classes will be\nignored. This filtering occurs before --adetailer-index-filter.\n\nExamples:\n--adetailer-class-filter 0 2        # Keep only class IDs 0 and 2\n--adetailer-class-filter person car # Keep only \"person\" and \"car\" classes\n--adetailer-class-filter 0 person   # Keep class ID 0 and class name \"person\"", "--adetailer-index-filter": "A list index values that indicates what adetailer YOLO detection indices to keep, the index values\nstart at zero. Detections are sorted by their top left bounding box coordinate from left to right,\ntop to bottom, by (confidence descending). The order of detections in the image is identical to the\nreading order of words on a page (english). Inpainting will only be performed on the specified\ndetection indices, if no indices are specified, then inpainting will be performed on all detections.\nThis filter is applied after class-filter.", "--adetailer-mask-shapes": "One or more adetailer mask shapes to try. This indicates what mask shape adetailer should attempt to\ndraw around a detected feature, the default value is \"rectangle\". You may also specify \"circle\" to\ngenerate an ellipsoid shaped mask, which might be helpful for achieving better blending.\n\nValid values are: (\"r\", \"rect\", \"rectangle\"), or (\"c\", \"circle\", \"ellipse\")\n\n(default: rectangle).", "--adetailer-detector-paddings": "One or more adetailer detector padding values to try. This value specifies the amount of padding\nthat will be added to the detection rectangle which is used to generate a masked area. The default\nis 0, you can make the mask area around the detected feature larger with positive padding and\nsmaller with negative padding.\n\nExample:\n\n32 (32px Uniform, all sides)\n\n10x20 (10px Horizontal, 20px Vertical)\n\n10x20x30x40 (10px Left, 20px Top, 30px Right, 40px Bottom)\n\n(default: 0).", "--adetailer-mask-paddings": "One or more adetailer mask padding values to try. This value indicates how much padding to place\naround the masked area when cropping out the image to be inpainted, this value must be large enough\nto accommodate any feathering on the edge of the mask caused by \"--adetailer-mask-blurs\" or\n\"--adetailer-mask-dilations\" for the best result.\n\nExample:\n\n32 (32px Uniform, all sides)\n\n10x20 (10px Horizontal, 20px Vertical)\n\n10x20x30x40 (10px Left, 20px Top, 30px Right, 40px Bottom)\n\n(default: 32).", "--adetailer-mask-blurs": "The level of gaussian blur to apply to the generated adetailer inpaint mask, which can help with\nsmooth blending in of the inpainted feature. (default: 4)", "--adetailer-mask-dilations": "The amount of dilation applied to the adetailer inpaint mask, see: cv2.dilate. (default: 4)", "--adetailer-crop-control-image": "Should adetailer crop ControlNet control images to the feature detection area? Your input image and\ncontrol image should be the the same dimension, otherwise this argument is ignored with a warning.\nWhen this argument is not specified, the control image provided is simply resized to the same size\nas the detection area.", "--text-encoders": "Specify Text Encoders for the main model using URIs, main models may use one or more text encoders\ndepending on the --model-type value and other dgenerate arguments. See: --text-encoders help for\ninformation about what text encoders are needed for your invocation.\n\nExamples:\n\n\"CLIPTextModel;model=huggingface/text_encoder\"\n\"CLIPTextModelWithProjection;model=huggingface/text_encoder;revision=main\"\n\"T5EncoderModel;model=text_encoder_folder_on_disk\"\n\"DistillT5EncoderModel;model=text_encoder_folder_on_disk\"\n\nFor main models which require multiple text encoders, the + symbol may be used to indicate that a\ndefault value should be used for a particular text encoder, for example: --text-encoders + +\nhuggingface/encoder3. Any trailing text encoders which are not specified are given their default\nvalue.\n\nThe value \"null\" may be used to indicate that a specific text encoder should not be loaded.\n\nThe \"revision\" argument specifies the model revision to use for the Text Encoder when loading from\nHugging Face repository, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the Text Encoder model variant. If \"variant\" is specified when\nloading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename,\ne.g. \"pytorch_model.<variant>.safetensors\". For this argument, \"variant\" defaults to the value of\n--variant if it is not specified in the URI.\n\nThe \"subfolder\" argument specifies the Text Encoder model subfolder, if specified when loading from\na Hugging Face repository or folder, weights from the specified subfolder.\n\nThe \"dtype\" argument specifies the Text Encoder model precision, it defaults to the value of\n-t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\nThe \"quantizer\" URI argument can be used to specify a quantization backend for the text encoder\nusing the same URI syntax as --quantizer. This is supported when loading from Hugging Face repo\nslugs / folders on disk, and when using the \"mode\" argument with monolithic (non-sharded)\ncheckpoints. This is not supported when loading a submodule out of a combined checkpoint file with\n\"subfolder\". If working from the command line you may need to nested quote this URI, i.e:\n\n--text-encoders 'CLIPTextModel;model=huggingface/text_encoder;quantizer=\"bnb;bits=8\"'\n\nThe \"mode\" argument can be used to load monolithic single file checkpoints with specific\narchitecture configurations. Available modes are:\n\nFlux & T5 universal modes:\n\n* \"clip-l\" for monolithic Flux CLIP-L checkpoints\n* \"t5-xxl\" for monolithic Flux T5 checkpoints\n\nSD3 and SD3.5 specific modes:\n\n* \"clip-l-sd3\" for SD3/SD3.5 medium CLIP-L checkpoints\n* \"clip-g-sd3\" for SD3/SD3.5 medium CLIP-G checkpoints\n* \"t5-xxl-sd3\" for SD3/SD3.5 T5-XXL checkpoints\n* \"clip-l-sd35-large\" for SD3.5 large variant CLIP-L checkpoints\n* \"clip-g-sd35-large\" for SD3.5 large variant CLIP-G checkpoints\n\n The \"mode\" option is mutually exclusive with \"subfolder\".\n\nAvailable encoder classes are:\n\n* CLIPTextModel\n* CLIPTextModelWithProjection\n* T5EncoderModel\n* DistillT5EncoderModel (see: LifuWang/DistillT5)\n* ChatGLMModel (for Kolors models)\n\nIf you wish to load weights directly from a path on disk, you must point this argument at the folder\nthey exist in, which should also contain the config.json file for the Text Encoder. For example, a\ndownloaded repository folder from Hugging Face.", "--second-model-text-encoders": "--text-encoders but for the SDXL refiner or Stable Cascade decoder model.", "--unet": "Specify a UNet using a URI.\n\nExamples:\n\n\"huggingface/unet\", \"huggingface/unet;revision=main\", \"unet_folder_on_disk\"\n\nThe \"revision\" argument specifies the model revision to use for the UNet when loading from Hugging\nFace repository, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the UNet model variant. If \"variant\" is specified when loading from\na Hugging Face repository or folder, weights will be loaded from \"variant\" filename, e.g.\n\"pytorch_model.<variant>.safetensors. For this argument, \"variant\" defaults to the value of\n--variant if it is not specified in the URI.\n\nThe \"subfolder\" argument specifies the UNet model subfolder, if specified when loading from a\nHugging Face repository or folder, weights from the specified subfolder. If you are loading from a\ncombined single file checkpoint containing multiple components, this value will be used to determine\nthe key in the checkpoint that contains the unet, by default \"unet\" is used if subfolder is not\nprovided.\n\nThe \"dtype\" argument specifies the UNet model precision, it defaults to the value of -t/--dtype and\nshould be one of: auto, bfloat16, float16, or float32.\n\nThe \"quantizer\" argument specifies a quantization backend and configuration for the UNet model\nindividually, and uses the same URI syntax as --quantizer. If working from the command line you may\nneed to nested quote this URI, i.e:\n\n--unet 'huggingface/unet;quantizer=\"bnb;bits=8\"'\n\nIf you wish to load weights directly from a path on disk, you must point this argument at the folder\nthey exist in, which should also contain the config.json file for the UNet. For example, a\ndownloaded repository folder from Hugging Face.", "--second-model-unet": "Specify a second UNet, this is only valid when using SDXL or Stable Cascade model types. This UNet\nwill be used for the SDXL refiner, or Stable Cascade decoder model.", "--transformer": "Specify a Stable Diffusion 3 or Flux Transformer model using a URI.\n\nExamples:\n\n\"huggingface/transformer\"\n\"huggingface/transformer;revision=main\"\n\"transformer_folder_on_disk\"\n\nBlob links / single file loads are supported for SD3 Transformers.\n\nThe \"revision\" argument specifies the model revision to use for the Transformer when loading from\nHugging Face repository or blob link, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the Transformer model variant. If \"variant\" is specified when\nloading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename,\ne.g. \"pytorch_model.<variant>.safetensors. For this argument, \"variant\" defaults to the value of\n--variant if it is not specified in the URI.\n\nThe \"subfolder\" argument specifies the Transformer model subfolder, if specified when loading from a\nHugging Face repository or folder, weights from the specified subfolder.\n\nThe \"dtype\" argument specifies the Transformer model precision, it defaults to the value of\n-t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\nThe \"quantizer\" argument specifies a quantization backend and configuration for the Transformer\nmodel individually, and uses the same URI syntax as --quantizer. If working from the command line\nyou may need to nested quote this URI, i.e:\n\n--transformer 'huggingface/transformer;quantizer=\"bnb;bits=8\"'\n\nIf you wish to load a weights file directly from disk, the simplest way is: --transformer\n\"transformer.safetensors\", or with a dtype \"transformer.safetensors;dtype=float16\". All loading\narguments except \"dtype\" and \"quantizer\" are unused in this case and may produce an error message if\nused.\n\nIf you wish to load a specific weight file from a Hugging Face repository, use the blob link loading\nsyntax: --transformer\n\"AutoencoderKL;https://huggingface.co/UserName/repository-name/blob/main/transformer.safetensors\",\nthe \"revision\" argument may be used with this syntax.", "--vae": "Specify a VAE using a URI, the URI syntax is: \"AutoEncoderClass;model=(Hugging Face repository\nslug/blob link or file/folder path)\".\n\nExamples:\n\n\"AutoencoderKL;model=vae.pt\"\n\"AsymmetricAutoencoderKL;model=huggingface/vae\"\n\"AutoencoderTiny;model=huggingface/vae\"\n\"ConsistencyDecoderVAE;model=huggingface/vae\"\n\nThe AutoencoderKL encoder class accepts Hugging Face repository slugs/blob links, .pt, .pth, .bin,\n.ckpt, and .safetensors files.\n\nOther encoders can only accept Hugging Face repository slugs/blob links, or a path to a folder on\ndisk with the model configuration and model file(s).\n\nIf an AutoencoderKL VAE model file exists at a URL which serves the file as a raw download, you may\nprovide an http/https link to it and it will be downloaded to dgenerate's web cache.\n\nAside from the \"model\" argument, there are four other optional arguments that can be specified,\nthese are: \"revision\", \"variant\", \"subfolder\", \"dtype\".\n\nThey can be specified as so in any order, they are not positional:\n\n\"AutoencoderKL;model=huggingface/vae;revision=main;variant=fp16;subfolder=sub_folder;dtype=float16\"\n\nThe \"revision\" argument specifies the model revision to use for the VAE when loading from Hugging\nFace repository or blob link, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the VAE model variant. If \"variant\" is specified when loading from\na Hugging Face repository or folder, weights will be loaded from \"variant\" filename, e.g.\n\"pytorch_model.<variant>.safetensors. \"variant\" in the case of --vae does not default to the value\nof --variant to prevent failures during common use cases.\n\nThe \"subfolder\" argument specifies the VAE model subfolder, if specified when loading from a Hugging\nFace repository or folder, weights from the specified subfolder.\n\nThe \"extract\" argument specifies that \"model\" points at a combind single file checkpoint containing\nmultiple components such as the UNet and Text Encoders, and that we should extract the VAE. When\nusing this argument you can use \"subfolder\" to indicate the key in the checkpoint containing the\nmodel, this defaults to \"vae\".\n\nThe \"dtype\" argument specifies the VAE model precision, it defaults to the value of -t/--dtype and\nshould be one of: auto, bfloat16, float16, or float32.\n\nIf you wish to load a weights file directly from disk, the simplest way is: --vae\n\"AutoencoderKL;my_vae.safetensors\", or with a dtype\n\"AutoencoderKL;my_vae.safetensors;dtype=float16\". All loading arguments except \"dtype\" are unused in\nthis case and may produce an error message if used.\n\nIf you wish to load a specific weight file from a Hugging Face repository, use the blob link loading\nsyntax: --vae\n\"AutoencoderKL;https://huggingface.co/UserName/repository-name/blob/main/vae_model.safetensors\", the\n\"revision\" argument may be used with this syntax.", "--vae-tiling": "Enable VAE tiling. Assists in the generation of large images with lower memory overhead. The VAE\nwill split the input tensor into tiles to compute decoding and encoding in several steps. This is\nuseful for saving a large amount of memory and to allow processing larger images. Note that if you\nare using --control-nets you may still run into memory issues generating large images, or with\n--batch-size greater than 1.", "--vae-slicing": "Enable VAE slicing. Assists in the generation of large images with lower memory overhead. The VAE\nwill split the input tensor in slices to compute decoding in several steps. This is useful to save\nsome memory, especially when --batch-size is greater than 1. Note that if you are using\n--control-nets you may still run into memory issues generating large images.", "--loras": "Specify one or more LoRA models using URIs. These should be a Hugging Face repository slug / blob\nlink, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file), or\nmodel folder containing model files.\n\nIf a LoRA model file exists at a URL which serves the file as a raw download, you may provide an\nhttp/https link to it and it will be downloaded to dgenerate's web cache.\n\nOptional arguments can be provided after a LoRA model specification, these are: \"scale\", \"revision\",\n\"subfolder\", and \"weight-name\".\n\nThey can be specified as so in any order, they are not positional:\n\n\"huggingface/lora;scale=1.0;revision=main;subfolder=repo_subfolder;weight-name=lora.safetensors\"\n\nThe \"scale\" argument indicates the scale factor of the LoRA.\n\nThe \"revision\" argument specifies the model revision to use for the LoRA when loading from Hugging\nFace repository, (The Git branch / tag, default is \"main\").\n\nThe \"subfolder\" argument specifies the LoRA model subfolder, if specified when loading from a\nHugging Face repository or folder, weights from the specified subfolder.\n\nThe \"weight-name\" argument indicates the name of the weights file to be loaded when loading from a\nHugging Face repository or folder on disk.\n\nIf you wish to load a weights file directly from disk, the simplest way is: --loras\n\"my_lora.safetensors\", or with a scale \"my_lora.safetensors;scale=1.0\", all other loading arguments\nare unused in this case and may produce an error message if used.", "--lora-fuse-scale": "LoRA weights are merged into the main model at this scale. When specifying multiple LoRA models,\nthey are fused together into one set of weights using their individual scale values, after which\nthey are fused into the main model at this scale value. (default: 1.0).", "--image-encoder": "Specify an Image Encoder using a URI.\n\nImage Encoders are used with --ip-adapters models, and must be specified if none of the loaded\n--ip-adapters contain one. An error will be produced in this situation, which requires you to use\nthis argument.\n\nAn image encoder can also be manually specified for Stable Cascade models.\n\nExamples:\n\n\"huggingface/image_encoder\"\n\"huggingface/image_encoder;revision=main\"\n\"image_encoder_folder_on_disk\"\n\nBlob links / single file loads are not supported for Image Encoders.\n\nThe \"revision\" argument specifies the model revision to use for the Image Encoder when loading from\nHugging Face repository or blob link, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the Image Encoder model variant. If \"variant\" is specified when\nloading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename,\ne.g. \"pytorch_model.<variant>.safetensors.\n\nSimilar to --vae, \"variant\" does not default to the value of --variant in order to prevent errors\nwith common use cases. If you specify multiple IP Adapters, they must all have the same \"variant\"\nvalue or you will receive a usage error.\n\nThe \"subfolder\" argument specifies the Image Encoder model subfolder, if specified when loading from\na Hugging Face repository or folder, weights from the specified subfolder.\n\nThe \"dtype\" argument specifies the Image Encoder model precision, it defaults to the value of\n-t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\nIf you wish to load weights directly from a path on disk, you must point this argument at the folder\nthey exist in, which should also contain the config.json file for the Image Encoder. For example, a\ndownloaded repository folder from Hugging Face.", "--ip-adapters": "Specify one or more IP Adapter models using URIs. These should be a Hugging Face repository slug /\nblob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file),\nor model folder containing model files.\n\nIf an IP Adapter model file exists at a URL which serves the file as a raw download, you may provide\nan http/https link to it and it will be downloaded to dgenerate's web cache.\n\nOptional arguments can be provided after an IP Adapter model specification, these are: \"scale\",\n\"revision\", \"subfolder\", and \"weight-name\".\n\nThey can be specified as so in any order, they are not positional:\n\n\"huggingface/ip-adapter;scale=1.0;revision=main;subfolder=repo_subfolder;weight-name=ip_adapter.safetensors\".\n\nThe \"scale\" argument indicates the scale factor of the IP Adapter.\n\nThe \"revision\" argument specifies the model revision to use for the IP Adapter when loading from\nHugging Face repository, (The Git branch / tag, default is \"main\").\n\nThe \"subfolder\" argument specifies the IP Adapter model subfolder, if specified when loading from a\nHugging Face repository or folder, weights from the specified subfolder.\n\nThe \"weight-name\" argument indicates the name of the weights file to be loaded when loading from a\nHugging Face repository or folder on disk.\n\nIf you wish to load a weights file directly from disk, the simplest way is: --ip-adapters\n\"ip_adapter.safetensors\", or with a scale \"ip_adapter.safetensors;scale=1.0\", all other loading\narguments are unused in this case and may produce an error message if used.", "--textual-inversions": "Specify one or more Textual Inversion models using URIs. These should be a Hugging Face repository\nslug / blob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors\nfile), or model folder containing model files.\n\nIf a Textual Inversion model file exists at a URL which serves the file as a raw download, you may\nprovide an http/https link to it and it will be downloaded to dgenerate's web cache.\n\nOptional arguments can be provided after the Textual Inversion model specification, these are:\n\"token\", \"revision\", \"subfolder\", and \"weight-name\".\n\nThey can be specified as so in any order, they are not positional:\n\n\"huggingface/ti_model;revision=main;subfolder=repo_subfolder;weight-name=ti_model.safetensors\".\n\nThe \"token\" argument can be used to override the prompt token used for the textual inversion prompt\nembedding. For normal Stable Diffusion the default token value is provided by the model itself, but\nfor Stable Diffusion XL and Flux the default token value is equal to the model file name with no\nextension and all spaces replaced by underscores.\n\nThe \"revision\" argument specifies the model revision to use for the Textual Inversion model when\nloading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n\nThe \"subfolder\" argument specifies the Textual Inversion model subfolder, if specified when loading\nfrom a Hugging Face repository or folder, weights from the specified subfolder.\n\nThe \"weight-name\" argument indicates the name of the weights file to be loaded when loading from a\nHugging Face repository or folder on disk.\n\nIf you wish to load a weights file directly from disk, the simplest way is: --textual-inversions\n\"my_ti_model.safetensors\", all other loading arguments are unused in this case and may produce an\nerror message if used.", "--control-nets": "Specify one or more ControlNet models using URIs. This should be a Hugging Face repository slug /\nblob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file),\nor model folder containing model files.\n\nIf a ControlNet model file exists at a URL which serves the file as a raw download, you may provide\nan http/https link to it and it will be downloaded to dgenerate's web cache.\n\nOptional arguments can be provided after the ControlNet model specification, these are: \"scale\",\n\"start\", \"end\", \"mode\", \"revision\", \"variant\", \"subfolder\", and \"dtype\".\n\nThey can be specified as so in any order, they are not positional:\n\n\"huggingface/controlnet;scale=1.0;start=0.0;end=1.0;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n\nThe \"scale\" argument specifies the scaling factor applied to the ControlNet model, the default value\nis 1.0.\n\nThe \"start\" argument specifies at what fraction of the total inference steps to begin applying the\nControlNet, defaults to 0.0, IE: the very beginning.\n\nThe \"end\" argument specifies at what fraction of the total inference steps to stop applying the\nControlNet, defaults to 1.0, IE: the very end.\n\nThe \"mode\" argument can be used when using --model-type sdxl / flux and a ControlNet Union model to\nspecify the ControlNet mode. This may be a string or an integer.\n\nFor --model-type sdxl Acceptable \"mode\" values are:\n\n    \"openpose\" = 0\n    \"depth\" = 1\n    \"hed\" = 2\n    \"pidi\" = 2\n    \"scribble\" = 2\n    \"ted\" = 2\n    \"canny\" = 3\n    \"lineart\" = 3\n    \"anime_lineart\" = 3\n    \"mlsd\" = 3\n    \"normal\" = 4\n    \"segment\" = 5\n\n\nFor --model-type flux Acceptable \"mode\" values are:\n\n    \"canny\" = 0\n    \"tile\" = 1\n    \"depth\" = 2\n    \"blur\" = 3\n    \"pose\" = 4\n    \"gray\" = 5\n    \"lq\" = 6\n\nThe \"revision\" argument specifies the model revision to use for the ControlNet model when loading\nfrom Hugging Face repository, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the ControlNet model variant, if \"variant\" is specified when\nloading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename,\ne.g. \"pytorch_model.<variant>.safetensors. \"variant\" defaults to automatic selection. \"variant\" in\nthe case of --control-nets does not default to the value of --variant to prevent failures during\ncommon use cases.\n\nThe \"subfolder\" argument specifies the ControlNet model subfolder, if specified when loading from a\nHugging Face repository or folder, weights from the specified subfolder.\n\nThe \"dtype\" argument specifies the ControlNet model precision, it defaults to the value of\n-t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\nIf you wish to load a weights file directly from disk, the simplest way is: --control-nets\n\"my_controlnet.safetensors\" or --control-nets \"my_controlnet.safetensors;scale=1.0;dtype=float16\",\nall other loading arguments aside from \"scale\", \"start\", \"end\", and \"dtype\" are unused in this case\nand may produce an error message if used.\n\nIf you wish to load a specific weight file from a Hugging Face repository, use the blob link loading\nsyntax: --control-nets\n\"https://huggingface.co/UserName/repository-name/blob/main/controlnet.safetensors\", the \"revision\"\nargument may be used with this syntax.", "--t2i-adapters": "Specify one or more T2IAdapter models using URIs. This should be a Hugging Face repository slug /\nblob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file),\nor model folder containing model files.\n\nIf a T2IAdapter model file exists at a URL which serves the file as a raw download, you may provide\nan http/https link to it and it will be downloaded to dgenerate's web cache.\n\nOptional arguments can be provided after the T2IAdapter model specification, these are: \"scale\",\n\"revision\", \"variant\", \"subfolder\", and \"dtype\".\n\nThey can be specified as so in any order, they are not positional:\n\n\"huggingface/t2iadapter;scale=1.0;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n\nThe \"scale\" argument specifies the scaling factor applied to the T2IAdapter model, the default value\nis 1.0.\n\nThe \"revision\" argument specifies the model revision to use for the T2IAdapter model when loading\nfrom Hugging Face repository, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the T2IAdapter model variant, if \"variant\" is specified when\nloading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename,\ne.g. \"pytorch_model.<variant>.safetensors. \"variant\"  defaults to automatic selection. \"variant\" in\nthe case of --t2i-adapters does not default to the value of --variant to prevent failures during\ncommon use cases.\n\nThe \"subfolder\" argument specifies the ControlNet model subfolder, if specified when loading from a\nHugging Face repository or folder, weights from the specified subfolder.\n\nThe \"dtype\" argument specifies the T2IAdapter model precision, it defaults to the value of\n-t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\nIf you wish to load a weights file directly from disk, the simplest way is: --t2i-adapters\n\"my_t2i_adapter.safetensors\" or --t2i-adapters \"my_t2i_adapter.safetensors;scale=1.0;dtype=float16\",\nall other loading arguments aside from \"scale\" and \"dtype\" are unused in this case and may produce\nan error message if used.\n\nIf you wish to load a specific weight file from a Hugging Face repository, use the blob link loading\nsyntax: --t2i-adapters\n\"https://huggingface.co/UserName/repository-name/blob/main/t2i_adapter.safetensors\", the \"revision\"\nargument may be used with this syntax.", "--quantizer": "Global quantization configuration via URI.\n\nThis URI specifies the quantization backend and its configuration.\n\nQuantization will be applied to all text encoders, and unet / transformer models with the provided\nsettings when using this argument.\n\nIf you wish to specify different quantization types per encoder or unet / transformer, you should\nuse the \"quantizer\" URI argument of --text-encoders and or --unet / --transformer to specify the\nquantization settings on a per model basis.\n\nAvailable backends are: (bnb / bitsandbytes, sdnq)\n\nbitsandbytes can be specified with \"bnb\" or \"bitsandbytes\"\n\nExample:\n\n--quantizer bnb;bits=4\n\nor:\n\n--quantizer bitsandbytes;bits=4\n\nThe bitsandbytes backend URI possesses these arguments and defaults:\n\n* bits: int = 8 (must be 4 or 8)\n* bits4-compute-dtype: str = None (auto set when not specified)\n* bits4-quant-type: str = \"fp4\"\n* bits4-use-double-quant = False\n* bits4-quant-storage: str = None\n\nSDNQ (SD.Next Quantization) backend can be specified with \"sdnq\"\n\nExample:\n\n--quantizer sdnq;type=int4\n\nThe SDNQ backend URI possesses these arguments and defaults:\n\n* type: str = \"int8\"\n* group-size: int = 0 (how many tensor elements will share a quantization group, must be >= 0)\n* quant-conv: bool = False (quantize convolutional layers)\n* quantized-matmul: bool = False (use quantized matrix multiplication)\n* quantized-matmul-conv: bool = False (use quantized matrix multiplication for convolutional layers)\n\nSDNQ supports the quantization types:\n\n* bool\n* int8, int7, int6, int5, int4, int3, int2\n* uint8, uint7, uint6, uint5, uint4, uint3, uint2, uint1,\n* float8_e4m3fn, float8_e4m3fnuz, float8_e5m2, float8_e5m2fnuz", "--quantizer-help": "Use this option alone with no model specification in order to list quantizer (quantization backend)\nnames.  Specifying one or more quantizer names after this option will cause usage documentation for\nthe specified quantization backend to be printed.", "--quantizer-map": "Global quantization map, used with --quantizer.\n\nThis argument can be used to specify which sub-modules have the quantization pre-process performed\non them.\n\nBy default when a --quantizer URI is specified, the UNet / Transformer, and all Text Encoders are\nprocessed.\n\nWhen using --quantizer, you can use this argument to specify exactly which sub-modules undergo\nquantization.\n\nAccepted values are: \"unet\", \"transformer\", \"text_encoder\", \"text_encoder_2\", \"text_encoder_3\"", "--second-model-quantizer": "Global quantization configuration via URI for the secondary model, such as the SDXL Refiner or\nStable Cascade decoder. See: --quantizer for syntax examples.", "--second-model-quantizer-map": "Global quantization map for the secondary model, used with --second-model-quantizer. This affects\nthe SDXL Refiner or Stable Cascade decoder, See: --quantizer-map for syntax examples.", "--scheduler": "Specify a scheduler (sampler) by URI.\n\nPassing \"help\" to this argument will print the compatible schedulers for a model without generating any images.\n\nPassing \"helpargs\" will yield a help message with a list of overridable arguments for each scheduler and their typical defaults.\n\nArguments listed by \"helpargs\" can be overridden using the URI syntax typical to other dgenerate URI\narguments.\n\nYou may pass multiple scheduler URIs to this argument, each URI will be tried in turn.", "--second-model-scheduler": "Specify a scheduler (sampler) by URI for the SDXL Refiner or Stable Cascade Decoder pass. Operates\nthe exact same way as --scheduler including the \"help\" option. Passing 'helpargs' will yield a help\nmessage with a list of overridable arguments for each scheduler and their typical defaults. Defaults\nto the value of --scheduler.\n\nYou may pass multiple scheduler URIs to this argument, each URI will be tried in turn.", "--freeu-params": "FreeU is a technique for improving image quality by re-balancing the contributions from the UNet's\nskip connections and backbone feature maps.\n\nThis can be used with no cost to performance, to potentially improve image quality.\n\nThis argument can be used to specify The FreeU parameters: s1, s2, b1, and b2 in that order.\n\nIt accepts CSV, for example: --freeu-params \"0.9,0.2,1.1,1.2\"\n\nIf you supply multiple CSV strings, they will be tried in turn.\n\nThis argument only applies to models that utilize a UNet: SD1.5/2, SDXL, and Kolors\n\nSee: https://huggingface.co/docs/diffusers/main/en/using-diffusers/freeu\n\nAnd: https://github.com/ChenyangSi/FreeU", "--hi-diffusion": "Activate HiDiffusion for the primary model?\n\nThis can increase the resolution at which the model can output images while retaining quality with\nno overhead, and possibly improved performance.\n\nSee: https://github.com/megvii-research/HiDiffusion\n\nThis is supported for --model-type sd, sdxl, and --kolors.", "--hi-diffusion-no-win-attn": "Disable window attention when using HiDiffusion for the primary model?\n\nThis disables the MSW-MSA (Multi-Scale Window Multi-Head Self-Attention) component of HiDiffusion.\n\nSee: https://github.com/megvii-research/HiDiffusion\n\nThis is supported for: --model-type sd, sdxl, and --kolors.", "--hi-diffusion-no-raunet": "Disable RAU-Net when using HiDiffusion for the primary model?\n\nThis disables the Resolution-Aware U-Net component of HiDiffusion.\n\nSee: https://github.com/megvii-research/HiDiffusion\n\nThis is supported for: --model-type sd, sdxl, and --kolors.", "--sdxl-refiner-freeu-params": "FreeU parameters for the SDXL refiner, see: --freeu-params", "--deep-cache": "Activate DeepCache for the main model?\n\nDeepCache caches intermediate attention layer outputs to speed up the diffusion process. Recommended\nfor higher inference steps.\n\nSee: https://github.com/horseee/DeepCache\n\nThis is supported for Stable Diffusion, Stable Diffusion XL, Stable Diffusion Upscaler X4, Kolors,\nand Pix2Pix variants.", "--deep-cache-intervals": "Cache interval for DeepCache for the main model.\n\nControls how frequently the attention layers are cached during the diffusion process. Lower values\ncache more frequently, potentially resulting in more speedup but using more memory.\n\nThis value must be greater than zero.\n\nEach value supplied will be tried in turn.\n\nSupplying any values implies --deep-cache.\n\nThis is supported for Stable Diffusion, Stable Diffusion XL, Stable Diffusion Upscaler X4, Kolors,\nand Pix2Pix variants.\n\n(default: 5)", "--deep-cache-branch-ids": "Branch ID for DeepCache for the main model.\n\nControls which branches of the UNet attention blocks the caching is applied to. Advanced usage only.\n\nThis value must be greater than or equal to 0.\n\nEach value supplied will be tried in turn.\n\nSupplying any values implies --deep-cache.\n\nThis is supported for Stable Diffusion, Stable Diffusion XL, Stable Diffusion Upscaler X4, Kolors,\nand Pix2Pix variants.\n\n(default: 1)", "--sdxl-refiner-deep-cache": "Activate DeepCache for the SDXL Refiner?\n\nSee: --deep-cache\n\nThis is supported for Stable Diffusion XL and Kolors based models.", "--sdxl-refiner-deep-cache-intervals": "Cache interval for DeepCache for the SDXL Refiner.\n\nControls how frequently the attention layers are cached during the diffusion process. Lower values\ncache more frequently, potentially resulting in more speedup but using more memory.\n\nThis value must be greater than zero.\n\nEach value supplied will be tried in turn.\n\nSupplying any values implies --sdxl-refiner-deep-cache.\n\nThis is supported for Stable Diffusion XL and Kolors based models.\n\n(default: 5)", "--sdxl-refiner-deep-cache-branch-ids": "Branch ID for DeepCache for the SDXL Refiner.\n\nControls which branches of the UNet attention blocks the caching is applied to. Advanced usage only.\n\nThis value must be greater than or equal to 0.\n\nEach value supplied will be tried in turn.\n\nSupplying any values implies --sdxl-refiner-deep-cache.\n\nThis is supported for Stable Diffusion XL and Kolors based models.\n\n(default: 1)", "--tea-cache": "Activate TeaCache for the primary model?\n\nThis is supported for Flux, TeaCache uses a novel caching mechanism in the forward pass of the flux\ntransformer to reduce the amount of computation needed to generate an image, this can speed up\ninference with small amounts of quality loss.\n\nSee: https://github.com/ali-vilab/TeaCache\n\nAlso see: --tea-cache-rel-l1-thresholds\n\nThis is supported for: --model-type flux*.", "--tea-cache-rel-l1-thresholds": "TeaCache relative L1 thresholds to try when --tea-cache is enabled.\n\nThis should be one or more float values between 0.0 and 1.0, each value will be tried in turn.\n\nHigher values mean more speedup.\n\nDefaults to 0.6 (2.0x speedup). 0.25 for 1.5x speedup, 0.4 for 1.8x speedup, 0.6 for 2.0x speedup,\n0.8 for 2.25x speedup\n\nSee: https://github.com/ali-vilab/TeaCache\n\nSupplying any values implies --tea-cache.\n\nThis is supported for: --model-type flux*.\n\n(default: 0.6)", "--ras": "Activate RAS (Region-Adaptive Sampling) for the primary model?\n\nThis can increase inference speed with SD3.\n\nSee: https://github.com/microsoft/ras\n\nThis is supported for: --model-type sd3.", "--ras-index-fusion": "Enable index fusion in RAS (Reinforcement Attention System) for the primary model?\n\nThis can improve attention computation in RAS for SD3 models.\n\nSupplying this flag implies --ras.\n\nThis is supported for: --model-type sd3, (but not for SD3.5 models)", "--ras-sample-ratios": "Average sample ratios for each RAS step.\n\nFor instance, setting this to 0.5 on a sequence of 4096 tokens will result in the noise of averagely\n2048 tokens to be updated during each RAS step.\n\nMust be between 0.0 and 1.0 (non-inclusive)\n\nEach value will be tried in turn.\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: 0.5)", "--ras-high-ratios": "Ratios of high value tokens to be cached in RAS.\n\nBased on the metric selected, the ratio of the high value chosen to be cached.\n\nMust be between 0.0 and 1.0 (non-inclusive) to balance the sample ratio between the main subject and\nthe background.\n\nEach value will be tried in turn.\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: 1.0)", "--ras-starvation-scales": "Starvation scales for RAS patch selection.\n\nRAS tracks how often a token is dropped and incorporates this count as a scaling factor in the\nmetric for selecting tokens. This scale factor prevents excessive blurring or noise in the final\ngenerated image.\n\nLarger scaling factor will result in more uniform sampling.\n\nMust be between 0.0 and 1.0 (non-inclusive)\n\nEach value will be tried in turn.\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: 0.1)", "--ras-error-reset-steps": "Dense sampling steps to reset accumulated error in RAS.\n\nThe dense sampling steps inserted between the RAS steps to reset the accumulated error. Each\nargument should be either a single integer or a comma-separated list of integers, e.g. 12 or\n\"12,22\".\n\nMultiple values or comma-separated lists can be provided, and each will be tried in turn.\n\nExample: --ras-error-reset-steps 12 \"5,10,15\"\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: \"12,22\")", "--ras-metrics": "Metrics to try for RAS (Region-Adaptive Sampling).\n\nThis controls how RAS measures the importance of tokens for caching. Valid values are \"std\"\n(standard deviation) or \"l2norm\" (L2 norm).\n\nEach value will be tried in turn.\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: \"std\")", "--ras-start-steps": "Starting steps to try for RAS (Region-Adaptive Sampling).\n\nThis controls when RAS begins applying its sampling strategy. Must be greater than or equal to 1.\n\nEach value will be tried in turn.\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: 4)", "--ras-end-steps": "Ending steps to try for RAS (Region-Adaptive Sampling).\n\nThis controls when RAS stops applying its sampling strategy. Must be greater than or equal to 1.\n\nEach value will be tried in turn.\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: --inference-steps)", "--ras-skip-num-steps": "Skip steps for RAS (Region-Adaptive Sampling).\n\nThis controls the number of steps to skip between RAS steps.\n\nThe actual number of tokens skipped will be rounded down to the nearest multiple of 64 to ensure\nefficient memory access patterns for attention computation.\n\nWhen used with --ras-skip-num-step-lengths greater than 0, this value will determine how the number\nof skipped tokens changes over time. Positive values will increase the number of skipped tokens over\ntime, while negative values will decrease it.\n\nEach value will be tried in turn.\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: 0)", "--ras-skip-num-step-lengths": "Skip step lengths for RAS (Region-Adaptive Sampling).\n\nThis controls the length of steps to skip between RAS steps. Must be greater than or equal to 0.\n\nWhen set to 0, static dropping is used where the number of skipped tokens remains constant\nthroughout the generation process.\n\nWhen greater than 0, dynamic dropping is enabled where the number of skipped tokens varies over time\nbased on --ras-skip-num-steps. The pattern of skipping will repeat every --ras-skip-num-step-lengths\nsteps.\n\nEach value will be tried in turn.\n\nSupplying any values implies --ras.\n\nThis is supported for: --model-type sd3.\n\n(default: 0)", "--pag": "Use perturbed attention guidance? This is supported for --model-type sd, sdxl, and sd3 for most use\ncases. This enables PAG for the main model using default scale values.", "--pag-scales": "One or more perturbed attention guidance scales to try. Specifying values enables PAG for the main\nmodel. (default: [3.0])", "--pag-adaptive-scales": "One or more adaptive perturbed attention guidance scales to try. Specifying values enables PAG for\nthe main model. (default: [0.0])", "--sdxl-refiner-pag": "Use perturbed attention guidance in the SDXL refiner? This is supported for --model-type sdxl for\nmost use cases. This enables PAG for the SDXL refiner model using default scale values.", "--sdxl-refiner-pag-scales": "One or more perturbed attention guidance scales to try with the SDXL refiner pass. Specifying values\nenables PAG for the refiner. (default: [3.0])", "--sdxl-refiner-pag-adaptive-scales": "One or more adaptive perturbed attention guidance scales to try with the SDXL refiner pass.\nSpecifying values enables PAG for the refiner. (default: [0.0])", "--model-sequential-offload": "Force sequential model offloading for the main pipeline, this may drastically reduce memory\nconsumption and allow large models to run when they would otherwise not fit in your GPUs VRAM.\nInference will be much slower. Mutually exclusive with --model-cpu-offload", "--model-cpu-offload": "Force model cpu offloading for the main pipeline, this may reduce memory consumption and allow large\nmodels to run when they would otherwise not fit in your GPUs VRAM. Inference will be slower.\nMutually exclusive with --model-sequential-offload", "--second-model-sequential-offload": "Force sequential model offloading for the SDXL Refiner or Stable Cascade Decoder pipeline, this may\ndrastically reduce memory consumption and allow large models to run when they would otherwise not\nfit in your GPUs VRAM. Inference will be much slower. Mutually exclusive with\n--second-model-cpu-offload", "--second-model-cpu-offload": "Force model cpu offloading for the SDXL Refiner or Stable Cascade Decoder pipeline, this may reduce\nmemory consumption and allow large models to run when they would otherwise not fit in your GPUs\nVRAM. Inference will be slower. Mutually exclusive with --second-model-sequential-offload", "--s-cascade-decoder": "Specify a Stable Cascade (s-cascade) decoder model path using a URI. This should be a Hugging Face\nrepository slug / blob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or\n.safetensors file), or model folder containing model files.\n\nOptional arguments can be provided after the decoder model specification, these are: \"revision\",\n\"variant\", \"subfolder\", and \"dtype\".\n\nThey can be specified as so in any order, they are not positional:\n\n\"huggingface/decoder_model;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n\nThe \"revision\" argument specifies the model revision to use for the decoder model when loading from\nHugging Face repository, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the decoder model variant and defaults to the value of --variant.\nWhen \"variant\" is specified when loading from a Hugging Face repository or folder, weights will be\nloaded from \"variant\" filename, e.g. \"pytorch_model.<variant>.safetensors.\n\nThe \"subfolder\" argument specifies the decoder model subfolder, if specified when loading from a\nHugging Face repository or folder, weights from the specified subfolder.\n\nThe \"dtype\" argument specifies the Stable Cascade decoder model precision, it defaults to the value\nof -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\nIf you wish to load a weights file directly from disk, the simplest way is: --sdxl-refiner\n\"my_decoder.safetensors\" or --sdxl-refiner \"my_decoder.safetensors;dtype=float16\", all other loading\narguments aside from \"dtype\" are unused in this case and may produce an error message if used.\n\nIf you wish to load a specific weight file from a Hugging Face repository, use the blob link loading\nsyntax: --s-cascade-decoder\n\"https://huggingface.co/UserName/repository-name/blob/main/decoder.safetensors\", the \"revision\"\nargument may be used with this syntax.", "--sdxl-refiner": "Specify a Stable Diffusion XL (sdxl) refiner model path using a URI. This should be a Hugging Face\nrepository slug / blob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or\n.safetensors file), or model folder containing model files.\n\nOptional arguments can be provided after the SDXL refiner model specification, these are:\n\"revision\", \"variant\", \"subfolder\", and \"dtype\".\n\nThey can be specified as so in any order, they are not positional:\n\n\"huggingface/refiner_model_xl;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n\nThe \"revision\" argument specifies the model revision to use for the refiner model when loading from\nHugging Face repository, (The Git branch / tag, default is \"main\").\n\nThe \"variant\" argument specifies the SDXL refiner model variant and defaults to the value of\n--variant. When \"variant\" is specified when loading from a Hugging Face repository or folder,\nweights will be loaded from \"variant\" filename, e.g. \"pytorch_model.<variant>.safetensors.\n\nThe \"subfolder\" argument specifies the SDXL refiner model subfolder, if specified when loading from\na Hugging Face repository or folder, weights from the specified subfolder.\n\nThe \"dtype\" argument specifies the SDXL refiner model precision, it defaults to the value of\n-t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\nIf you wish to load a weights file directly from disk, the simplest way is: --sdxl-refiner\n\"my_sdxl_refiner.safetensors\" or --sdxl-refiner \"my_sdxl_refiner.safetensors;dtype=float16\", all\nother loading arguments aside from \"dtype\" are unused in this case and may produce an error message\nif used.\n\nIf you wish to load a specific weight file from a Hugging Face repository, use the blob link loading\nsyntax: --sdxl-refiner\n\"https://huggingface.co/UserName/repository-name/blob/main/refiner_model.safetensors\", the\n\"revision\" argument may be used with this syntax.", "--sdxl-refiner-edit": "Force the SDXL refiner to operate in edit mode instead of cooperative denoising mode as it would\nnormally do for inpainting and ControlNet usage. The main model will perform the full amount of\ninference steps requested by --inference-steps. The output of the main model will be passed to the\nrefiner model and processed with an image seed strength in img2img mode determined by (1.0 -\nhigh-noise-fraction)", "--sdxl-t2i-adapter-factors": "One or more SDXL specific T2I adapter factors to try, this controls the amount of time-steps for\nwhich a T2I adapter applies guidance to an image, this is a value between 0.0 and 1.0. A value of\n0.5 for example indicates that the T2I adapter is only active for half the amount of time-steps it\ntakes to completely render an image.", "--sdxl-aesthetic-scores": "One or more Stable Diffusion XL (sdxl) \"aesthetic-score\" micro-conditioning parameters. Used to\nsimulate an aesthetic score of the generated image by influencing the positive text condition. Part\nof SDXL's micro-conditioning as explained in section 2.2 of\n[https://huggingface.co/papers/2307.01952].", "--sdxl-crops-coords-top-left": "One or more Stable Diffusion XL (sdxl) \"negative-crops-coords-top-left\" micro-conditioning\nparameters in the format \"0,0\". --sdxl-crops-coords-top-left can be used to generate an image that\nappears to be \"cropped\" from the position --sdxl-crops-coords-top-left downwards. Favorable,\nwell-centered images are usually achieved by setting --sdxl-crops-coords-top-left to \"0,0\". Part of\nSDXL's micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952].", "--sdxl-original-sizes": "One or more Stable Diffusion XL (sdxl) \"original-size\" micro-conditioning parameters in the format\n(WIDTH)x(HEIGHT). If not the same as --sdxl-target-size the image will appear to be down or\nup-sampled. --sdxl-original-size defaults to --output-size or the size of any input images if not\nspecified. Part of SDXL's micro-conditioning as explained in section 2.2 of\n[https://huggingface.co/papers/2307.01952]", "--sdxl-target-sizes": "One or more Stable Diffusion XL (sdxl) \"target-size\" micro-conditioning parameters in the format\n(WIDTH)x(HEIGHT). For most cases, --sdxl-target-size should be set to the desired height and width\nof the generated image. If not specified it will default to --output-size or the size of any input\nimages. Part of SDXL's micro-conditioning as explained in section 2.2 of\n[https://huggingface.co/papers/2307.01952]", "--sdxl-negative-aesthetic-scores": "One or more Stable Diffusion XL (sdxl) \"negative-aesthetic-score\" micro-conditioning parameters.\nPart of SDXL's micro-conditioning as explained in section 2.2 of\n[https://huggingface.co/papers/2307.01952]. Can be used to simulate an aesthetic score of the\ngenerated image by influencing the negative text condition.", "--sdxl-negative-original-sizes": "One or more Stable Diffusion XL (sdxl) \"negative-original-sizes\" micro-conditioning parameters.\nNegatively condition the generation process based on a specific image resolution. Part of SDXL's\nmicro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952]. For\nmore information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208", "--sdxl-negative-target-sizes": "One or more Stable Diffusion XL (sdxl) \"negative-original-sizes\" micro-conditioning parameters. To\nnegatively condition the generation process based on a target image resolution. It should be as same\nas the \"--sdxl-target-size\" for most cases. Part of SDXL's micro-conditioning as explained in\nsection 2.2 of [https://huggingface.co/papers/2307.01952]. For more information, refer to this issue\nthread: https://github.com/huggingface/diffusers/issues/4208.", "--sdxl-negative-crops-coords-top-left": "One or more Stable Diffusion XL (sdxl) \"negative-crops-coords-top-left\" micro-conditioning\nparameters in the format \"0,0\". Negatively condition the generation process based on a specific crop\ncoordinates. Part of SDXL's micro-conditioning as explained in section 2.2 of\n[https://huggingface.co/papers/2307.01952]. For more information, refer to this issue thread:\nhttps://github.com/huggingface/diffusers/issues/4208.", "--sdxl-refiner-aesthetic-scores": "See: --sdxl-aesthetic-scores, applied to SDXL refiner pass.", "--sdxl-refiner-crops-coords-top-left": "See: --sdxl-crops-coords-top-left, applied to SDXL refiner pass.", "--sdxl-refiner-original-sizes": "See: --sdxl-refiner-original-sizes, applied to SDXL refiner pass.", "--sdxl-refiner-target-sizes": "See: --sdxl-refiner-target-sizes, applied to SDXL refiner pass.", "--sdxl-refiner-negative-aesthetic-scores": "See: --sdxl-negative-aesthetic-scores, applied to SDXL refiner pass.", "--sdxl-refiner-negative-original-sizes": "See: --sdxl-negative-original-sizes, applied to SDXL refiner pass.", "--sdxl-refiner-negative-target-sizes": "See: --sdxl-negative-target-sizes, applied to SDXL refiner pass.", "--sdxl-refiner-negative-crops-coords-top-left": "See: --sdxl-negative-crops-coords-top-left, applied to SDXL refiner pass.", "--sdxl-high-noise-fractions": "One or more high-noise-fraction values for Stable Diffusion XL (sdxl), this fraction of inference\nsteps will be processed by the base model, while the rest will be processed by the refiner model.\nMultiple values to this argument will result in additional generation steps for each value. In\ncertain situations when collaborative denoising is not supported, such as when using --control-nets\nand inpainting with SDXL, the inverse proportion of this value IE: (1.0 - high-noise-fraction)\nbecomes the --image-seed-strengths input to the SDXL refiner in plain img2img mode. Edit mode may be\nforced with the option --sdxl-refiner-edit (default: [0.8])", "--sdxl-refiner-guidance-rescales": "One or more guidance rescale values for the SDXL refiner when in use. Override the guidance rescale\nvalue used by the SDXL refiner, which defaults to the value taken from --guidance-rescales.", "--safety-checker": "Enable safety checker loading, this is off by default. When turned on images with NSFW content\ndetected may result in solid black output. Some pretrained models have no safety checker model\npresent, in that case this option has no effect.", "--device": "cuda / cpu, or other device supported by torch.  For example mps on MacOS, and xpu for intel GPUs.\n\ndefault: cuda [prioritize when available] then xpu. And only mps on MacOS.\n\nUse: cuda:0, cuda:1, cuda:2, etc. to specify a specific cuda supporting GPU.\n\nDevice indices are also supported for xpu, but not for mps.", "--dtype": "Model precision: auto, bfloat16, float16, or float32. (default: auto)", "--output-size": "Image output size, for txt2img generation this is the exact output size. The dimensions specified\nfor this value must be aligned by 8 or you will receive an error message. If an --image-seeds URI is\nused its Seed, Mask, and/or Control component image sources will be resized to this dimension with\naspect ratio maintained before being used for generation by default, except in the case of Stable\nCascade where the images are used as a style prompt (not a noised seed), and can be of varying\ndimensions.\n\nIf --no-aspect is not specified, width will be fixed and a new height (aligned by 8) will be\ncalculated for the input images. In most cases resizing the image inputs will result in an image\noutput of an equal size to the inputs, except for upscalers and Deep Floyd --model-type values\n(if*).\n\nIf only one integer value is provided, that is the value for both dimensions. X/Y dimension values\nshould be separated by \"x\".\n\nThis value defaults to 512x512 for Stable Diffusion when no --image-seeds are specified (IE txt2img\nmode), 1024x1024 for Stable Cascade and Stable Diffusion 3/XL or Flux model types, and 64x64 for\n--model-type if (Deep Floyd stage 1).\n\nDeep Floyd stage 1 images passed to superscaler models (--model-type ifs*) that are specified  with\nthe 'floyd' keyword argument in an --image-seeds definition are never resized or processed in any\nway.", "--no-aspect": "This option disables aspect correct resizing of images provided to --image-seeds globally. Seed,\nMask, and Control guidance images will be resized to the closest dimension specified by\n--output-size that is aligned by 8 pixels with no consideration of the source aspect ratio. This can\nbe overriden at the --image-seeds level with the image seed keyword argument 'aspect=true/false'.", "--output-path": "Output path for generated images and files. This directory will be created if it does not exist.\n\n(default: ./output)", "--output-prefix": "Name prefix for generated images and files. This prefix will be added to the beginning of every\ngenerated file, followed by an underscore.", "--output-overwrite": "Enable overwrites of files in the output directory that already exists. The default behavior is not\nto do this, and instead append a filename suffix: \"_duplicate_(number)\" when it is detected that the\ngenerated file name already exists.", "--output-configs": "Write a configuration text file for every output image or animation. The text file can be used\nreproduce that particular output image or animation by piping it to dgenerate STDIN or by using the\n--file option, for example \"dgenerate < config.dgen\" or \"dgenerate --file config.dgen\". These files\nwill be written to --output-path and are affected by --output-prefix and --output-overwrite as well.\nThe files will be named after their corresponding image or animation file. Configuration files\nproduced for animation frame images will utilize --frame-start and --frame-end to specify the frame\nnumber.", "--output-metadata": "Write the information produced by --output-configs to the image metadata of each image. Metadata\nwill not be written to animated files. For PNGs, the data is written to a PNG metadata property\nnamed \"DgenerateConfig\" and can be read using ImageMagick like so: \"magick identify -format\n\"%%[Property:DgenerateConfig] generated_file.png\". For JPEGs, the data is written to the EXIF\nUserComment on the image. Only PNGs and JPEGs are supported for metadata writing, see:\n--image-format", "--output-auto1111-metadata": "Write Automatic1111 compatible metadata to the image metadata of each image, this includes hashes\nfor single file model checkpoints. Metadata will not be written to animated files. For PNGs, the\ndata is written to a PNG metadata property named \"parameters\". For JPEGs, the data is written to the\nEXIF UserComment on the image. Only PNGs and JPEGs are supported for metadata writing, see:\n--image-format", "--prompt-weighter": "Specify a prompt weighter implementation by URI, for example:\n\n--prompt-weighter compel, or --prompt-weighter sd-embed.\n\nBy default, no prompt weighting syntax is enabled,\nmeaning that you cannot adjust token weights as you may be able to do in software such as\nComfyUI, Automatic1111, CivitAI etc. And in some cases the length of your prompt is limited.\nPrompt weighters support these special token weighting syntaxes and long prompts,\ncurrently there are two implementations \"compel\" and \"sd-embed\". See: --prompt-weighter-help\nfor a list of implementation names. You may also use --prompt-weighter-help \"name\" to\nsee comprehensive documentation for a specific prompt weighter implementation.", "--second-model-prompt-weighter": "--prompt-weighter URI value that that applies to to --sdxl-refiner or --s-cascade-decoder.", "--prompt-weighter-help": "Use this option alone (or with --plugin-modules) and no model specification in order to list\navailable prompt weighter names. Specifying one or more prompt weighter names after this option will\ncause usage documentation for the specified prompt weighters to be printed. When used with\n--plugin-modules, prompt weighters implemented by the specified plugins will also be listed.", "--latents-processors": "Specify one or more latents processor URIs for processing raw input latents before pipeline\nexecution. These processors are applied to latents provided through --image-seeds when using latents\nsyntax such as \"latents: file.pt\", \"img2img.png;latents=file.pt\", or directly \"file.pt\" (raw latents\nused as noise initialization). The processors are applied in sequence before the latents are passed\nto the diffusion pipeline.\n\nYou may specify multiple processor URIs and they will be chained together sequentially.\n\nIf you have multiple latents specified for batching, for example\n\n(--image-seeds \"latents: latents-1.pt, latents-2.pt\"),\n\nyou may use the delimiter \"+\" to separate latents processor chains, so that a certain chain affects\na certain latents input, the plus symbol may also be used to represent a null processor.\n\nFor example:\n\n(--latents-processors affect-1 + affect-2)\n\n(--latents-processors + affect-2)\n\n(--latents-processors affect-1 +)\n\nSee: --latents-processor-help for a list of available implementations.", "--img2img-latents-processors": "Specify one or more latents processor URIs for processing img2img latents before pipeline execution.\nThese processors are applied to latent tensors provided through the --image-seeds argument when\ndoing img2img with tensor inputs. The processors are applied in sequence and may occur before VAE\ndecoding (for models that decode img2img latents) or before direct pipeline usage.\n\nYou may specify multiple processor URIs and they will be chained together sequentially.\n\nIf you have multiple img2img latents specified for batching, for example\n\n(--image-seeds \"images: latents-1.pt, latents-2.pt\"),\n\nyou may use the delimiter \"+\" to separate latents processor chains, so that a certain chain affects\na certain latents input, the plus symbol may also be used to represent a null processor.\n\nFor example:\n\n(--img2img-latents-processors affect-1 + affect-2)\n\n(--img2img-latents-processors + affect-2)\n\n(--img2img-latents-processors affect-1 +)\n\nSee: --latents-processor-help for a list of available implementations.", "--latents-post-processors": "Specify one or more latents processor URIs for processing output latents when outputting to latents.\nThese processors are applied to latents when --image-format is set to a tensor format (pt, pth,\nsafetensors). The processors are applied in sequence after the diffusion pipeline generates the\nlatents but before they are returned in the result.\n\nYou may specify multiple processor URIs and they will be chained together sequentially.\n\nSee: --latents-processor-help for a list of available implementations.", "--latents-processor-help": "Use this option alone (or with --plugin-modules) and no model specification in order to list\navailable latents processor names. Specifying one or more latents processor names after this option\nwill cause usage documentation for the specified latents processors to be printed. When used with\n--plugin-modules, latents processors implemented by the specified plugins will also be listed.", "--prompt-upscaler": "Specify a prompt upscaler implementation by URI, for example: --prompt-weighter dynamicprompts.\nPrompt upscaler plugins can perform pure text processing and expansion on incoming prompt text,\npossibly resulting in more generation steps (variations) if the prompt upscaler returns multiple\nprompts per input prompt.\n\nFor example: --prompt-upscaler \"dynamicprompts;scale=1.5\"\n\nYou may specify multiple upscaler URIs and they will be chained together sequentially.", "--second-model-prompt-upscaler": "Specify a --prompt-upscaler URI that will affect --second-model-prompts only, by default the prompt\nupscaler specified by --prompt-upscaler will be used.", "--second-model-second-prompt-upscaler": "Specify a --prompt-upscaler URI that will affect --second-model-second-prompts only, by default the\nprompt upscaler specified by --prompt-upscaler will be used.", "--second-prompt-upscaler": "Specify a --prompt-upscaler URI that will affect --second-prompts only, by default the prompt\nupscaler specified by --prompt-upscaler will be used.", "--third-prompt-upscaler": "Specify a --prompt-upscaler URI that will affect --third-prompts only, by default the prompt\nupscaler specified by --prompt-upscaler will be used.", "--prompt-upscaler-help": "Use this option alone (or with --plugin-modules) and no model specification in order to list\navailable prompt upscaler names. Specifying one or more prompt upscaler names after this option will\ncause usage documentation for the specified prompt upscalers to be printed. When used with\n--plugin-modules, prompt upscalers implemented by the specified plugins will also be listed.", "--prompts": "One or more prompts to try, an image group is generated for each prompt, prompt data is split by ;\n(semi-colon). The first value is the positive text influence, things you want to see. The Second\nvalue is negative influence IE. things you don't want to see.\n\nExample: --prompts \"photo of a horse in a field; artwork, painting, rain\".\n\n(default: [(empty string)])", "--second-prompts": "One or more secondary prompts to try using the sdxl (SDXL), sd3 (Stable Diffusion 3) or flux (Flux)\nsecondary text encoder. By default the model is passed the primary prompt for this value, this\noption allows you to choose a different prompt. The negative prompt component can be specified with\nthe same syntax as --prompts", "--third-prompts": "One or more tertiary prompts to try using the sd3 (Stable Diffusion 3) tertiary (T5) text encoder,\nFlux does not support this argument. By default the model is passed the primary prompt for this\nvalue, this option allows you to choose a different prompt. The negative prompt component can be\nspecified with the same syntax as --prompts", "--second-model-prompts": "One or more prompts to try with the SDXL Refiner or Stable Cascade decoder model, by default the\ndecoder model gets the primary prompt, this argument overrides that with a prompt of your choosing.\nThe negative prompt component can be specified with the same syntax as --prompts", "--second-model-second-prompts": "One or more prompts to try with the SDXL refiner models secondary text encoder (Stable Cascade\nDecoder is not supported), by default the SDXL refiner model gets the primary prompt passed to its\nsecond text encoder, this argument overrides that with a prompt of your choosing. The negative\nprompt component can be specified with the same syntax as --prompts", "--max-sequence-length": "The maximum amount of prompt tokens that the T5EncoderModel (third text encoder) of Stable Diffusion\n3 or Flux can handle. This should be an integer value between 1 and 512 inclusive. The higher the\nvalue the more resources and time are required for processing. (default: 256 for SD3, 512 for Flux)", "--clip-skips": "One or more clip skip values to try. Clip skip is the number of layers to be skipped from CLIP while\ncomputing the prompt embeddings, it must be a value greater than or equal to zero. A value of 1\nmeans that the output of the pre-final layer will be used for computing the prompt embeddings. This\nis only supported for --model-type values \"sd\", \"sdxl\", and \"sd3\".", "--seeds": "One or more seeds to try, define fixed seeds to achieve deterministic output. This argument may not\nbe used when --gse/--gen-seeds is used. (default: [randint(0, 99999999999999)])", "--seeds-to-images": "When this option is enabled, each provided --seeds value or value generated by --gen-seeds is used\nfor the corresponding image input given by --image-seeds. If the amount of --seeds given is not\nidentical to that of the amount of --image-seeds given, the seed is determined as: seed =\nseeds[image_seed_index %% len(seeds)], IE: it wraps around.", "--gen-seeds": "Auto generate N random seeds to try. This argument may not be used when -se/--seeds is used.", "--animation-format": "Output format when generating an animation from an input video / gif / webp etc. Value must be one\nof: mp4, png, apng, gif, or webp. You may also specify \"frames\" to indicate that only frames should\nbe output and no coalesced animation file should be rendered. (default: mp4)", "--image-format": "Output format when writing static images or tensors. For image formats, any selection other than\n\"png\", \"jpg\", or \"jpeg\" is not compatible with --output-metadata. For tensor formats (pt, pth,\nsafetensors), raw latent tensors will be saved instead of decoded images. Value must be one of: png,\napng, avif, avifs, blp, bmp, dib, bufr, pcx, dds, ps, eps, gif, grib, h5, hdf, jp2, j2k, jpc, jpf,\njpx, j2c, icns, ico, im, jfif, jpe, jpg, jpeg, tif, tiff, mpo, msp, palm, pdf, pbm, pgm, ppm, pnm,\npfm, qoi, bw, rgb, rgba, sgi, tga, icb, vda, vst, webp, wmf, emf, xbm, pt, pth, or safetensors.\n(default: png)", "--no-frames": "Do not write frame images individually when rendering an animation, only write the animation file.\nThis option is incompatible with --animation-format frames.", "--frame-start": "Starting frame slice point for animated files (zero-indexed), the specified frame will be included.\n(default: 0)", "--frame-end": "Ending frame slice point for animated files (zero-indexed), the specified frame will be included.", "--image-seeds": "One or more image seed URIs to process, these may consist of URLs or file paths. Videos / GIFs /\nWEBP files will result in frames being rendered as well as an animated output file being generated\nif more than one frame is available in the input file. Inpainting for static images can be achieved\nby specifying a black and white mask image in each image seed string using a semicolon as the\nseparating character, like so:\n\n\"my-seed-image.png;my-image-mask.png\", white areas of the mask indicate where\n\ngenerated content is to be placed in your seed image.\n\nOutput dimensions specific to the image seed can be specified by placing the dimension at the end of\nthe string following a semicolon like so:\n\n\"my-seed-image.png;512x512\" or \"my-seed-image.png;my-image-mask.png;512x512\".\n\nWhen using --control-nets, a singular image specification is interpreted as the control guidance\nimage, and you can specify multiple control image sources by separating them with commas in the case\nwhere multiple ControlNets are specified, IE:\n\n(--image-seeds \"control-image1.png, control-image2.png\") OR (--image-seeds \"seed.png;control=control-image1.png, control-image2.png\").\n\nUsing --control-nets with img2img or inpainting can be accomplished with the syntax:\n\n\"my-seed-image.png;mask=my-image-mask.png;control=my-control-image.png;resize=512x512\".\n\nThe \"mask\" and \"resize\" arguments are optional when using --control-nets. Videos, GIFs, and WEBP are\nalso supported as inputs when using --control-nets, even for the \"control\" argument.\n\n--image-seeds is capable of reading from multiple animated files at once or any combination of\nanimated files and images, the animated file with the least amount of frames dictates how many\nframes are generated and static images are duplicated over the total amount of frames. The keyword\nargument \"aspect\" can be used to determine resizing behavior when the global argument --output-size\nor the local keyword argument \"resize\" is specified, it is a boolean argument indicating whether\naspect ratio of the input image should be respected or ignored.\n\nThe keyword argument \"floyd\" can be used to specify images from a previous deep floyd stage when\nusing --model-type ifs*. When keyword arguments are present, all applicable images such as \"mask\",\n\"control\", etc. must also be defined with keyword arguments instead of with the short syntax.\n\nIn place of static images, you may pass a latents file generated by dgenerate containing the raw\nun-decoded latents from a previous generation, latents can be generated with --image-format pt, pth,\nor safetensors. Latents may be passed for img2img input only. Latents will first be decoded back\ninto pixel space (into a normal image) by the receiving models VAE. Except in the case of\n--model-type upscaler-x2, which can handle the denoised latents directly.\n\nLatent img2img input is not supported for --model-type s-cascade as Stable Cascade cannot perform\ntraditional img2img, and will result in an error if attempted. Latent input is also not supported\nfor ControlNet/T2I Adapter guidance images, or IP Adapter images, as these guidance models operate\non images in pixel space.", "--seed-image-processors": "Specify one or more image processor actions to perform on the primary img2img image(s) specified by\n--image-seeds.\n\nWhen specifying latents as img2img input, these processors will run on the image after the latents\nare decoded by the VAE.\n\nFor example: --seed-image-processors \"flip\" \"mirror\" \"grayscale\".\n\nTo obtain more information about what image processors are available and how to use them, see:\n--image-processor-help.\n\nIf you have multiple images specified for batching, for example\n\n(--image-seeds \"images: img2img-1.png, img2img-2.png\"),\n\nyou may use the delimiter \"+\" to separate image processor chains, so that a certain chain affects a\ncertain seed image, the plus symbol may also be used to represent a null processor.\n\nFor example:\n\n(--seed-image-processors affect-img-1 + affect-img-2)\n\n(--seed-image-processors + affect-img-2)\n\n(--seed-image-processors affect-img-1 +)\n\nThe amount of processors / processor chains must not exceed the amount of input images, or you will\nreceive a syntax error message. To obtain more information about what image processors are available\nand how to use them, see: --image-processor-help.", "--mask-image-processors": "Specify one or more image processor actions to perform on the inpaint mask image(s) specified by\n--image-seeds.\n\nFor example: --mask-image-processors \"invert\".\n\nTo obtain more information about what image processors are available and how to use them, see:\n--image-processor-help.\n\nIf you have multiple masks specified for batching, for example --image-seeds (\"images:\nimg2img-1.png, img2img-2.png; mask-1.png, mask-2.png\"), you may use the delimiter \"+\" to separate\nimage processor chains, so that a certain chain affects a certain mask image, the plus symbol may\nalso be used to represent a null processor.\n\nFor example: (--mask-image-processors affect-mask-1 + affect-mask-2), or (--mask-image-processors +\naffect-mask-2), or (--mask-image-processors affect-mask-1 +).\n\nThe amount of processors / processor chains must not exceed the amount of input mask images, or you\nwill receive a syntax error message. To obtain more information about what image processors are\navailable and how to use them, see: --image-processor-help.", "--control-image-processors": "Specify one or more image processor actions to perform on the control image specified by\n--image-seeds, this option is meant to be used with --control-nets.\n\nExample: --control-image-processors \"canny;lower=50;upper=100\".\n\nThe delimiter \"+\" can be used to specify a different processor group for each image when using\nmultiple control images with --control-nets.\n\nFor example if you have\n\n--image-seeds \"img1.png, img2.png\"\n\nor\n\n--image-seeds \"...;control=img1.png, img2.png\"\n\nspecified and multiple ControlNet models specified with --control-nets, you can specify processors\nfor those control images with the syntax:\n\n(--control-image-processors \"processes-img1\" + \"processes-img2\").\n\nThis syntax also supports chaining of processors, for example:\n\n(--control-image-processors \"first-process-img1\" \"second-process-img1\" + \"process-img2\").\n\nThe amount of specified processors must not exceed the amount of specified control images, or you\nwill receive a syntax error message.\n\nImages which do not have a processor defined for them will not be processed, and the plus character\ncan be used to indicate an image is not to be processed and instead skipped over when that image is\na leading element, for example\n\n(--control-image-processors + \"process-second\")\n\nwould indicate that the first control guidance image is not to be processed, only the second.\n\nTo obtain more information about what image processors are available and how to use them, see:\n--image-processor-help.", "--image-processor-help": "Use this option alone (or with --plugin-modules) and no model specification in order to list\navailable image processor names. Specifying one or more image processor names after this option will\ncause usage documentation for the specified image processors to be printed. When used with\n--plugin-modules, image processors implemented by the specified plugins will also be listed.", "--post-processors": "Specify one or more image processor actions to perform on generated output before it is saved.\n\nFor example: --post-processors \"upcaler;model=4x_ESRGAN.pth\".\n\nTo obtain more information about what processors are available and how to use them, see:\n--image-processor-help.", "--image-seed-strengths": "One or more image strength values to try when using --image-seeds for img2img or inpaint mode.\nCloser to 0 means high usage of the seed image (less noise convolution), 1 effectively means no\nusage (high noise convolution). Low values will produce something closer or more relevant to the\ninput image, high values will give the AI more creative freedom. This value must be greater than 0\nand less than or equal to 1. (default: [0.8])", "--upscaler-noise-levels": "One or more upscaler noise level values to try when using the super resolution upscaler --model-type\nupscaler-x4 or ifs. Specifying this option for --model-type upscaler-x2 will produce an error\nmessage. The higher this value the more noise is added to the image before upscaling (similar to\n--image-seed-strengths). (default: [20 for x4, 250 for ifs/ifs-img2img, 0 for ifs inpainting mode])", "--inpaint-crop": "Enable cropping to mask bounds for inpainting. When enabled, input images will be automatically\ncropped to the bounds of their masks (plus any padding) before processing, then the generated result\nwill be pasted back onto the original uncropped image. This allows inpainting at higher effective\nresolutions for better quality results.\n\nCannot be used with image seed batching (--image-seeds with multiple images/masks in the\ndefinition).\n\nEach image/mask pair must be processed individually as different masks may have different crop\nbounds. However, --batch-size > 1 is supported for generating multiple variations of a single crop.", "--inpaint-crop-paddings": "One or more padding values to use around mask bounds for inpaint cropping. Automatically enables\n--inpaint-crop. Each value will be tried in turn (combinatorial).\n\nExample:\n\n32 (32px Uniform, all sides)\n\n10x20 (10px Horizontal, 20px Vertical)\n\n10x20x30x40 (10px Left, 20px Top, 30px Right, 40px Bottom)\n\nNote: Inpaint crop cannot be used with multiple input images. See --inpaint-crop for details.\n\n(default: [32])", "--inpaint-crop-masked": "Use the mask when pasting the generated result back onto the original image for inpaint cropping.\nAutomatically enables --inpaint-crop. This means only the masked areas will be replaced. Cannot be\nused together with --inpaint-crop-feathers.\n\nNote: Inpaint crop cannot be used with individual --image-seeds batching. See --inpaint-crop for\ndetails.", "--inpaint-crop-feathers": "One or more feather values to use when pasting the generated result back onto the original image for\ninpaint cropping. Automatically enables --inpaint-crop. Each value will be tried in turn\n(combinatorial). Feathering creates smooth transitions from opaque to transparent. Cannot be used\ntogether with --inpaint-crop-masked.\n\nNote: Inpaint crop cannot be used with individual --image-seeds batching. See --inpaint-crop for\ndetails.\n\n(default: none - simple paste without feathering)", "--guidance-scales": "One or more guidance scale values to try. Guidance scale effects how much your text prompt is\nconsidered. Low values draw more data from images unrelated to text prompt.\n\n(default: [5])", "--sigmas": "One or more comma-separated lists (or singular values) of floating point sigmas to try. This is\nsupported when using a --scheduler that supports setting sigmas. Sigma values control the noise\nschedule in the diffusion process, allowing for fine-grained control over how noise is added and\nremoved during image generation.\n\nExample: --sigmas \"1.0,0.8,0.6,0.4,0.2\"\n\nOr expressions:\n\n\"expr: sigmas * .95\"\n\nsigmas from --scheduler are represented as a numpy array in an interpreted expression, numpy is\navailable through the namespace \"np\", this uses asteval.\n\nOr singular values:\n\n--sigmas 0.4\n\nExpressions and CSV lists can be intermixed: --sigmas \"1.0,...\" \"expr: sigmas * 0.95\"\n\nEach provided value (each quoted string in the example above) will be tried in turn.", "--image-guidance-scales": "One or more image guidance scale values to try. This can push the generated image towards the\ninitial image when using --model-type *-pix2pix models, it is unsupported for other model types. Use\nin conjunction with --image-seeds, inpainting (masks) and --control-nets are not supported. Image\nguidance scale is enabled by setting image-guidance-scale > 1. Higher image guidance scale\nencourages generated images that are closely linked to the source image, usually at the expense of\nlower image quality. Requires a value of at least 1. (default: [1.5])", "--guidance-rescales": "One or more guidance rescale factors to try. Proposed by\n\n[Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)\n\n\"guidance_scale\" is defined as \"\u03c6\" in equation 16. of\n\n[Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).\n\nGuidance rescale factor should fix overexposure when using zero terminal SNR. This is supported for\nbasic text to image generation when using --model-type \"sd\" but not inpainting, img2img, or\n--control-nets. When using --model-type \"sdxl\" it is supported for basic generation, inpainting, and\nimg2img, unless --control-nets is specified in which case only inpainting is supported. It is\nsupported for --model-type \"sdxl-pix2pix\" but not --model-type \"pix2pix\".\n\n(default: [0.0])", "--inference-steps": "One or more inference steps values to try. The amount of inference (de-noising) steps effects image\nclarity to a degree, higher values bring the image closer to what the AI is targeting for the\ncontent of the image. Values between 30-40 produce good results, higher values may improve image\nquality and or change image content.\n\n(default: [30])", "--second-model-inference-steps": "One or more inference steps values for the SDXL refiner or Stable Cascade decoder when in use.\nOverride the number of inference steps used by the second model, which defaults to the value taken\nfrom --inference-steps for SDXL and 10 for Stable Cascade.", "--second-model-guidance-scales": "One or more inference steps values for the SDXL refiner or Stable Cascade decoder when in use.\nOverride the guidance scale value used by the second model, which defaults to the value taken from\n--guidance-scales for SDXL and 0 for Stable Cascade.", "--sdxl-refiner-sigmas": "See: --sigmas, but for the SDXL Refiner.", "--denoising-start": "Fraction of total timesteps at which denoising should start (0.0 to 1.0). This allows you to skip\nthe early noising steps and start denoising from a specific point in the noise schedule. Useful for\ncooperative denoising workflows where one model handles the initial denoising and another model\nrefines the result.\n\nScheduler Compatibility:\n\nFor SD 1.5 models, only stateless schedulers are supported:\n\n* EulerDiscreteScheduler\n* LMSDiscreteScheduler\n* EDMEulerScheduler,\n* DPMSolverMultistepScheduler\n* DDIMScheduler\n* DDPMScheduler\n* PNDMScheduler\n\nFor SDXL models, all schedulers are supported via native denoising_start/denoising_end.\n\nFor SD3/Flux models, FlowMatchEulerDiscreteScheduler is supported.\n\nExample: --denoising-start 0.8\n\nA value of 0.8 means denoising will start at 80 percent through the total timesteps, effectively\nskipping the first 20 percent of the normal denoising process.", "--denoising-end": "Fraction of total timesteps at which denoising should end (0.0 to 1.0). This allows you to stop\ndenoising early, leaving the output in a partially noisy state. Useful for generating noisy latents\nthat can be saved with --image-format pt/pth/safetensors and passed to another model or generation\nstage using the \"latents: ...\" or \"img2img.png;latents= ...\" syntax of --image-seeds.\n\nScheduler Compatibility:\n\nFor SD 1.5 models, only stateless schedulers are supported:\n\n* EulerDiscreteScheduler\n* LMSDiscreteScheduler\n* EDMEulerScheduler,\n* DPMSolverMultistepScheduler\n* DDIMScheduler\n* DDPMScheduler\n* PNDMScheduler\n\nFor SDXL models, all schedulers are supported via native denoising_start/denoising_end.\n\nFor SD3/Flux models, FlowMatchEulerDiscreteScheduler is supported.\n\nExample: --denoising-end 0.5\n\nA value of 0.5 means denoising will stop at 50 percent through the total timesteps, leaving the\nresult partially noisy for further processing by another model."}