{"--verbose": "Output information useful for debugging, such as pipeline \n                    call and model load parameters.", "--version": "Show dgenerate's version and exit", "--file": "Convenience argument for reading a configuration script from a file instead \n                    of using a pipe. This is a meta argument which can not be used within a \n                    configuration script and is only valid from the command line or during a \n                    popen invocation of dgenerate.", "--shell": "When reading configuration from STDIN (a pipe), read forever, even when \n                    configuration errors occur. This allows dgenerate to run in the background and \n                    be controlled by another process sending commands. Launching\n                    dgenerate with this option and not piping it input will attach it to the \n                    terminal like a shell. Entering configuration into this shell requires\n                    two newlines to submit a command due to parsing lookahead. IE: two presses \n                    of the enter key. This is a meta argument which can not be used within a \n                    configuration script and is only valid from the command line or during a \n                    popen invocation of dgenerate.", "--no-stdin": "Can be used to indicate to dgenerate that it will not receive any\n                    piped in input. This is useful for running dgenerate via popen from Python\n                    or another application using normal arguments, where it would otherwise\n                    try to read from STDIN and block forever because it is not attached to \n                    a terminal. This is a meta argument which can not be used within a \n                    configuration script and is only valid from the command line or during \n                    a popen invocation of dgenerate.", "--console": "Launch a terminal-like Tkinter GUI that interacts with an instance\n                    of dgenerate running in the background. This allows you to interactively write\n                    dgenerate config scripts as if dgenerate were a shell / REPL. This is a meta argument\n                    which can not be used within a configuration script and is only valid from the command\n                    line or during a popen invocation of dgenerate.", "--plugin-modules": "Specify one or more plugin module folder paths (folder containing __init__.py) or\n                    Python .py file paths, or Python module names to load as plugins. Plugin modules can\n                    currently implement image processors, config directives, config template functions,\n                    prompt weighters, and sub-commands.", "--sub-command": "Specify the name a sub-command to invoke. dgenerate exposes some extra image processing\n                    functionality through the use of sub-commands. Sub commands essentially replace the entire set\n                    of accepted arguments with those of a sub-command which implements additional functionality.\n                    See --sub-command-help for a list of sub-commands and help.", "--sub-command-help": "Use this option alone (or with --plugin-modules) and no model specification\n                    in order to list available sub-command names. Calling a sub-command with\n                    \"--sub-command name --help\" will produce argument help output for that sub-command.\n                    When used with --plugin-modules, sub-commands implemented by the specified plugins\n                    will also be listed.", "--offline-mode": "Whether dgenerate should try to download Hugging Face models that do not \n                    exist in the disk cache, or only use what is available in the cache. Referencing \n                    a model on Hugging Face that has not been cached because it was not previously \n                    downloaded will result in a failure when using this option.", "--templates-help": "Print a list of template variables available in the interpreter environment\n                    used for dgenerate config scripts, particularly the variables set after a dgenerate \n                    invocation occurs. When used as a command line option, their values are not presented, \n                    just their names and types. Specifying names will print type information for \n                    those variable names.", "--directives-help": "Use this option alone (or with --plugin-modules) and no model specification\n                    in order to list available config directive names. Providing names will print documentation\n                    for the specified directive names. When used with --plugin-modules, directives implemented\n                    by the specified plugins will also be listed.", "--functions-help": "Use this option alone (or with --plugin-modules) and no model specification\n                    in order to list available config template function names. Providing names will print\n                    documentation for the specified function names. When used with --plugin-modules,\n                    functions implemented by the specified plugins will also be listed.", "--model-type": "Use when loading different model types. \n                     Currently supported: torch, torch-pix2pix, torch-sdxl, torch-sdxl-pix2pix, torch-upscaler-x2, torch-upscaler-x4, torch-if, torch-ifs, torch-ifs-img2img, torch-s-cascade, torch-sd3, or torch-flux. (default: torch)", "--revision": "The model revision to use when loading from a Hugging Face repository,\n                    (The Git branch / tag, default is \"main\")", "--variant": "If specified when loading from a Hugging Face repository or folder, load weights\n                    from \"variant\" filename, e.g. \"pytorch_model.<variant>.safetensors\".\n                    Defaults to automatic selection.", "--subfolder": "Main model subfolder.\n                    If specified when loading from a Hugging Face repository or folder,\n                    load weights from the specified subfolder.", "--auth-token": "Huggingface auth token.\n                    Required to download restricted repositories that have access permissions\n                    granted to your Hugging Face account.", "--batch-size": "The number of image variations to produce per set of individual diffusion parameters\n                    in one rendering step simultaneously on a single GPU. \n                    \n                    When generating animations with a --batch-size greater than one, a separate animation \n                    (with the filename suffix \"animation_N\") will be written to for each image in the batch. \n                    \n                    If --batch-grid-size is specified when producing an animation then the image grid is used \n                    for the output frames. \n                    \n                    During animation rendering each image in the batch will still be  written to the output directory \n                    along side the produced animation as either suffixed files or image grids depending on the \n                    options you choose. (Default: 1)", "--batch-grid-size": "Produce a single image containing a grid of images with the number of COLUMNSxROWS \n                    given to this argument when --batch-size is greater than 1. If not specified with a\n                    --batch-size greater than 1, images will be written individually with an image number suffix\n                    (image_N) in the filename signifying which image in the batch they are.", "--text-encoders": "Specify Text Encoders for the main model using URIs, main models \n                    may use one or more text encoders depending on the --model-type value and other\n                    dgenerate arguments. See: --text-encoders help for information \n                    about what text encoders are needed for your invocation.\n                    \n                    Examples: \"CLIPTextModel;model=huggingface/text_encoder\", \n                    \"CLIPTextModelWithProjection;model=huggingface/text_encoder;revision=main\", \n                    \"T5EncoderModel;model=text_encoder_folder_on_disk\". \n                    \n                    For main models which require multiple text encoders, the + symbol may be used\n                    to indicate that a default value should be used for a particular text encoder,\n                    for example: --text-encoders + + huggingface/encoder3.  Any trailing text \n                    encoders which are not specified are given their default value.\n                    \n                    The value \"null\" may be used to indicate that a specific text \n                    encoder should not be loaded.\n                    \n                    Blob links / single file loads are not supported for Text Encoders.\n                    \n                    The \"revision\" argument specifies the model revision to use for the Text Encoder\n                    when loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the Text Encoder model variant. If \"variant\" is specified \n                    when loading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename, \n                    e.g. \"pytorch_model.<variant>.safetensors\".\n                    For this argument, \"variant\" defaults to the value of --variant if it is not specified in the URI.\n                    \n                    The \"subfolder\" argument specifies the UNet model subfolder, if specified when loading from a \n                    Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the Text Encoder model precision, it defaults to the value of -t/--dtype\n                    and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    The \"quantize\" argument specifies whether or not to use optimum-quanto to quantize the text encoder weights, \n                    and may be passed the values \"qint2\", \"qint4\", \"qint8\", \"qfloat8_e4m3fn\", \"qfloat8_e4m3fnuz\", \"qfloat8_e5m2\", or \"qfloat8\" to \n                    specify the quantization datatype, this can be utilized to run Flux models with much less GPU memory.\n                    \n                    If you wish to load weights directly from a path on disk, you must point this argument at the folder\n                    they exist in, which should also contain the config.json file for the Text Encoder. \n                    For example, a downloaded repository folder from Hugging Face.", "--text-encoders2": "--text-encoders but for the SDXL refiner or Stable Cascade decoder model.", "--unet": "Specify a UNet using a URI.\n                    \n                    Examples: \"huggingface/unet\", \"huggingface/unet;revision=main\", \"unet_folder_on_disk\". \n                    \n                    Blob links / single file loads are not supported for UNets.\n                    \n                    The \"revision\" argument specifies the model revision to use for the UNet when loading from \n                    Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the UNet model variant. If \"variant\" is specified \n                    when loading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename, \n                    e.g. \"pytorch_model.<variant>.safetensors.\n                    For this argument, \"variant\" defaults to the value of --variant if it is not specified in the URI.\n                    \n                    The \"subfolder\" argument specifies the UNet model subfolder, if specified when loading from a \n                    Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the UNet model precision, it defaults to the value of -t/--dtype\n                    and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    If you wish to load weights directly from a path on disk, you must point this argument at the folder\n                    they exist in, which should also contain the config.json file for the UNet. \n                    For example, a downloaded repository folder from Hugging Face.", "--unet2": "Specify a second UNet, this is only valid when using SDXL or Stable Cascade \n                    model types. This UNet will be used for the SDXL refiner, or Stable Cascade decoder model.", "--transformer": "Specify a Stable Diffusion 3 or Flux Transformer model using a URI.\n                    \n                    Examples: \"huggingface/transformer\", \"huggingface/transformer;revision=main\", \"transformer_folder_on_disk\". \n                    \n                    Blob links / single file loads are supported for SD3 Transformers.\n                    \n                    The \"revision\" argument specifies the model revision to use for the Transformer when loading from \n                    Hugging Face repository or blob link, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the Transformer model variant. If \"variant\" is specified \n                    when loading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename, \n                    e.g. \"pytorch_model.<variant>.safetensors.\n                    For this argument, \"variant\" defaults to the value of --variant if it is not specified in the URI.\n                    \n                    The \"subfolder\" argument specifies the Transformer model subfolder, if specified when loading from a \n                    Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the Transformer model precision, it defaults to the value of -t/--dtype\n                    and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    The \"quantize\" argument specifies whether or not to use optimum-quanto to quantize the transformer weights, \n                    and may be passed the values \"qint2\", \"qint4\", \"qint8\", \"qfloat8_e4m3fn\", \"qfloat8_e4m3fnuz\", \"qfloat8_e5m2\", or \"qfloat8\" to \n                    specify the quantization datatype, this can be utilized to run Flux models with much less GPU memory.\n                    \n                    If you wish to load a weights file directly from disk, the simplest\n                    way is: --transformer \"transformer.safetensors\", or with a dtype \"transformer.safetensors;dtype=float16\". \n                    All loading arguments except \"dtype\" and \"quantize\" are unused in this case and may produce an \n                    error message if used.\n                    \n                    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n                    loading syntax: --transformer \n                    \"AutoencoderKL;https://huggingface.co/UserName/repository-name/blob/main/transformer.safetensors\",\n                    the \"revision\" argument may be used with this syntax.", "--vae": "Specify a VAE using a URI, the URI syntax is: \n                    \"AutoEncoderClass;model=(Hugging Face repository slug/blob link or file/folder path)\".\n                    \n                    Examples: \"AutoencoderKL;model=vae.pt\", \"AsymmetricAutoencoderKL;model=huggingface/vae\",\n                    \"AutoencoderTiny;model=huggingface/vae\", \"ConsistencyDecoderVAE;model=huggingface/vae\". \n                    \n                    The AutoencoderKL encoder class accepts Hugging Face repository slugs/blob links, \n                    .pt, .pth, .bin, .ckpt, and .safetensors files.\n                    \n                    Other encoders can only accept Hugging Face repository slugs/blob links, or a path to\n                    a folder on disk with the model configuration and model file(s).\n                    \n                    If an AutoencoderKL VAE model file exists at a URL which serves the file as\n                    a raw download, you may provide an http/https link to it and it will be\n                    downloaded to dgenerates web cache.\n                    \n                    Aside from the \"model\" argument, there are four other optional arguments that can be specified,\n                    these are: \"revision\", \"variant\", \"subfolder\", \"dtype\".\n                    \n                    They can be specified as so in any order, they are not positional:\n                    \"AutoencoderKL;model=huggingface/vae;revision=main;variant=fp16;subfolder=sub_folder;dtype=float16\".\n                    \n                    The \"revision\" argument specifies the model revision to use for the VAE when loading from \n                    Hugging Face repository or blob link, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the VAE model variant. If \"variant\" is specified \n                    when loading from a Hugging Face repository or folder, weights will be loaded from \n                    \"variant\" filename, e.g. \"pytorch_model.<variant>.safetensors. \"variant\" in the case \n                    of --vae does not default to the value of --variant to prevent failures during \n                    common use cases.\n                    \n                    The \"subfolder\" argument specifies the VAE model subfolder, if specified when loading from a \n                    Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the VAE model precision, it defaults to the value of -t/--dtype\n                    and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    If you wish to load a weights file directly from disk, the simplest\n                    way is: --vae \"AutoencoderKL;my_vae.safetensors\", or with a dtype \"AutoencoderKL;my_vae.safetensors;dtype=float16\". \n                    All loading arguments except \"dtype\" are unused in this case and may produce an error message if used.\n                    \n                    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n                    loading syntax: --vae \"AutoencoderKL;https://huggingface.co/UserName/repository-name/blob/main/vae_model.safetensors\",\n                    the \"revision\" argument may be used with this syntax.", "--vae-tiling": "Enable VAE tiling. Assists in the generation of\n                    large images with lower memory overhead. The VAE will split the input tensor \n                    into tiles to compute decoding and encoding in several steps. This is \n                    useful for saving a large amount of memory and to allow processing larger images. \n                    Note that if you are using --control-nets you may still run into memory \n                    issues generating large images, or with --batch-size greater than 1.", "--vae-slicing": "Enable VAE slicing. Assists in the generation \n                    of large images with lower memory overhead. The VAE will split the input tensor\n                    in slices to compute decoding in several steps. This is useful to save some memory,\n                    especially when --batch-size is greater than 1. Note that if you are using --control-nets\n                    you may still run into memory issues generating large images.", "--loras": "Specify one or more LoRA models using URIs. These should be a\n                    Hugging Face repository slug, path to model file on disk (for example, a .pt, .pth, .bin,\n                    .ckpt, or .safetensors file), or model folder containing model files.\n                    \n                    If a LoRA model file exists at a URL which serves the file as\n                    a raw download, you may provide an http/https link to it and it will be\n                    downloaded to dgenerates web cache.\n                    \n                    Hugging Face blob links are not supported, see \"subfolder\" and \"weight-name\" below instead.\n                    \n                    Optional arguments can be provided after a LoRA model specification, \n                    these are: \"scale\", \"revision\", \"subfolder\", and \"weight-name\".\n                    \n                    They can be specified as so in any order, they are not positional:\n                    \"huggingface/lora;scale=1.0;revision=main;subfolder=repo_subfolder;weight-name=lora.safetensors\".\n                    \n                    The \"scale\" argument indicates the scale factor of the LoRA.\n                    \n                    The \"revision\" argument specifies the model revision to use for the LoRA when loading from \n                    Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"subfolder\" argument specifies the LoRA model subfolder, if specified when loading from a \n                    Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"weight-name\" argument indicates the name of the weights file to be loaded when \n                    loading from a Hugging Face repository or folder on disk. \n                    \n                    If you wish to load a weights file directly from disk, the simplest\n                    way is: --loras \"my_lora.safetensors\", or with a scale \"my_lora.safetensors;scale=1.0\", \n                    all other loading arguments are unused in this case and may produce an error message if used.", "--lora-fuse-scale": "LoRA weights are merged into the main model at this scale.  When specifying multiple\n                    LoRA models, they are fused together into one set of weights using their individual scale values, \n                    after which they are fused into the main model at this scale value. (default: 1.0).", "--image-encoder": "Specify an Image Encoder using a URI.  \n                    \n                    Image Encoders are used with --ip-adapters models, and must be specified if none of the \n                    loaded --ip-adapters contain one.  An error will be produced in this situation, which\n                    requires you to use this argument.\n                    \n                    An image encoder can also be manually specified for Stable Cascade models.\n                    \n                    Examples: \"huggingface/image_encoder\", \"huggingface/image_encoder;revision=main\", \"image_encoder_folder_on_disk\". \n                    \n                    Blob links / single file loads are not supported for Image Encoders.\n                    \n                    The \"revision\" argument specifies the model revision to use for the Image Encoder when loading from \n                    Hugging Face repository or blob link, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the Image Encoder model variant. If \"variant\" is specified when \n                    loading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename, \n                    e.g. \"pytorch_model.<variant>.safetensors.\n                    \n                    Similar to --vae, \"variant\" does not default to the value of --variant in order to prevent \n                    errors with common use cases. If you specify multiple IP Adapters, they must all\n                    have the same \"variant\" value or you will receive a usage error.\n                    \n                    The \"subfolder\" argument specifies the Image Encoder model subfolder, if specified when loading from a \n                    Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the Image Encoder model precision, it defaults to the value of -t/--dtype\n                    and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    If you wish to load weights directly from a path on disk, you must point this argument at the folder\n                    they exist in, which should also contain the config.json file for the Image Encoder. For example, a downloaded\n                    repository folder from Hugging Face.", "--ip-adapters": "Specify one or more IP Adapter models using URIs. These should be a\n                    Hugging Face repository slug, path to model file on disk (for example, a .pt, .pth, .bin,\n                    .ckpt, or .safetensors file), or model folder containing model files.\n                    \n                    If an IP Adapter model file exists at a URL which serves the file as\n                    a raw download, you may provide an http/https link to it and it will be\n                    downloaded to dgenerates web cache.\n                    \n                    Hugging Face blob links are not supported, see \"subfolder\" and \"weight-name\" below instead.\n                    \n                    Optional arguments can be provided after an IP Adapter model specification, \n                    these are: \"scale\", \"revision\", \"subfolder\", and \"weight-name\".\n                    \n                    They can be specified as so in any order, they are not positional:\n                    \"huggingface/ip-adapter;scale=1.0;revision=main;subfolder=repo_subfolder;weight-name=ip_adapter.safetensors\".\n                    \n                    The \"scale\" argument indicates the scale factor of the IP Adapter.\n                    \n                    The \"revision\" argument specifies the model revision to use for the IP Adapter \n                    when loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"subfolder\" argument specifies the IP Adapter model subfolder, if specified when\n                    loading from a Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"weight-name\" argument indicates the name of the weights file to be loaded when \n                    loading from a Hugging Face repository or folder on disk. \n                    \n                    If you wish to load a weights file directly from disk, the simplest\n                    way is: --ip-adapters \"ip_adapter.safetensors\", or with a scale \"ip_adapter.safetensors;scale=1.0\", \n                    all other loading arguments are unused in this case and may produce an error message if used.", "--textual-inversions": "Specify one or more Textual Inversion models using URIs. \n                    These should be a Hugging Face repository slug, path to model file on disk \n                    (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file), or model folder \n                    containing model files.\n                    \n                    If a Textual Inversion model file exists at a URL which serves the file as\n                    a raw download, you may provide an http/https link to it and it will be\n                    downloaded to dgenerates web cache.\n                    \n                    Hugging Face blob links are not supported, see \"subfolder\" and \"weight-name\" below instead.\n                    \n                    Optional arguments can be provided after the Textual Inversion model specification, \n                    these are: \"token\", \"revision\", \"subfolder\", and \"weight-name\".\n                    \n                    They can be specified as so in any order, they are not positional:\n                    \"huggingface/ti_model;revision=main;subfolder=repo_subfolder;weight-name=ti_model.safetensors\".\n                    \n                    The \"token\" argument can be used to override the prompt token used for the \n                    textual inversion prompt embedding. For normal Stable Diffusion the default \n                    token value is provided by the model itself, but for Stable Diffusion XL the\n                    default token value is equal to the model file name with no extension and all\n                    spaces replaced by underscores. \n                    \n                    The \"revision\" argument specifies the model revision to use for the Textual Inversion model\n                    when loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"subfolder\" argument specifies the Textual Inversion model subfolder, if specified \n                    when loading from a Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"weight-name\" argument indicates the name of the weights file to be loaded when \n                    loading from a Hugging Face repository or folder on disk. \n                    \n                    If you wish to load a weights file directly from disk, the simplest way is: \n                    --textual-inversions \"my_ti_model.safetensors\", all other loading arguments \n                    are unused in this case and may produce an error message if used.", "--control-nets": "Specify one or more ControlNet models using URIs. This should be a\n                    Hugging Face repository slug / blob link, path to model file on disk \n                    (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file), or model \n                    folder containing model files.\n                    \n                    If a ControlNet model file exists at a URL which serves the file as\n                    a raw download, you may provide an http/https link to it and it will be\n                    downloaded to dgenerates web cache.\n                    \n                    Optional arguments can be provided after the ControlNet model specification, \n                    these are: \"scale\", \"start\", \"end\", \"revision\", \"variant\", \"subfolder\", and \"dtype\".\n                    \n                    They can be specified as so in any order, they are not positional:\n                    \"huggingface/controlnet;scale=1.0;start=0.0;end=1.0;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n                    \n                    The \"scale\" argument specifies the scaling factor applied to the ControlNet model, \n                    the default value is 1.0.\n                    \n                    The \"start\" argument specifies at what fraction of the total inference steps to begin applying \n                    the ControlNet, defaults to 0.0, IE: the very beginning.\n                    \n                    The \"end\"  argument specifies at what fraction of the total inference steps to stop applying \n                    the ControlNet, defaults to 1.0, IE: the very end.\n                    \n                    The \"mode\" argument can be used when using --model-type torch-flux and ControlNet Union\n                    to specify the ControlNet mode.  Acceptable values are: \"canny\", \"tile\", \"depth\", \"blur\",\n                    \"pose\", \"gray\", \"lq\". This value may also be an integer between 0 and 6, inclusive.\n                    \n                    The \"revision\" argument specifies the model revision to use for the ControlNet model\n                    when loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the ControlNet model variant, if \"variant\" is specified \n                    when loading from a Hugging Face repository or folder, weights will be loaded from \"variant\" \n                    filename, e.g. \"pytorch_model.<variant>.safetensors. \"variant\" defaults to automatic selection. \n                    \"variant\" in the case of --control-nets does not default to the value of --variant to prevent \n                    failures during common use cases.\n                    \n                    The \"subfolder\" argument specifies the ControlNet model subfolder, if specified \n                    when loading from a Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the ControlNet model precision, it defaults to the value of -t/--dtype\n                    and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    If you wish to load a weights file directly from disk, the simplest way is: \n                    --control-nets \"my_controlnet.safetensors\" or --control-nets \"my_controlnet.safetensors;scale=1.0;dtype=float16\", \n                    all other loading arguments aside from \"scale\", \"start\", \"end\", and \"dtype\" are unused in this case and may produce\n                    an error message if used.\n                    \n                    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n                    loading syntax: --control-nets \n                    \"https://huggingface.co/UserName/repository-name/blob/main/controlnet.safetensors\",\n                    the \"revision\" argument may be used with this syntax.", "--t2i-adapters": "Specify one or more T2IAdapter models using URIs. This should be a\n                    Hugging Face repository slug / blob link, path to model file on disk \n                    (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file), or model \n                    folder containing model files.\n                    \n                    If a T2IAdapter model file exists at a URL which serves the file as\n                    a raw download, you may provide an http/https link to it and it will be\n                    downloaded to dgenerates web cache.\n                    \n                    Optional arguments can be provided after the T2IAdapter model specification,\n                    these are: \"scale\", \"revision\", \"variant\", \"subfolder\", and \"dtype\".\n                    \n                    They can be specified as so in any order, they are not positional:\n                    \"huggingface/t2iadapter;scale=1.0;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n                    \n                    The \"scale\" argument specifies the scaling factor applied to the T2IAdapter model, \n                    the default value is 1.0.\n                    \n                    The \"revision\" argument specifies the model revision to use for the T2IAdapter model\n                    when loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the T2IAdapter model variant, if \"variant\" is specified when \n                    loading from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename, \n                    e.g. \"pytorch_model.<variant>.safetensors. \"variant\"  defaults to automatic selection. \n                    \"variant\" in the case of --t2i-adapters does not default to the value of --variant to \n                    prevent failures during common use cases.\n                    \n                    The \"subfolder\" argument specifies the ControlNet model subfolder, if specified \n                    when loading from a Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the T2IAdapter model precision, it defaults to the value of -t/--dtype\n                    and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    If you wish to load a weights file directly from disk, the simplest way is: \n                    --t2i-adapters \"my_t2i_adapter.safetensors\" or --t2i-adapters \"my_t2i_adapter.safetensors;scale=1.0;dtype=float16\", \n                    all other loading arguments aside from \"scale\" and \"dtype\" are unused in this case and may produce\n                    an error message if used.\n                    \n                    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n                    loading syntax: --t2i-adapters\n                    \"https://huggingface.co/UserName/repository-name/blob/main/t2i_adapter.safetensors\",\n                    the \"revision\" argument may be used with this syntax.", "--scheduler": "Specify a scheduler (sampler) by URI. Passing \"help\" to this argument \n                    will print the compatible schedulers for a model without generating any images. Passing \"helpargs\" \n                    will yield a help message with a list of overridable arguments for each scheduler and their typical defaults. \n                    Arguments listed by \"helpargs\" can be overridden using the URI syntax typical to other dgenerate URI arguments.\n                    \n                    You may pass multiple scheduler URIs to this argument, each URI will be tried in turn.\n                    ", "--pag": "Use perturbed attention guidance? This is supported \n            for --model-type torch, torch-sdxl, and torch-sd3 for most use cases.\n            This enables PAG for the main model using default scale values.", "--pag-scales": "One or more perturbed attention guidance scales to try. \n            Specifying values enables PAG for the main model. \n            (default: [3.0])", "--pag-adaptive-scales": "One or more adaptive perturbed attention guidance scales to try.\n            Specifying values enables PAG for the main model. \n            (default: [0.0])", "--sdxl-refiner-pag": "Use perturbed attention guidance in the SDXL refiner? \n            This is supported for --model-type torch-sdxl for most use cases.\n            This enables PAG for the SDXL refiner model using default scale \n            values.", "--sdxl-refiner-pag-scales": "One or more perturbed attention guidance scales to try \n            with the SDXL refiner pass. Specifying values enables PAG for the refiner.\n            (default: [3.0])", "--sdxl-refiner-pag-adaptive-scales": "One or more adaptive perturbed attention guidance scales to try \n            with the SDXL refiner pass. Specifying values enables PAG for the refiner.\n            (default: [0.0])", "--model-sequential-offload": "Force sequential model offloading for the main pipeline, this may drastically reduce memory consumption\n                    and allow large models to run when they would otherwise not fit in your GPUs VRAM. \n                    Inference will be much slower. Mutually exclusive with --model-cpu-offload", "--model-cpu-offload": "Force model cpu offloading for the main pipeline, this may reduce memory consumption\n                    and allow large models to run when they would otherwise not fit in your GPUs VRAM. \n                    Inference will be slower. Mutually exclusive with --model-sequential-offload", "--s-cascade-decoder": "Specify a Stable Cascade (torch-s-cascade) decoder model path using a URI. \n                    This should be a Hugging Face repository slug / blob link, path to model file \n                    on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file), or model\n                    folder containing model files. \n                    \n                    Optional arguments can be provided after the decoder model specification, \n                    these are: \"revision\", \"variant\", \"subfolder\", and \"dtype\".\n                    \n                    They can be specified as so in any order, they are not positional:\n                    \"huggingface/decoder_model;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n                    \n                    The \"revision\" argument specifies the model revision to use for the decoder model\n                    when loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the decoder model variant and defaults to the value of \n                    --variant. When \"variant\" is specified when loading from a Hugging Face repository or folder,\n                    weights will be loaded from \"variant\" filename, e.g. \"pytorch_model.<variant>.safetensors.\n                    \n                    The \"subfolder\" argument specifies the decoder model subfolder, if specified \n                    when loading from a Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the Stable Cascade decoder model precision, it defaults to \n                    the value of -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    If you wish to load a weights file directly from disk, the simplest way is: \n                    --sdxl-refiner \"my_decoder.safetensors\" or --sdxl-refiner \"my_decoder.safetensors;dtype=float16\", \n                    all other loading arguments aside from \"dtype\" are unused in this case and may produce\n                    an error message if used.\n                    \n                    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n                    loading syntax: --s-cascade-decoder \n                    \"https://huggingface.co/UserName/repository-name/blob/main/decoder.safetensors\",\n                    the \"revision\" argument may be used with this syntax.", "--s-cascade-decoder-sequential-offload": "Force sequential model offloading for the Stable Cascade decoder pipeline, this may drastically\n                    reduce memory consumption and allow large models to run when they would otherwise not fit in \n                    your GPUs VRAM. Inference will be much slower. Mutually exclusive with --s-cascade-decoder-cpu-offload", "--s-cascade-decoder-cpu-offload": "Force model cpu offloading for the Stable Cascade decoder pipeline, this may reduce memory consumption\n                    and allow large models to run when they would otherwise not fit in your GPUs VRAM. \n                    Inference will be slower. Mutually exclusive with --s-cascade-decoder-sequential-offload", "--s-cascade-decoder-prompts": "One or more prompts to try with the Stable Cascade decoder model, \n                    by default the decoder model gets the primary prompt, this argument \n                    overrides that with a prompt of your choosing. The negative prompt \n                    component can be specified with the same syntax as --prompts", "--s-cascade-decoder-inference-steps": "One or more inference steps values to try with the Stable Cascade decoder. \n                    (default: [10])", "--s-cascade-decoder-guidance-scales": "One or more guidance scale values to try with the Stable Cascade decoder.\n                     (default: [0])", "--s-cascade-decoder-schedulers": "Specify a scheduler (sampler) by URI for the Stable Cascade decoder pass. \n                    Operates the exact same way as --scheduler including the \"help\" option. Passing 'helpargs' \n                    will yield a help message with a list of overridable arguments for each scheduler and \n                    their typical defaults. Defaults to the value of --scheduler.\n                    \n                    You may pass multiple scheduler URIs to this argument, each URI will be tried in turn.\n                    ", "--sdxl-refiner": "Specify a Stable Diffusion XL (torch-sdxl) refiner model path using a URI. \n                    This should be a Hugging Face repository slug / blob link, path to model file \n                    on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors file), or model\n                    folder containing model files. \n                    \n                    Optional arguments can be provided after the SDXL refiner model specification, \n                    these are: \"revision\", \"variant\", \"subfolder\", and \"dtype\".\n                    \n                    They can be specified as so in any order, they are not positional:\n                    \"huggingface/refiner_model_xl;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n                    \n                    The \"revision\" argument specifies the model revision to use for the refiner model\n                    when loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n                    \n                    The \"variant\" argument specifies the SDXL refiner model variant and defaults to the value of \n                    --variant. When \"variant\" is specified when loading from a Hugging Face repository or folder,\n                    weights will be loaded from \"variant\" filename, e.g. \"pytorch_model.<variant>.safetensors.\n                    \n                    The \"subfolder\" argument specifies the SDXL refiner model subfolder, if specified \n                    when loading from a Hugging Face repository or folder, weights from the specified subfolder.\n                    \n                    The \"dtype\" argument specifies the SDXL refiner model precision, it defaults to the value of -t/--dtype\n                    and should be one of: auto, bfloat16, float16, or float32.\n                    \n                    If you wish to load a weights file directly from disk, the simplest way is: \n                    --sdxl-refiner \"my_sdxl_refiner.safetensors\" or --sdxl-refiner \"my_sdxl_refiner.safetensors;dtype=float16\", \n                    all other loading arguments aside from \"dtype\" are unused in this case and may produce\n                    an error message if used.\n                    \n                    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n                    loading syntax: --sdxl-refiner \n                    \"https://huggingface.co/UserName/repository-name/blob/main/refiner_model.safetensors\",\n                    the \"revision\" argument may be used with this syntax.", "--sdxl-refiner-cpu-offload": "Force model cpu offloading for the SDXL refiner pipeline, this may reduce memory consumption\n                    and allow large models to run when they would otherwise not fit in your GPUs VRAM. \n                    Inference will be slower. Mutually exclusive with --refiner-sequential-offload", "--sdxl-refiner-schedulers": "Specify a scheduler (sampler) by URI for the SDXL refiner pass. Operates the exact\n                 same way as --scheduler including the \"help\" option. Passing 'helpargs' will yield a help \n                 message with a list of overridable arguments for each scheduler and their typical defaults.\n                 Defaults to the value of --scheduler.\n                 \n                 You may pass multiple scheduler URIs to this argument, each URI will be tried in turn.\n                 ", "--sdxl-refiner-edit": "Force the SDXL refiner to operate in edit mode instead of cooperative denoising mode\n                    as it would normally do for inpainting and ControlNet usage. The main model will perform\n                    the full amount of inference steps requested by --inference-steps. The output of the main model\n                    will be passed to the refiner model and processed with an image seed strength in img2img mode\n                    determined by (1.0 - high-noise-fraction)", "--sdxl-second-prompts": "One or more secondary prompts to try using SDXL's secondary text encoder. \n                    By default the model is passed the primary prompt for this value, this option\n                    allows you to choose a different prompt. The negative prompt component can be\n                    specified with the same syntax as --prompts", "--sdxl-t2i-adapter-factors": "One or more SDXL specific T2I adapter factors to try, this controls the amount of\n                    time-steps for which a T2I adapter applies guidance to an image, this is a value between \n                    0.0 and 1.0. A value of 0.5 for example indicates that the T2I adapter is only active for \n                    half the amount of time-steps it takes to completely render an image.", "--sdxl-aesthetic-scores": "One or more Stable Diffusion XL (torch-sdxl) \"aesthetic-score\" micro-conditioning parameters.\n                    Used to simulate an aesthetic score of the generated image by influencing the positive text\n                    condition. Part of SDXL's micro-conditioning as explained in section 2.2 of\n                    [https://huggingface.co/papers/2307.01952].", "--sdxl-crops-coords-top-left": "One or more Stable Diffusion XL (torch-sdxl) \"negative-crops-coords-top-left\" micro-conditioning\n                    parameters in the format \"0,0\". --sdxl-crops-coords-top-left can be used to generate an image that\n                    appears to be \"cropped\" from the position --sdxl-crops-coords-top-left downwards. Favorable,\n                    well-centered images are usually achieved by setting --sdxl-crops-coords-top-left to \"0,0\".\n                    Part of SDXL's micro-conditioning as explained in section 2.2 of \n                    [https://huggingface.co/papers/2307.01952].", "--sdxl-original-sizes": "One or more Stable Diffusion XL (torch-sdxl) \"original-size\" micro-conditioning parameters in\n                    the format (WIDTH)x(HEIGHT). If not the same as --sdxl-target-size the image will appear to be\n                    down or up-sampled. --sdxl-original-size defaults to --output-size or the size of any input\n                    images if not specified. Part of SDXL's micro-conditioning as explained in section 2.2 of \n                    [https://huggingface.co/papers/2307.01952]", "--sdxl-target-sizes": "One or more Stable Diffusion XL (torch-sdxl) \"target-size\" micro-conditioning parameters in\n                    the format (WIDTH)x(HEIGHT). For most cases, --sdxl-target-size should be set to the desired\n                    height and width of the generated image. If not specified it will default to --output-size or\n                    the size of any input images. Part of SDXL's micro-conditioning as explained in section 2.2 of \n                    [https://huggingface.co/papers/2307.01952]", "--sdxl-negative-aesthetic-scores": "One or more Stable Diffusion XL (torch-sdxl) \"negative-aesthetic-score\" micro-conditioning parameters.\n                    Part of SDXL's micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952].\n                    Can be used to simulate an aesthetic score of the generated image by influencing the negative text condition.", "--sdxl-negative-original-sizes": "One or more Stable Diffusion XL (torch-sdxl) \"negative-original-sizes\" micro-conditioning parameters.\n                    Negatively condition the generation process based on a specific image resolution. Part of SDXL's\n                    micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952].\n                    For more information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208", "--sdxl-negative-target-sizes": "One or more Stable Diffusion XL (torch-sdxl) \"negative-original-sizes\" micro-conditioning parameters.\n                    To negatively condition the generation process based on a target image resolution. It should be as same\n                    as the \"--sdxl-target-size\" for most cases. Part of SDXL's micro-conditioning as explained in section 2.2 of\n                    [https://huggingface.co/papers/2307.01952]. For more information, refer to this issue thread:\n                    https://github.com/huggingface/diffusers/issues/4208.", "--sdxl-negative-crops-coords-top-left": "One or more Stable Diffusion XL (torch-sdxl) \"negative-crops-coords-top-left\" micro-conditioning\n                    parameters in the format \"0,0\". Negatively condition the generation process based on a specific\n                    crop coordinates. Part of SDXL's micro-conditioning as explained in section 2.2 of\n                    [https://huggingface.co/papers/2307.01952]. For more information, refer\n                    to this issue thread: https://github.com/huggingface/diffusers/issues/4208.", "--sdxl-refiner-prompts": "One or more prompts to try with the SDXL refiner model, \n                    by default the refiner model gets the primary prompt, this argument \n                    overrides that with a prompt of your choosing. The negative prompt \n                    component can be specified with the same syntax as --prompts", "--sdxl-refiner-clip-skips": "One or more clip skip override values to try for the SDXL refiner, \n                    which normally uses the clip skip value for the main model when it is \n                    defined by --clip-skips.", "--sdxl-refiner-second-prompts": "One or more prompts to try with the SDXL refiner models secondary \n                    text encoder, by default the refiner model gets the primary prompt passed\n                    to its second text encoder, this argument overrides that with a prompt \n                    of your choosing. The negative prompt component can be specified with the \n                    same syntax as --prompts", "--sdxl-refiner-aesthetic-scores": "See: --sdxl-aesthetic-scores, applied to SDXL refiner pass.", "--sdxl-refiner-crops-coords-top-left": "See: --sdxl-crops-coords-top-left, applied to SDXL refiner pass.", "--sdxl-refiner-original-sizes": "See: --sdxl-refiner-original-sizes, applied to SDXL refiner pass.", "--sdxl-refiner-target-sizes": "See: --sdxl-refiner-target-sizes, applied to SDXL refiner pass.", "--sdxl-refiner-negative-aesthetic-scores": "See: --sdxl-negative-aesthetic-scores, applied to SDXL refiner pass.", "--sdxl-refiner-negative-original-sizes": "See: --sdxl-negative-original-sizes, applied to SDXL refiner pass.", "--sdxl-refiner-negative-target-sizes": "See: --sdxl-negative-target-sizes, applied to SDXL refiner pass.", "--sdxl-refiner-negative-crops-coords-top-left": "See: --sdxl-negative-crops-coords-top-left, applied to SDXL refiner pass.", "--sdxl-high-noise-fractions": "One or more high-noise-fraction values for Stable Diffusion XL (torch-sdxl),\n                    this fraction of inference steps will be processed by the base model, while the rest\n                    will be processed by the refiner model. Multiple values to this argument will result in\n                    additional generation steps for each value. In certain situations when the mixture of denoisers\n                    algorithm is not supported, such as when using --control-nets and inpainting with SDXL, the inverse\n                    proportion of this value IE: (1.0 - high-noise-fraction) becomes the --image-seed-strengths\n                    input to the SDXL refiner. (default: [0.8])", "--sdxl-refiner-inference-steps": "One or more inference steps values for the SDXL refiner when in use. \n                    Override the number of inference steps used by the SDXL refiner, \n                    which defaults to the value taken from --inference-steps.", "--sdxl-refiner-guidance-scales": "One or more guidance scale values for the SDXL refiner when in use. \n                    Override the guidance scale value used by the SDXL refiner, \n                    which defaults to the value taken from --guidance-scales.", "--sdxl-refiner-guidance-rescales": "One or more guidance rescale values for the SDXL refiner when in use. \n                    Override the guidance rescale value used by the SDXL refiner,\n                    which defaults to the value taken from --guidance-rescales.", "--safety-checker": "Enable safety checker loading, this is off by default.\n                    When turned on images with NSFW content detected may result in solid black output.\n                    Some pretrained models have no safety checker model present, in that case this \n                    option has no effect.", "--device": "cuda / cpu, or other device supported by torch, for example mps on MacOS. \n            (default: cuda, mps on MacOS). Use: cuda:0, cuda:1, cuda:2, etc. to specify a specific \n            cuda supporting GPU.", "--dtype": "Model precision: auto, bfloat16, float16, or float32. (default: auto)", "--output-size": "Image output size, for txt2img generation this is the exact output size.\n                    The dimensions specified for this value must be aligned by 8 or you will receive an error message.\n                    If an --image-seeds URI is used its Seed, Mask, and/or Control component image sources will be \n                    resized to this dimension with aspect ratio maintained before being used for generation by default,\n                    except in the case of Stable Cascade where the images are used as a style prompt (not a noised seed),\n                    and can be of varying dimensions. \n                    \n                    If --no-aspect is not specified, width will be fixed and a new height \n                    (aligned by 8) will be calculated for the input images. In most cases resizing the image inputs \n                    will result in an image output of an equal size to the inputs, except for upscalers and Deep Floyd \n                    --model-type values (torch-if*). \n                    \n                    If only one integer value is provided, that is the value for both dimensions. \n                    X/Y dimension values should be separated by \"x\".  \n                    \n                    This value defaults to 512x512 for Stable Diffusion when no --image-seeds are \n                    specified (IE txt2img mode), 1024x1024 for Stable Cascade and Stable Diffusion 3/XL or \n                    Flux model types, and 64x64 for  --model-type torch-if (Deep Floyd stage 1). \n                    \n                    Deep Floyd stage 1 images passed to superscaler models (--model-type torch-ifs*) \n                    that are specified  with the 'floyd' keyword argument in an --image-seeds definition are \n                    never resized or processed in any way.", "--no-aspect": "This option disables aspect correct resizing of images provided to --image-seeds globally.\n                    Seed, Mask, and Control guidance images will be resized to the closest dimension specified by --output-size\n                    that is aligned by 8 pixels with no consideration of the source aspect ratio. This can be \n                    overriden at the --image-seeds level with the image seed keyword argument 'aspect=true/false'.", "--output-path": "Output path for generated images and files.\n                    This directory will be created if it does not exist. (default: ./output)", "--output-prefix": "Name prefix for generated images and files.\n                    This prefix will be added to the beginning of every generated file,\n                    followed by an underscore.", "--output-overwrite": "Enable\n\n overwrites of files in the output directory that already exists.\n                    The default behavior is not to do this, and instead append a filename suffix:\n                    \"_duplicate_(number)\" when it is detected that the generated file name already exists.", "--output-configs": "Write a configuration text file for every output image or animation.\n                    The text file can be used reproduce that particular output image or animation by piping\n                    it to dgenerate STDIN or by using the --file option, for example \"dgenerate < config.dgen\" \n                    or \"dgenerate --file config.dgen\".  These files will be written to --output-path and are \n                    affected by --output-prefix and --output-overwrite as well. The files will be named \n                    after their corresponding image or animation file. Configuration files produced for \n                    animation frame images will utilize --frame-start and --frame-end to specify the \n                    frame number.", "--output-metadata": "Write the information produced by --output-configs to the PNG metadata of each image.\n                    Metadata will not be written to animated files (yet). The data is written to a \n                    PNG metadata property named DgenerateConfig and can be read using ImageMagick like so: \n                    \"magick identify -format \"%%[Property:DgenerateConfig] generated_file.png\".", "--prompt-weighter": "Specify a prompt weighter implementation by URI, for example: --prompt-weighter compel, or --prompt-weighter sd-embed. By default, no prompt weighting syntax is enabled, meaning that you cannot adjust token weights as you may be able to do in software such as ComfyUI, Automatic1111, CivitAI etc. And in some cases the length of your prompt is limited. Prompt weighters support these special token weighting syntaxes and long prompts, currently there are two implementations \"compel\" and \"sd-embed\". See: --prompt-weighter-help for a list of implementation names. You may also use --prompt-weighter-help \"name\" to see comprehensive documentation for a specific prompt weighter implementation.", "--prompt-weighter-help": "Use this option alone (or with --plugin-modules) and no model specification\n                 in order to list available prompt weighter names. Specifying one or more\n                 prompt weighter names after this option will cause usage documentation for the specified\n                 prompt weighters to be printed. When used with --plugin-modules, prompt weighters\n                 implemented by the specified plugins will also be listed.", "--prompts": "One or more prompts to try, an image group is generated for each prompt,\n                    prompt data is split by ; (semi-colon). The first value is the positive\n                    text influence, things you want to see. The Second value is negative\n                    influence IE. things you don't want to see.\n                    Example: --prompts \"photo of a horse in a field; artwork, painting, rain\".\n                    (default: [(empty string)])", "--sd3-max-sequence-length": "The maximum amount of prompt tokens that the T5EncoderModel \n                    (third text encoder) of Stable Diffusion 3 can handle. This should be \n                    an integer value between 1 and 512 inclusive. The higher the value\n                    the more resources and time are required for processing. (default: 256)", "--sd3-second-prompts": "One or more secondary prompts to try using the torch-sd3 (Stable Diffusion 3) \n                    secondary text encoder. By default the model is passed the primary prompt for this value, \n                    this option allows you to choose a different prompt. The negative prompt component can be\n                    specified with the same syntax as --prompts", "--sd3-third-prompts": "One or more tertiary prompts to try using the torch-sd3 (Stable Diffusion 3) \n                    tertiary (T5) text encoder. By default the model is passed the primary prompt for this value, \n                    this option allows you to choose a different prompt. The negative prompt component can be\n                    specified with the same syntax as --prompts", "--flux-second-prompts": "One or more secondary prompts to try using the torch-flux (Flux) \n                    secondary (T5) text encoder. By default the model is passed the primary prompt for this value,\n                    this option allows you to choose a different prompt.", "--flux-max-sequence-length": "The maximum amount of prompt tokens that the T5EncoderModel \n                    (second text encoder) of Flux can handle. This should be \n                    an integer value between 1 and 512 inclusive. The higher the value\n                    the more resources and time are required for processing. (default: 512)", "--clip-skips": "One or more clip skip values to try. Clip skip is the number of layers to be skipped from CLIP \n                    while computing the prompt embeddings, it must be a value greater than or equal to zero. A value of 1 means \n                    that the output of the pre-final layer will be used for computing the prompt embeddings. This is only \n                    supported for --model-type values \"torch\", \"torch-sdxl\", and \"torch-sd3\".", "--seeds": "One or more seeds to try, define fixed seeds to achieve deterministic output.\n                    This argument may not be used when --gse/--gen-seeds is used.\n                    (default: [randint(0, 99999999999999)])", "--seeds-to-images": "When this option is enabled, each provided --seeds value or value generated by --gen-seeds\n                    is used for the corresponding image input given by --image-seeds. If the amount of --seeds given\n                    is not identical to that of the amount of --image-seeds given, the seed is determined as:\n                    seed = seeds[image_seed_index %% len(seeds)], IE: it wraps around.", "--gen-seeds": "Auto generate N random seeds to try. This argument may not\n                    be used when -se/--seeds is used.", "--animation-format": "Output format when generating an animation from an input video / gif / webp etc.\n                    Value must be one of: mp4, png, apng, gif, or webp. You may also specify \"frames\"\n                    to indicate that only frames should be output and no coalesced animation file should be rendered.\n                    (default: mp4)", "--image-format": "Output format when writing static images. Any selection other than \"png\" is not \n                    compatible with --output-metadata. Value must be one of: png, apng, blp, bmp, dib, bufr, pcx, dds, ps, eps, gif, grib, h5, hdf, jp2, j2k, jpc, jpf, jpx, j2c, icns, ico, im, jfif, jpe, jpg, jpeg, tif, tiff, mpo, msp, palm, pdf, pbm, pgm, ppm, pnm, pfm, bw, rgb, rgba, sgi, tga, icb, vda, vst, webp, wmf, emf, or xbm. (default: png)", "--no-frames": "Do not write frame images individually when rendering an animation, \n                    only write the animation file. This option is incompatible with --animation-format frames.", "--frame-start": "Starting frame slice point for animated files (zero-indexed), the specified frame \n                    will be included. (default: 0)", "--frame-end": "Ending frame slice point for animated files (zero-indexed), the specified frame \n                    will be included.", "--image-seeds": "One or more image seed URIs to process, these may consist of URLs or file paths. \n                    Videos / GIFs / WEBP files will result in frames being rendered as well as an animated \n                    output file being generated if more than one frame is available in the input file. \n                    Inpainting for static images can be achieved by specifying a black and white mask image in each \n                    image seed string using a semicolon as the separating character, like so: \n                    \"my-seed-image.png;my-image-mask.png\", white areas of the mask indicate where \n                    generated content is to be placed in your seed image. \n                    \n                    Output dimensions specific to the image seed can be specified by placing the dimension \n                    at the end of the string following a semicolon like so: \"my-seed-image.png;512x512\" or \n                    \"my-seed-image.png;my-image-mask.png;512x512\". When using --control-nets, a singular \n                    image specification is interpreted as the control guidance image, and you can specify \n                    multiple control image sources by separating them with commas in the case where multiple \n                    ControlNets are specified, IE: (--image-seeds \"control-image1.png, control-image2.png\") OR\n                    (--image-seeds \"seed.png;control=control-image1.png, control-image2.png\").\n                     \n                    Using --control-nets with img2img or inpainting can be accomplished with the syntax: \n                    \"my-seed-image.png;mask=my-image-mask.png;control=my-control-image.png;resize=512x512\".\n                    The \"mask\" and \"resize\" arguments are optional when using --control-nets. Videos, GIFs,\n                    and WEBP are also supported as inputs when using --control-nets, even for the \"control\"\n                    argument. \n                    \n                    --image-seeds is capable of reading from multiple animated files at once or any \n                    combination of animated files and images, the animated file with the least amount of frames \n                    dictates how many frames are generated and static images are duplicated over the total amount\n                    of frames. The keyword argument \"aspect\" can be used to determine resizing behavior when\n                    the global argument --output-size or the local keyword argument \"resize\" is specified, \n                    it is a boolean argument indicating whether aspect ratio of the input image should be \n                    respected or ignored.  \n                    \n                    The keyword argument \"floyd\" can be used to specify images from\n                    a previous deep floyd stage when using --model-type torch-ifs*. When keyword arguments \n                    are present, all applicable images such as \"mask\", \"control\", etc. must also be defined\n                    with keyword arguments instead of with the short syntax.", "--seed-image-processors": "Specify one or more image processor actions to perform on the primary\n                    image(s) specified by --image-seeds. \n                    \n                    For example: --seed-image-processors \"flip\" \"mirror\" \"grayscale\".\n                    \n                    To obtain more information about what image processors are available and how to use them, \n                    see: --image-processor-help. \n                    \n                    If you have multiple images specified for batching, for example\n                    (--image-seeds \"images: img2img-1.png, img2img-2.png\"), you may use the delimiter \"+\" to separate \n                    image processor chains, so that a certain chain affects a certain seed image, the plus symbol \n                    may also be used to represent a null processor. \n                    \n                    For example: (--seed-image-processors affect-img-1 + affect-img-2), or \n                    (--seed-image-processors + affect-img-2), or (--seed-image-processors affect-img-1 +).\n                    \n                    The amount of processors / processor chains must not exceed the amount of input images, \n                    or you will receive a syntax error message. To obtain more information about what image \n                    processors  are available and how to use them, see: --image-processor-help.", "--mask-image-processors": "Specify one or more image processor actions to perform on the inpaint mask\n                    image(s) specified by --image-seeds. \n                    \n                    For example: --mask-image-processors \"invert\".\n                    \n                    To obtain more information about what image processors are available and how to use them, \n                    see: --image-processor-help. \n                    \n                    If you have multiple masks specified for batching, for example\n                    --image-seeds (\"images: img2img-1.png, img2img-2.png; mask-1.png, mask-2.png\"), you may use \n                    the delimiter \"+\" to separate image processor chains, so that a certain chain affects a certain \n                    mask image, the plus symbol may also be used to represent a null processor. \n                    \n                    For example: \n                    (--mask-image-processors affect-mask-1 + affect-mask-2), or (--mask-image-processors + affect-mask-2), \n                    or (--mask-image-processors affect-mask-1 +). \n                    \n                    The amount of processors / processor chains must not\n                    exceed the amount of input mask images, or you will receive a syntax error message. To obtain \n                    more information about what image processors are available and how to use them, \n                    see: --image-processor-help.", "--control-image-processors": "Specify one or more image processor actions to perform on the control\n                    image specified by --image-seeds, this option is meant to be used with --control-nets. \n                    \n                    Example: --control-image-processors \"canny;lower=50;upper=100\". \n                    \n                    The delimiter \"+\" can be used to specify a different processor group for each image when using \n                    multiple control images with --control-nets. \n                    \n                    For example if you have --image-seeds \"img1.png, img2.png\" or --image-seeds \"...;control=img1.png, img2.png\" \n                    specified and multiple ControlNet models specified with --control-nets, you can specify processors for \n                    those control images with the syntax: (--control-image-processors \"processes-img1\" + \"processes-img2\").\n                    \n                    This syntax also supports chaining of processors, for example: \n                    (--control-image-processors \"first-process-img1\" \"second-process-img1\" + \"process-img2\"). \n                     \n                    The amount of specified processors must not exceed the amount of specified control images, or you\n                    will receive a syntax error message. \n                    \n                    Images which do not have a processor defined for them will not be processed, and the plus character can \n                    be used to indicate an image is not to be processed and instead skipped over when that image is a \n                    leading element, for example (--control-image-processors + \"process-second\") would indicate that \n                    the first control guidance image is not to be processed, only the second. \n                    \n                    To obtain more information about what image processors \n                    are available and how to use them, see: --image-processor-help.", "--image-processor-help": "Use this option alone (or with --plugin-modules) and no model\n                    specification in order to list available image processor names.\n                    Specifying one or more image processor names after this option will cause usage\n                    documentation for the specified image processors to be printed. When used with\n                    --plugin-modules, image processors implemented by the specified plugins\n                    will also be listed.", "--post-processors": "Specify one or more image processor actions to perform on generated \n                    output before it is saved. For example: --post-processors \"upcaler;model=4x_ESRGAN.pth\".\n                    To obtain more information about what processors are available and how to use them, \n                    see: --image-processor-help.", "--image-seed-strengths": "One or more image strength values to try when using --image-seeds for \n                    img2img or inpaint mode. Closer to 0 means high usage of the seed image (less noise convolution), \n                    1 effectively means no usage (high noise convolution). Low values will produce something closer \n                    or more relevant to the input image, high values will give the AI\n\n more creative freedom. This \n                    value must be greater than 0 and less than or equal to 1. (default: [0.8])", "--upscaler-noise-levels": "One or more upscaler noise level values to try when using the super\n                    resolution upscaler --model-type torch-upscaler-x4 or torch-ifs. Specifying \n                    this option for --model-type torch-upscaler-x2 will produce an error message.\n                    The higher this value the more noise is added to the image before upscaling \n                    (similar to --image-seed-strengths). (default: [20 for x4, 250 for\n                    torch-ifs/torch-ifs-img2img, 0 for torch-ifs inpainting mode])", "--guidance-scales": "One or more guidance scale values to try. Guidance scale effects how much your\n                    text prompt is considered. Low values draw more data from images unrelated\n                    to text prompt. (default: [5])", "--image-guidance-scales": "One or more image guidance scale values to try. This can push the generated image towards the\n                    initial image when using --model-type *-pix2pix models, it is unsupported for other model types. \n                    Use in conjunction with --image-seeds, inpainting (masks) and --control-nets are not supported. \n                    Image guidance scale is enabled by setting image-guidance-scale > 1. Higher image guidance scale \n                    encourages generated images that are closely linked to the source image, usually at the expense of \n                    lower image quality. Requires a value of at least 1. (default: [1.5])", "--guidance-rescales": "One or more guidance rescale factors to try. Proposed by [Common Diffusion Noise Schedules and \n                    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) \"guidance_scale\" is defined \n                    as \"\u03c6\" in equation 16. of [Common Diffusion Noise Schedules and Sample Steps are Flawed]\n                    (https://arxiv.org/pdf/2305.08891.pdf). Guidance rescale factor should fix overexposure \n                    when using zero terminal SNR. This is supported for basic text to image generation \n                    when using --model-type \"torch\" but not inpainting, img2img, or --control-nets. \n                    When using --model-type \"torch-sdxl\" it is supported for basic generation, inpainting, \n                    and img2img, unless --control-nets is specified in which case only inpainting is supported.\n                    It is supported for --model-type \"torch-sdxl-pix2pix\" but not --model-type \"torch-pix2pix\".\n                    (default: [0.0])", "--inference-steps": "One or more inference steps values to try. The amount of inference (de-noising) steps\n                    effects image clarity to a degree, higher values bring the image closer to what\n                    the AI is targeting for the content of the image. Values between 30-40\n                    produce good results, higher values may improve image quality and or\n                    change image content. (default: [30])", "--cache-memory-constraints": "Cache constraint expressions describing when to clear all model caches\n                    automatically (DiffusionPipeline, UNet, VAE, ControlNet, and Text Encoder) considering current memory\n                    usage. If any of these constraint expressions are met all models cached in memory will be cleared. \n                    Example, and default value: \"used_percent > 70\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.CACHE_MEMORY_CONSTRAINTS]", "--pipeline-cache-memory-constraints": "Cache constraint expressions describing when to automatically clear the in memory \n                    DiffusionPipeline cache considering current memory usage, and estimated memory usage of \n                    new models that are about to enter memory. If any of these constraint expressions are \n                    met all DiffusionPipeline objects cached in memory will be cleared. Example, and default \n                    value: \"pipeline_size > (available * 0.75)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.PIPELINE_CACHE_MEMORY_CONSTRAINTS]", "--unet-cache-memory-constraints": "Cache constraint expressions describing when to automatically clear the in memory UNet\n                    cache considering current memory usage, and estimated memory usage of new UNet models that \n                    are about to enter memory. If any of these constraint expressions are met all UNet \n                    models cached in memory will be cleared. Example, and default \n                    value: \"unet_size > (available * 0.75)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.UNET_CACHE_MEMORY_CONSTRAINTS]", "--vae-cache-memory-constraints": "Cache constraint expressions describing when to automatically clear the in memory VAE\n                    cache considering current memory usage, and estimated memory usage of new VAE models that \n                    are about to enter memory. If any of these constraint expressions are met all VAE \n                    models cached in memory will be cleared. Example, and default \n                    value: \"vae_size > (available * 0.75)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.VAE_CACHE_MEMORY_CONSTRAINTS]", "--control-net-cache-memory-constraints": "Cache constraint expressions describing when to automatically clear the in memory ControlNet\n                    cache considering current memory usage, and estimated memory usage of new ControlNet models that \n                    are about to enter memory. If any of these constraint expressions are met all ControlNet\n                    models cached in memory will be cleared. Example, and default \n                    value: \"controlnet_size > (available * 0.75)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.CONTROLNET_CACHE_MEMORY_CONSTRAINTS]", "--text-encoder-cache-memory-constraints": "Cache constraint expressions describing when to automatically clear the in memory Text Encoder\n                    cache considering current memory usage, and estimated memory usage of new Text Encoder models that \n                    are about to enter memory. If any of these constraint expressions are met all Text Encoder\n                    models cached in memory will be cleared. Example, and default \n                    value: \"text_encoder_size > (available * 0.75)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.TEXT_ENCODER_CACHE_MEMORY_CONSTRAINTS]", "--image-encoder-cache-memory-constraints": "Cache constraint expressions describing when to automatically clear the in memory Image Encoder\n                    cache considering current memory usage, and estimated memory usage of new Image Encoder models that \n                    are about to enter memory. If any of these constraint expressions are met all Image Encoder\n                    models cached in memory will be cleared. Example, and default \n                    value: \"image_encoder_size > (available * 0.75)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.IMAGE_ENCODER_CACHE_MEMORY_CONSTRAINTS]", "--adapter-cache-memory-constraints": "Cache constraint expressions describing when to automatically clear the in memory T2I Adapter\n                    cache considering current memory usage, and estimated memory usage of new T2I Adapter models that \n                    are about to enter memory. If any of these constraint expressions are met all T2I Adapter\n                    models cached in memory will be cleared. Example, and default \n                    value: \"adapter_size > (available * 0.75)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.ADAPTER_CACHE_MEMORY_CONSTRAINTS]", "--transformer-cache-memory-constraints": "Cache constraint expressions describing when to automatically clear the in memory Transformer\n                    cache considering current memory usage, and estimated memory usage of new Transformer models that \n                    are about to enter memory. If any of these constraint expressions are met all Transformer\n                    models cached in memory will be cleared. Example, and default \n                    value: \"transformer_size > (available * 0.75)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.pipelinewrapper.TRANSFORMER_CACHE_MEMORY_CONSTRAINTS]", "--image-processor-memory-constraints": "Cache constraint expressions describing when to automatically clear the entire in memory \n                    diffusion model cache considering current memory usage, and estimated memory usage of new \n                    image processor models that are about to enter memory. If any of these constraint expressions \n                    are met all diffusion related models cached in memory will be cleared. Example, and default \n                    value: \"processor_size > (available * 0.70)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.imageprocessors.IMAGE_PROCESSOR_MEMORY_CONSTRAINTS]", "--image-processor-cuda-memory-constraints": "Cache constraint expressions describing when to automatically clear the last active \n                    diffusion model from VRAM considering current GPU memory usage, and estimated GPU memory \n                    usage of new image processor models that are about to enter VRAM. If any of these \n                    constraint expressions are met the last active diffusion model in VRAM will be destroyed. \n                    Example, and default value: \"processor_size > (available * 0.70)\" For Syntax See: [https://dgenerate.readthedocs.io/en/v4.4.0/dgenerate_submodules.html#dgenerate.imageprocessors.IMAGE_PROCESSOR_CUDA_MEMORY_CONSTRAINTS]"}