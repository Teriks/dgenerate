{"UNet": {"model": {"optional": false, "types": ["str"], "files": {"mode": "dir"}}, "revision": {"optional": true, "types": ["str"], "default": null}, "variant": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32"]}, "quantizer": {"optional": true, "types": ["str"], "default": null}, "SUBMODEL_HELP": "UNet:\n    arguments:\n        model: str\n        revision: str | None = None\n        variant: str | None = None\n        subfolder: str | None = None\n        dtype: str | None = None\n        quantizer: str | None = None\n\n    Specify a UNet using a URI.\n\n    Examples:\n\n    \"huggingface/unet\", \"huggingface/unet;revision=main\", \"unet_folder_on_disk\"\n\n    The \"revision\" argument specifies the model revision to use for the UNet when loading from\n    Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the UNet model variant. If \"variant\" is specified when loading\n    from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename, e.g.\n    \"pytorch_model.<variant>.safetensors. For this argument, \"variant\" defaults to the value of\n    --variant if it is not specified in the URI.\n\n    The \"subfolder\" argument specifies the UNet model subfolder, if specified when loading from a\n    Hugging Face repository or folder, weights from the specified subfolder. If you are loading from\n    a combined single file checkpoint containing multiple components, this value will be used to\n    determine the key in the checkpoint that contains the unet, by default \"unet\" is used if\n    subfolder is not provided.\n\n    The \"dtype\" argument specifies the UNet model precision, it defaults to the value of -t/--dtype\n    and should be one of: auto, bfloat16, float16, or float32.\n\n    The \"quantizer\" argument specifies a quantization backend and configuration for the UNet model\n    individually, and uses the same URI syntax as --quantizer. If working from the command line you\n    may need to nested quote this URI, i.e:\n\n    --unet 'huggingface/unet;quantizer=\"bnb;bits=8\"'\n\n    If you wish to load weights directly from a path on disk, you must point this argument at the\n    folder they exist in, which should also contain the config.json file for the UNet. For example,\n    a downloaded repository folder from Hugging Face."}, "Transformer": {"model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"], "default": null}, "variant": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32"]}, "quantizer": {"optional": true, "types": ["str"], "default": null}, "SUBMODEL_HELP": "Transformer:\n    arguments:\n        model: str\n        revision: str | None = None\n        variant: str | None = None\n        subfolder: str | None = None\n        dtype: str | None = None\n        quantizer: str | None = None\n\n    Specify a Stable Diffusion 3 or Flux Transformer model using a URI.\n\n    Examples:\n\n    \"huggingface/transformer\"\n    \"huggingface/transformer;revision=main\"\n    \"transformer_folder_on_disk\"\n\n    Blob links / single file loads are supported for SD3 Transformers.\n\n    The \"revision\" argument specifies the model revision to use for the Transformer when loading\n    from Hugging Face repository or blob link, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the Transformer model variant. If \"variant\" is specified when\n    loading from a Hugging Face repository or folder, weights will be loaded from \"variant\"\n    filename, e.g. \"pytorch_model.<variant>.safetensors. For this argument, \"variant\" defaults to\n    the value of --variant if it is not specified in the URI.\n\n    The \"subfolder\" argument specifies the Transformer model subfolder, if specified when loading\n    from a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"dtype\" argument specifies the Transformer model precision, it defaults to the value of\n    -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\n    The \"quantizer\" argument specifies a quantization backend and configuration for the Transformer\n    model individually, and uses the same URI syntax as --quantizer. If working from the command\n    line you may need to nested quote this URI, i.e:\n\n    --transformer 'huggingface/transformer;quantizer=\"bnb;bits=8\"'\n\n    If you wish to load a weights file directly from disk, the simplest way is: --transformer\n    \"transformer.safetensors\", or with a dtype \"transformer.safetensors;dtype=float16\". All loading\n    arguments except \"dtype\" and \"quantizer\" are unused in this case and may produce an error\n    message if used.\n\n    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n    loading syntax: --transformer\n    \"AutoencoderKL;https://huggingface.co/UserName/repository-name/blob/main/transformer.safetensors\",\n    the \"revision\" argument may be used with this syntax."}, "Text Encoder": {"encoder": {"optional": false, "types": ["str"], "options": ["CLIPTextModel", "CLIPTextModelWithProjection", "T5EncoderModel", "DistillT5EncoderModel", "ChatGLMModel"]}, "model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"], "default": null}, "variant": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["auto", "float16", "bfloat16", "float32"]}, "quantizer": {"optional": true, "types": ["str"], "default": null}, "mode": {"optional": true, "types": ["str"], "default": null, "options": ["clip-l", "clip-l-sd3", "clip-g-sd3", "clip-l-sd35-large", "clip-g-sd35-large", "t5-xxl", "t5-xxl-sd3"]}, "SUBMODEL_HELP": "Text Encoder:\n    arguments:\n        encoder: str\n        model: str\n        revision: str | None = None\n        variant: str | None = None\n        subfolder: str | None = None\n        dtype: str | None = None\n        quantizer: str | None = None\n        mode: str | None = None\n\n    Specify Text Encoders for the main model using URIs, main models may use one or more text\n    encoders depending on the --model-type value and other dgenerate arguments. See: --text-encoders\n    help for information about what text encoders are needed for your invocation.\n\n    Examples:\n\n    \"CLIPTextModel;model=huggingface/text_encoder\"\n    \"CLIPTextModelWithProjection;model=huggingface/text_encoder;revision=main\"\n    \"T5EncoderModel;model=text_encoder_folder_on_disk\"\n    \"DistillT5EncoderModel;model=text_encoder_folder_on_disk\"\n\n    For main models which require multiple text encoders, the + symbol may be used to indicate that\n    a default value should be used for a particular text encoder, for example: --text-encoders + +\n    huggingface/encoder3. Any trailing text encoders which are not specified are given their default\n    value.\n\n    The value \"null\" may be used to indicate that a specific text encoder should not be loaded.\n\n    The \"revision\" argument specifies the model revision to use for the Text Encoder when loading\n    from Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the Text Encoder model variant. If \"variant\" is specified when\n    loading from a Hugging Face repository or folder, weights will be loaded from \"variant\"\n    filename, e.g. \"pytorch_model.<variant>.safetensors\". For this argument, \"variant\" defaults to\n    the value of --variant if it is not specified in the URI.\n\n    The \"subfolder\" argument specifies the Text Encoder model subfolder, if specified when loading\n    from a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"dtype\" argument specifies the Text Encoder model precision, it defaults to the value of\n    -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\n    The \"quantizer\" URI argument can be used to specify a quantization backend for the text encoder\n    using the same URI syntax as --quantizer. This is supported when loading from Hugging Face repo\n    slugs / folders on disk, and when using the \"mode\" argument with monolithic (non-sharded)\n    checkpoints. This is not supported when loading a submodule out of a combined checkpoint file\n    with \"subfolder\". If working from the command line you may need to nested quote this URI, i.e:\n\n    --text-encoders 'CLIPTextModel;model=huggingface/text_encoder;quantizer=\"bnb;bits=8\"'\n\n    The \"mode\" argument can be used to load monolithic single file checkpoints with specific\n    architecture configurations. Available modes are:\n\n    Flux & T5 universal modes:\n\n    * \"clip-l\" for monolithic Flux CLIP-L checkpoints\n    * \"t5-xxl\" for monolithic Flux T5 checkpoints\n\n    SD3 and SD3.5 specific modes:\n\n    * \"clip-l-sd3\" for SD3/SD3.5 medium CLIP-L checkpoints\n    * \"clip-g-sd3\" for SD3/SD3.5 medium CLIP-G checkpoints\n    * \"t5-xxl-sd3\" for SD3/SD3.5 T5-XXL checkpoints\n    * \"clip-l-sd35-large\" for SD3.5 large variant CLIP-L checkpoints\n    * \"clip-g-sd35-large\" for SD3.5 large variant CLIP-G checkpoints\n\n     The \"mode\" option is mutually exclusive with \"subfolder\".\n\n    Available encoder classes are:\n\n    * CLIPTextModel\n    * CLIPTextModelWithProjection\n    * T5EncoderModel\n    * DistillT5EncoderModel (see: LifuWang/DistillT5)\n    * ChatGLMModel (for Kolors models)\n\n    If you wish to load weights directly from a path on disk, you must point this argument at the\n    folder they exist in, which should also contain the config.json file for the Text Encoder. For\n    example, a downloaded repository folder from Hugging Face."}, "VAE": {"encoder": {"optional": false, "types": ["str"], "options": ["AutoencoderKL", "AsymmetricAutoencoderKL", "AutoencoderTiny", "ConsistencyDecoderVAE"]}, "model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"], "default": null}, "variant": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "extract": {"optional": false, "types": ["bool"], "default": false}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32"]}, "SUBMODEL_HELP": "VAE:\n    arguments:\n        encoder: str\n        model: str\n        revision: str | None = None\n        variant: str | None = None\n        subfolder: str | None = None\n        extract: bool = False\n        dtype: str | None = None\n\n    Specify a VAE using a URI, the URI syntax is: \"AutoEncoderClass;model=(Hugging Face repository\n    slug/blob link or file/folder path)\".\n\n    Examples:\n\n    \"AutoencoderKL;model=vae.pt\"\n    \"AsymmetricAutoencoderKL;model=huggingface/vae\"\n    \"AutoencoderTiny;model=huggingface/vae\"\n    \"ConsistencyDecoderVAE;model=huggingface/vae\"\n\n    The AutoencoderKL encoder class accepts Hugging Face repository slugs/blob links, .pt, .pth,\n    .bin, .ckpt, and .safetensors files.\n\n    Other encoders can only accept Hugging Face repository slugs/blob links, or a path to a folder\n    on disk with the model configuration and model file(s).\n\n    If an AutoencoderKL VAE model file exists at a URL which serves the file as a raw download, you\n    may provide an http/https link to it and it will be downloaded to dgenerate's web cache.\n\n    Aside from the \"model\" argument, there are four other optional arguments that can be specified,\n    these are: \"revision\", \"variant\", \"subfolder\", \"dtype\".\n\n    They can be specified as so in any order, they are not positional:\n\n    \"AutoencoderKL;model=huggingface/vae;revision=main;variant=fp16;subfolder=sub_folder;dtype=float16\"\n\n    The \"revision\" argument specifies the model revision to use for the VAE when loading from\n    Hugging Face repository or blob link, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the VAE model variant. If \"variant\" is specified when loading\n    from a Hugging Face repository or folder, weights will be loaded from \"variant\" filename, e.g.\n    \"pytorch_model.<variant>.safetensors. \"variant\" in the case of --vae does not default to the\n    value of --variant to prevent failures during common use cases.\n\n    The \"subfolder\" argument specifies the VAE model subfolder, if specified when loading from a\n    Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"extract\" argument specifies that \"model\" points at a combind single file checkpoint\n    containing multiple components such as the UNet and Text Encoders, and that we should extract\n    the VAE. When using this argument you can use \"subfolder\" to indicate the key in the checkpoint\n    containing the model, this defaults to \"vae\".\n\n    The \"dtype\" argument specifies the VAE model precision, it defaults to the value of -t/--dtype\n    and should be one of: auto, bfloat16, float16, or float32.\n\n    If you wish to load a weights file directly from disk, the simplest way is: --vae\n    \"AutoencoderKL;my_vae.safetensors\", or with a dtype\n    \"AutoencoderKL;my_vae.safetensors;dtype=float16\". All loading arguments except \"dtype\" are\n    unused in this case and may produce an error message if used.\n\n    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n    loading syntax: --vae\n    \"AutoencoderKL;https://huggingface.co/UserName/repository-name/blob/main/vae_model.safetensors\",\n    the \"revision\" argument may be used with this syntax."}, "Image Encoder": {"model": {"optional": false, "types": ["str"], "files": {"mode": "dir"}}, "revision": {"optional": true, "types": ["str"], "default": null}, "variant": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32"]}, "SUBMODEL_HELP": "Image Encoder:\n    arguments:\n        model: str\n        revision: str | None = None\n        variant: str | None = None\n        subfolder: str | None = None\n        dtype: str | None = None\n\n    Specify an Image Encoder using a URI.\n\n    Image Encoders are used with --ip-adapters models, and must be specified if none of the loaded\n    --ip-adapters contain one. An error will be produced in this situation, which requires you to\n    use this argument.\n\n    An image encoder can also be manually specified for Stable Cascade models.\n\n    Examples:\n\n    \"huggingface/image_encoder\"\n    \"huggingface/image_encoder;revision=main\"\n    \"image_encoder_folder_on_disk\"\n\n    Blob links / single file loads are not supported for Image Encoders.\n\n    The \"revision\" argument specifies the model revision to use for the Image Encoder when loading\n    from Hugging Face repository or blob link, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the Image Encoder model variant. If \"variant\" is specified when\n    loading from a Hugging Face repository or folder, weights will be loaded from \"variant\"\n    filename, e.g. \"pytorch_model.<variant>.safetensors.\n\n    Similar to --vae, \"variant\" does not default to the value of --variant in order to prevent\n    errors with common use cases. If you specify multiple IP Adapters, they must all have the same\n    \"variant\" value or you will receive a usage error.\n\n    The \"subfolder\" argument specifies the Image Encoder model subfolder, if specified when loading\n    from a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"dtype\" argument specifies the Image Encoder model precision, it defaults to the value of\n    -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\n    If you wish to load weights directly from a path on disk, you must point this argument at the\n    folder they exist in, which should also contain the config.json file for the Image Encoder. For\n    example, a downloaded repository folder from Hugging Face."}, "LoRA": {"model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "weight-name": {"optional": true, "types": ["str"], "default": null}, "scale": {"optional": false, "types": ["float"], "default": 1.0}, "SUBMODEL_HELP": "LoRA:\n    arguments:\n        model: str\n        revision: str | None = None\n        subfolder: str | None = None\n        weight-name: str | None = None\n        scale: float = 1.0\n\n    Specify one or more LoRA models using URIs. These should be a Hugging Face repository slug /\n    blob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors\n    file), or model folder containing model files.\n\n    If a LoRA model file exists at a URL which serves the file as a raw download, you may provide an\n    http/https link to it and it will be downloaded to dgenerate's web cache.\n\n    Optional arguments can be provided after a LoRA model specification, these are: \"scale\",\n    \"revision\", \"subfolder\", and \"weight-name\".\n\n    They can be specified as so in any order, they are not positional:\n\n    \"huggingface/lora;scale=1.0;revision=main;subfolder=repo_subfolder;weight-name=lora.safetensors\"\n\n    The \"scale\" argument indicates the scale factor of the LoRA.\n\n    The \"revision\" argument specifies the model revision to use for the LoRA when loading from\n    Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"subfolder\" argument specifies the LoRA model subfolder, if specified when loading from a\n    Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"weight-name\" argument indicates the name of the weights file to be loaded when loading from\n    a Hugging Face repository or folder on disk.\n\n    If you wish to load a weights file directly from disk, the simplest way is: --loras\n    \"my_lora.safetensors\", or with a scale \"my_lora.safetensors;scale=1.0\", all other loading\n    arguments are unused in this case and may produce an error message if used."}, "IP Adapter": {"model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "weight-name": {"optional": true, "types": ["str"], "default": null}, "scale": {"optional": false, "types": ["float"], "default": 1.0}, "SUBMODEL_HELP": "IP Adapter:\n    arguments:\n        model: str\n        revision: str | None = None\n        subfolder: str | None = None\n        weight-name: str | None = None\n        scale: float = 1.0\n\n    Specify one or more IP Adapter models using URIs. These should be a Hugging Face repository slug\n    / blob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors\n    file), or model folder containing model files.\n\n    If an IP Adapter model file exists at a URL which serves the file as a raw download, you may\n    provide an http/https link to it and it will be downloaded to dgenerate's web cache.\n\n    Optional arguments can be provided after an IP Adapter model specification, these are: \"scale\",\n    \"revision\", \"subfolder\", and \"weight-name\".\n\n    They can be specified as so in any order, they are not positional:\n\n    \"huggingface/ip-adapter;scale=1.0;revision=main;subfolder=repo_subfolder;weight-name=ip_adapter.safetensors\".\n\n    The \"scale\" argument indicates the scale factor of the IP Adapter.\n\n    The \"revision\" argument specifies the model revision to use for the IP Adapter when loading from\n    Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"subfolder\" argument specifies the IP Adapter model subfolder, if specified when loading\n    from a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"weight-name\" argument indicates the name of the weights file to be loaded when loading from\n    a Hugging Face repository or folder on disk.\n\n    If you wish to load a weights file directly from disk, the simplest way is: --ip-adapters\n    \"ip_adapter.safetensors\", or with a scale \"ip_adapter.safetensors;scale=1.0\", all other loading\n    arguments are unused in this case and may produce an error message if used."}, "Control Net": {"model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"]}, "variant": {"optional": true, "types": ["str"]}, "subfolder": {"optional": true, "types": ["str"]}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32"]}, "scale": {"optional": false, "types": ["float"], "default": 1.0}, "start": {"optional": false, "types": ["float"], "default": 0.0}, "end": {"optional": false, "types": ["float"], "default": 1.0}, "mode": {"optional": true, "types": ["int", "str"], "default": null}, "SUBMODEL_HELP": "Control Net:\n    arguments:\n        model: str\n        revision: str | None\n        variant: str | None\n        subfolder: str | None\n        dtype: str | None = None\n        scale: float = 1.0\n        start: float = 0.0\n        end: float = 1.0\n        mode: int | str | None = None\n\n    Specify one or more ControlNet models using URIs. This should be a Hugging Face repository slug\n    / blob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors\n    file), or model folder containing model files.\n\n    If a ControlNet model file exists at a URL which serves the file as a raw download, you may\n    provide an http/https link to it and it will be downloaded to dgenerate's web cache.\n\n    Optional arguments can be provided after the ControlNet model specification, these are: \"scale\",\n    \"start\", \"end\", \"mode\", \"revision\", \"variant\", \"subfolder\", and \"dtype\".\n\n    They can be specified as so in any order, they are not positional:\n\n    \"huggingface/controlnet;scale=1.0;start=0.0;end=1.0;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n\n    The \"scale\" argument specifies the scaling factor applied to the ControlNet model, the default\n    value is 1.0.\n\n    The \"start\" argument specifies at what fraction of the total inference steps to begin applying\n    the ControlNet, defaults to 0.0, IE: the very beginning.\n\n    The \"end\" argument specifies at what fraction of the total inference steps to stop applying the\n    ControlNet, defaults to 1.0, IE: the very end.\n\n    The \"mode\" argument can be used when using --model-type sdxl / flux and a ControlNet Union model\n    to specify the ControlNet mode. This may be a string or an integer.\n\n    For --model-type sdxl Acceptable \"mode\" values are:\n\n        \"openpose\" = 0\n        \"depth\" = 1\n        \"hed\" = 2\n        \"pidi\" = 2\n        \"scribble\" = 2\n        \"ted\" = 2\n        \"canny\" = 3\n        \"lineart\" = 3\n        \"anime_lineart\" = 3\n        \"mlsd\" = 3\n        \"normal\" = 4\n        \"segment\" = 5\n    \n\n    For --model-type flux Acceptable \"mode\" values are:\n\n        \"canny\" = 0\n        \"tile\" = 1\n        \"depth\" = 2\n        \"blur\" = 3\n        \"pose\" = 4\n        \"gray\" = 5\n        \"lq\" = 6\n\n    The \"revision\" argument specifies the model revision to use for the ControlNet model when\n    loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the ControlNet model variant, if \"variant\" is specified when\n    loading from a Hugging Face repository or folder, weights will be loaded from \"variant\"\n    filename, e.g. \"pytorch_model.<variant>.safetensors. \"variant\" defaults to automatic selection.\n    \"variant\" in the case of --control-nets does not default to the value of --variant to prevent\n    failures during common use cases.\n\n    The \"subfolder\" argument specifies the ControlNet model subfolder, if specified when loading\n    from a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"dtype\" argument specifies the ControlNet model precision, it defaults to the value of\n    -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\n    If you wish to load a weights file directly from disk, the simplest way is: --control-nets\n    \"my_controlnet.safetensors\" or --control-nets\n    \"my_controlnet.safetensors;scale=1.0;dtype=float16\", all other loading arguments aside from\n    \"scale\", \"start\", \"end\", and \"dtype\" are unused in this case and may produce an error message if\n    used.\n\n    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n    loading syntax: --control-nets\n    \"https://huggingface.co/UserName/repository-name/blob/main/controlnet.safetensors\", the\n    \"revision\" argument may be used with this syntax."}, "T2I Adapter": {"model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"]}, "variant": {"optional": true, "types": ["str"]}, "subfolder": {"optional": true, "types": ["str"]}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32"]}, "scale": {"optional": false, "types": ["float"], "default": 1.0}, "SUBMODEL_HELP": "T2I Adapter:\n    arguments:\n        model: str\n        revision: str | None\n        variant: str | None\n        subfolder: str | None\n        dtype: str | None = None\n        scale: float = 1.0\n\n    Specify one or more T2IAdapter models using URIs. This should be a Hugging Face repository slug\n    / blob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt, or .safetensors\n    file), or model folder containing model files.\n\n    If a T2IAdapter model file exists at a URL which serves the file as a raw download, you may\n    provide an http/https link to it and it will be downloaded to dgenerate's web cache.\n\n    Optional arguments can be provided after the T2IAdapter model specification, these are: \"scale\",\n    \"revision\", \"variant\", \"subfolder\", and \"dtype\".\n\n    They can be specified as so in any order, they are not positional:\n\n    \"huggingface/t2iadapter;scale=1.0;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n\n    The \"scale\" argument specifies the scaling factor applied to the T2IAdapter model, the default\n    value is 1.0.\n\n    The \"revision\" argument specifies the model revision to use for the T2IAdapter model when\n    loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the T2IAdapter model variant, if \"variant\" is specified when\n    loading from a Hugging Face repository or folder, weights will be loaded from \"variant\"\n    filename, e.g. \"pytorch_model.<variant>.safetensors. \"variant\"  defaults to automatic selection.\n    \"variant\" in the case of --t2i-adapters does not default to the value of --variant to prevent\n    failures during common use cases.\n\n    The \"subfolder\" argument specifies the ControlNet model subfolder, if specified when loading\n    from a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"dtype\" argument specifies the T2IAdapter model precision, it defaults to the value of\n    -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\n    If you wish to load a weights file directly from disk, the simplest way is: --t2i-adapters\n    \"my_t2i_adapter.safetensors\" or --t2i-adapters\n    \"my_t2i_adapter.safetensors;scale=1.0;dtype=float16\", all other loading arguments aside from\n    \"scale\" and \"dtype\" are unused in this case and may produce an error message if used.\n\n    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n    loading syntax: --t2i-adapters\n    \"https://huggingface.co/UserName/repository-name/blob/main/t2i_adapter.safetensors\", the\n    \"revision\" argument may be used with this syntax."}, "Textual Inversion": {"model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "token": {"optional": true, "types": ["str"], "default": null}, "revision": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "weight-name": {"optional": true, "types": ["str"], "default": null}, "SUBMODEL_HELP": "Textual Inversion:\n    arguments:\n        model: str\n        token: str | None = None\n        revision: str | None = None\n        subfolder: str | None = None\n        weight-name: str | None = None\n\n    Specify one or more Textual Inversion models using URIs. These should be a Hugging Face\n    repository slug / blob link, path to model file on disk (for example, a .pt, .pth, .bin, .ckpt,\n    or .safetensors file), or model folder containing model files.\n\n    If a Textual Inversion model file exists at a URL which serves the file as a raw download, you\n    may provide an http/https link to it and it will be downloaded to dgenerate's web cache.\n\n    Optional arguments can be provided after the Textual Inversion model specification, these are:\n    \"token\", \"revision\", \"subfolder\", and \"weight-name\".\n\n    They can be specified as so in any order, they are not positional:\n\n    \"huggingface/ti_model;revision=main;subfolder=repo_subfolder;weight-name=ti_model.safetensors\".\n\n    The \"token\" argument can be used to override the prompt token used for the textual inversion\n    prompt embedding. For normal Stable Diffusion the default token value is provided by the model\n    itself, but for Stable Diffusion XL and Flux the default token value is equal to the model file\n    name with no extension and all spaces replaced by underscores.\n\n    The \"revision\" argument specifies the model revision to use for the Textual Inversion model when\n    loading from Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"subfolder\" argument specifies the Textual Inversion model subfolder, if specified when\n    loading from a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"weight-name\" argument indicates the name of the weights file to be loaded when loading from\n    a Hugging Face repository or folder on disk.\n\n    If you wish to load a weights file directly from disk, the simplest way is: --textual-inversions\n    \"my_ti_model.safetensors\", all other loading arguments are unused in this case and may produce\n    an error message if used."}, "Adetailer Detector": {"model": {"optional": false, "types": ["str"], "files": {"mode": "in", "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "weight-name": {"optional": true, "types": ["str"], "default": null}, "confidence": {"optional": false, "types": ["float"], "default": 0.3}, "detector-padding": {"optional": true, "types": ["int", "tuple"], "default": null}, "mask-shape": {"optional": true, "types": ["str"], "default": null, "options": ["r", "rect", "rectangle", "c", "circle", "ellipse"]}, "mask-padding": {"optional": true, "types": ["int", "tuple"], "default": null}, "mask-blur": {"optional": true, "types": ["int"], "default": null}, "mask-dilation": {"optional": true, "types": ["int"], "default": null}, "model-masks": {"optional": true, "types": ["bool"], "default": null}, "index-filter": {"optional": true, "types": ["list"], "default": null}, "class-filter": {"optional": true, "types": ["list"], "default": null}, "prompt": {"optional": true, "types": ["str"], "default": null}, "negative-prompt": {"optional": true, "types": ["str"], "default": null}, "device": {"optional": true, "types": ["str"], "default": null}, "SUBMODEL_HELP": "Adetailer Detector:\n    arguments:\n        model: str\n        revision: str | None = None\n        subfolder: str | None = None\n        weight-name: str | None = None\n        confidence: float = 0.3\n        detector-padding: int | tuple | None = None\n        mask-shape: str | None = None\n        mask-padding: int | tuple | None = None\n        mask-blur: int | None = None\n        mask-dilation: int | None = None\n        model-masks: bool | None = None\n        index-filter: list | None = None\n        class-filter: list | None = None\n        prompt: str | None = None\n        negative-prompt: str | None = None\n        device: str | None = None\n\n    Specify one or more adetailer YOLO detector model URIs. When specifying this option, you must\n    provide an image to --image-seeds, inpaint masks will be auto generated based on what is\n    detected by the provided detector models.\n\n    The models will be used in sequence to detect and then inpaint your image within the detection\n    areas. This can be used for face detailing, face swapping, hand detailing, etc. on any arbitrary\n    image provided using an image generation model of your choice.\n\n    This option supports: --model-type sd, sdxl, kolors, sd3, flux, and flux-fill\n\n    Example: --adetailer-detectors Bingsu/adetailer;weight-name=face_yolov8n.pt\n\n    The \"revision\" argument specifies the model revision to use for the adetailer model when loading\n    from Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"subfolder\" argument specifies the adetailer model subfolder, if specified when loading from\n    a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"weight-name\" argument indicates the name of the weights file to be loaded when loading from\n    a Hugging Face repository or folder on disk.\n\n    The \"class-filter\" (overrides --adetailer-class-filter) argument is a list of class IDs or class\n    names that indicates what YOLO detection classes to keep. This filter is applied first, before\n    index-filter. Detections that don't match any of the specified classes will be ignored.\n\n    Example \"class-filter\" values:\n\n        * Only keep detection class ID 0:\n        class-filter=0\n    \n        * Only keep detection class \"hand\":\n        class-filter=hand\n    \n        * Keep class IDs 2 and 3:\n        class-filter=2,3\n    \n        * Keep class ID 0 and class name \"hand\":\n        class-filter=0,hand\n    \n        * String digits are interpreted as integers:\n        class-filter=\"0\" (interpreted as class name \"0\", not likely useful)\n    \n        * List syntax is also supported:\n        class-filter=[0, \"hand\"]\n    \n    The \"index-filter\" (overrides --adetailer-index-filter) argument is a list values or a\n    single value that indicates what YOLO detection indices to keep, the index values start\n    at zero. Detections are sorted by their top left bounding box coordinate from left to right,\n    top to bottom, by (confidence descending). The order of detections in the image is identical to\n    the reading order of words on a page (english). Inpainting will only be performed on the\n    specified detection indices, if no indices are specified, then inpainting\n    will be performed on all detections. This filter is applied after class-filter.\n\n    Example \"index-filter\" values:\n\n        * keep the first, leftmost, topmost detection:\n        index-filter=0\n    \n        * keep detections 1 and 3:\n        index-filter=[1, 3]\n    \n        * CSV syntax is supported (tuple):\n        index-filter=1,3\n\n    The \"detector-padding\" (overrides --adetailer-detector-paddings) argument specifies the amount\n    of padding that will be added to the detection rectangle which is used to generate a masked\n    area. The default is 0, you can make the mask area around the detected feature larger with\n    positive padding and smaller with negative padding.\n\n    Padding examples:\n\n        32 (32px Uniform, all sides)\n    \n        10x20 (10px Horizontal, 20px Vertical)\n    \n        10x20x30x40 (10px Left, 20px Top, 30px Right, 40px Bottom)\n\n    The \"mask-padding\" (overrides --adetailer-mask-paddings) argument indicates how much padding to\n    place around the masked area when cropping out the image to be inpainted. This value must be\n    large enough to accommodate any feathering on the edge of the mask caused by \"mask-blur\" or\n    \"mask-dilation\" for the best result, the default value is 32. The syntax for specifying this\n    value is identical to \"detector-padding\".\n\n    The \"mask-shape\" (overrides --adetailer-mask-shapes) argument indicates what mask shape\n    adetailer should attempt to draw around a detected feature, the default value is \"rectangle\".\n    You may also specify \"circle\" to generate an ellipsoid shaped mask, which might be helpful for\n    achieving better blending. Valid values are: (\"r\", \"rect\", \"rectangle\"), or (\"c\", \"circle\",\n    \"ellipse\").\n\n    The \"mask-blur\" (overrides --adetailer-mask-blurs) argument indicates the level of gaussian blur\n    to apply to the generated inpaint mask, which can help with smooth blending in of the inpainted\n    feature\n\n    The \"model-masks\" (overrides --adetailer-model-masks) argument indicates that masks generated by\n    the model itself should be preferred over masks generated from the detection bounding box. If\n    this is True, and the model itself returns mask data, \"mask-shape\", \"mask-padding\", and\n    \"detector-padding\" will all be ignored.\n\n    The \"mask-dilation\" (overrides --adetailer-mask-dilations) argument indicates the amount of\n    dilation applied to the inpaint mask, see: cv2.dilate\n\n    The \"confidence\" argument indicates the confidence value to use with the YOLO detector model,\n    this value defaults to 0.3 if not specified.\n\n    The \"prompt\" (overrides --prompt positive) argument overrides the positive inpainting prompt for\n    detections by this detector.\n\n    The \"negative-prompt\" (overrides --prompt negative) argument overrides the negative inpainting\n    prompt for detections by this detector.\n\n    The \"device\" argument indicates a device override for the YOLO detector model, the detector\n    model can be set to run on a different device if desired, for example: cuda:0, cuda:1, cpu, etc.\n    It runs on the same device as --device by default.\n\n    If you wish to load a weights file directly from disk, use: --adetailer-detectors\n    \"yolo_model.pt\"\n\n    You may also load a YOLO model directly from a URL or Hugging Face blob link.\n\n    Example: --adetailer-detectors https://modelsite.com/yolo-model.pt"}, "Stable Cascade Decoder": {"model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"], "default": null}, "variant": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32"]}, "SUBMODEL_HELP": "Stable Cascade Decoder:\n    arguments:\n        model: str\n        revision: str | None = None\n        variant: str | None = None\n        subfolder: str | None = None\n        dtype: str | None = None\n\n    Specify a Stable Cascade (s-cascade) decoder model path using a URI. This should be a Hugging\n    Face repository slug / blob link, path to model file on disk (for example, a .pt, .pth, .bin,\n    .ckpt, or .safetensors file), or model folder containing model files.\n\n    Optional arguments can be provided after the decoder model specification, these are: \"revision\",\n    \"variant\", \"subfolder\", and \"dtype\".\n\n    They can be specified as so in any order, they are not positional:\n\n    \"huggingface/decoder_model;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n\n    The \"revision\" argument specifies the model revision to use for the decoder model when loading\n    from Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the decoder model variant and defaults to the value of\n    --variant. When \"variant\" is specified when loading from a Hugging Face repository or folder,\n    weights will be loaded from \"variant\" filename, e.g. \"pytorch_model.<variant>.safetensors.\n\n    The \"subfolder\" argument specifies the decoder model subfolder, if specified when loading from a\n    Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"dtype\" argument specifies the Stable Cascade decoder model precision, it defaults to the\n    value of -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\n    If you wish to load a weights file directly from disk, the simplest way is: --sdxl-refiner\n    \"my_decoder.safetensors\" or --sdxl-refiner \"my_decoder.safetensors;dtype=float16\", all other\n    loading arguments aside from \"dtype\" are unused in this case and may produce an error message if\n    used.\n\n    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n    loading syntax: --s-cascade-decoder\n    \"https://huggingface.co/UserName/repository-name/blob/main/decoder.safetensors\", the \"revision\"\n    argument may be used with this syntax."}, "SDXL Refiner": {"model": {"optional": false, "types": ["str"], "files": {"mode": ["in", "dir"], "filetypes": [["Models", ["*.safetensors", "*.pt", "*.pth", "*.cpkt", "*.bin"]]]}}, "revision": {"optional": true, "types": ["str"], "default": null}, "variant": {"optional": true, "types": ["str"], "default": null}, "subfolder": {"optional": true, "types": ["str"], "default": null}, "dtype": {"optional": true, "types": ["str"], "default": null, "options": ["float16", "bfloat16", "float32"]}, "SUBMODEL_HELP": "SDXL Refiner:\n    arguments:\n        model: str\n        revision: str | None = None\n        variant: str | None = None\n        subfolder: str | None = None\n        dtype: str | None = None\n\n    Specify a Stable Diffusion XL (sdxl) refiner model path using a URI. This should be a Hugging\n    Face repository slug / blob link, path to model file on disk (for example, a .pt, .pth, .bin,\n    .ckpt, or .safetensors file), or model folder containing model files.\n\n    Optional arguments can be provided after the SDXL refiner model specification, these are:\n    \"revision\", \"variant\", \"subfolder\", and \"dtype\".\n\n    They can be specified as so in any order, they are not positional:\n\n    \"huggingface/refiner_model_xl;revision=main;variant=fp16;subfolder=repo_subfolder;dtype=float16\".\n\n    The \"revision\" argument specifies the model revision to use for the refiner model when loading\n    from Hugging Face repository, (The Git branch / tag, default is \"main\").\n\n    The \"variant\" argument specifies the SDXL refiner model variant and defaults to the value of\n    --variant. When \"variant\" is specified when loading from a Hugging Face repository or folder,\n    weights will be loaded from \"variant\" filename, e.g. \"pytorch_model.<variant>.safetensors.\n\n    The \"subfolder\" argument specifies the SDXL refiner model subfolder, if specified when loading\n    from a Hugging Face repository or folder, weights from the specified subfolder.\n\n    The \"dtype\" argument specifies the SDXL refiner model precision, it defaults to the value of\n    -t/--dtype and should be one of: auto, bfloat16, float16, or float32.\n\n    If you wish to load a weights file directly from disk, the simplest way is: --sdxl-refiner\n    \"my_sdxl_refiner.safetensors\" or --sdxl-refiner \"my_sdxl_refiner.safetensors;dtype=float16\", all\n    other loading arguments aside from \"dtype\" are unused in this case and may produce an error\n    message if used.\n\n    If you wish to load a specific weight file from a Hugging Face repository, use the blob link\n    loading syntax: --sdxl-refiner\n    \"https://huggingface.co/UserName/repository-name/blob/main/refiner_model.safetensors\", the\n    \"revision\" argument may be used with this syntax."}}