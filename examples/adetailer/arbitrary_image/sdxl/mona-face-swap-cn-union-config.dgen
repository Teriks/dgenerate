#! /usr/bin/env dgenerate --file
#! dgenerate 4.5.1

# You can use any SDXL model you want in combination with the adetailer inpainting algorithm
# this can be preformed on any image of your choosing in this manner

# This can be used for highly configurable face detailing on diffusion output, or
# for face swapping and other interesting effects

# In this example we can attempt to face swap the monalisa with a little help from
# the SDXL depth ControlNet to orient the new face in the correct direction.
# This works because ControlNets are compatible with SDXL inpainting

# we generate a depth image from the incoming control image using the midas image processor

# --adetailer-crop-control-image tells the adetailer algorithm to crop the depth
# image down to just the detection area, so that its just the face depth map
# that is used as the control image

# we specify --adetailer-detectors with a model that detects faces, an inpaint
# mask will be auto generated around the face and then inpainting will occur
# with the control net active

# The generated inpaint masks properties can be adjusted with: --adetailer-mask-paddings,
# --adetailer-mask-blurs, and --adetailer-mask-dilations. These arguments are plural because
# they can accept multiple values, when you provide multiple values dgenerate will
# create multiple variations of the image with those values.

stabilityai/stable-diffusion-xl-base-1.0
--model-type torch-sdxl
--variant fp16
--dtype float16
--image-seeds ../../../media/monalisa.png;control=../../../media/monalisa.png
--inference-steps 30
--guidance-scales 7
--control-nets xinsir/controlnet-union-sdxl-1.0;mode=depth;scale=0.6
--control-image-processors midas
--adetailer-detectors Bingsu/adetailer;weight-name=face_yolov8n.pt
--adetailer-crop-control-image
--image-seed-strengths 0.7
--seeds 1
--model-cpu-offload # save some memory, a lot of models are being used
--output-size 800
--output-path face-swap-mona-cn-union
--prompts "woman smiling really wide and showing teeth; deformed, weird chin"